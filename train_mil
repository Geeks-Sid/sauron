# train_mil.py
import argparse
import os
import pandas as pd
from torch.utils.tensorboard import SummaryWriter

# Sauron specific imports
from sauron.parse.argparse import get_args # Assuming get_args is here
from sauron.utils.environment_setup import (
    setup_device,
    seed_everything,
    create_results_directory,
    log_experiment_details,
)
from sauron.data.dataset_factory import get_dataset, determine_split_directory
from sauron.training.pipeline import train_fold # This is the refactored train_fold
from sauron.utils.generic_utils import log_results, save_pkl # Assuming these are general utils

def run_experiment_folds(
    dataset_instance, # Renamed from 'dataset' to avoid conflict with 'datasets' from return_splits
    args: argparse.Namespace,
    experiment_results_dir: str # Pass the main experiment results dir for fold-level saving
) -> pd.DataFrame:
    """
    Manages the k-fold cross-validation loop.
    """
    # Define expected metrics based on task type for clarity
    if args.task_type == "classification":
        metric_keys = ["test_auc", "val_auc", "test_acc", "val_acc"]
    elif args.task_type == "survival":
        metric_keys = ["test_c_index", "val_c_index"] # Example for survival
        # train_fold must return these for survival task
    else:
        raise ValueError(f"Unknown task_type: {args.task_type} for defining metrics.")
    
    all_fold_metrics = {key: [] for key in metric_keys}

    # Dataset creates and manages its own splits internally based on k
    dataset_instance.create_splits(k=args.k, seed=args.seed, split_dir=args.split_dir, val_frac = getattr(args, "val_frac", 0.2))

    for i in range(args.k_start, args.k_end):
        print(f"\n{'='*10} Processing Fold: {i} {'='*10}")
        
        # Fold-specific results directory (for logs, checkpoints from train_fold)
        # This is now handled INSIDE train_fold. train_fold receives `experiment_results_dir`
        # and creates `experiment_results_dir/fold_i` itself.
        
        dataset_instance.set_splits(fold_idx=i) # Tell dataset to use fold 'i'
        
        # `return_splits` should give (train_dataset, val_dataset, test_dataset_or_None)
        # It needs to know about args.backbone and args.patch_size for feature path resolution
        # or these need to be part of the dataset's internal config.
        # Assuming GenericMILDataset's return_splits handles this.
        current_fold_datasets = dataset_instance.return_splits(
            from_id=True, # This seems specific to how features are loaded
            # Potentially pass feature-related args if dataset needs them here:
            # backbone=args.backbone, patch_size=args.patch_size 
        )

        if args.preloading == "yes":
            print(f"Preloading data for fold {i}...")
            for d_idx, d in enumerate(current_fold_datasets):
                if d is not None: # test_split can be None
                    print(f"  Preloading dataset part {d_idx} ({type(d).__name__})...")
                    d.pre_loading() # Ensure pre_loading exists on dataset objects

        # train_fold now handles its own SummaryWriter for fold-specific logs
        # It needs the main experiment_results_dir to create its fold-specific sub-directory.
        fold_results_tuple = train_fold(
            datasets=current_fold_datasets, 
            cur_fold_num=i, 
            args=args,
            # Removed task_type as it should be in args
            # experiment_base_results_dir=experiment_results_dir # Pass main dir
        )
        # train_fold returns:
        # Classification (non-kfold): (results_dict, test_metric, val_metric, test_acc, val_acc_placeholder)
        # Survival (non-kfold): (results_dict, test_cindex, val_cindex)
        # k-fold mode: val_metric (e.g. val_auc or val_cindex)
        
        if args.k_fold: # Special handling for k-fold return from train_fold
            # If train_fold in k-fold mode returns only val_metric, we need to adjust
            # For now, assuming train_fold returns the full tuple even in k-fold,
            # and we just use the val_metric. Or, train_fold's signature needs adjustment.
            # Let's assume train_fold returns a simplified tuple for k-fold or we extract.
            # The original code implies train_fold returns more than just val_metric.
            # The unpack `results, *metrics` suggests this.
            
            patient_level_results_dict, *fold_metrics_values = fold_results_tuple
        else: # Not k-fold, means it's a single run with train/val/test
             patient_level_results_dict, *fold_metrics_values = fold_results_tuple


        if len(fold_metrics_values) != len(metric_keys):
            raise ValueError(
                f"Mismatch in number of metrics returned by train_fold ({len(fold_metrics_values)}) "
                f"and expected metric_keys ({len(metric_keys)}). "
                f"Task: {args.task_type}. Check train_fold's return for this task."
            )

        for key, value in zip(metric_keys, fold_metrics_values):
            all_fold_metrics[key].append(value)
            # Main summary writer (passed to this func) can log aggregated fold metrics
            # writer_main.add_scalar(f"FoldMetrics/{key}_fold_{i}", value, i) # Optional

        if patient_level_results_dict: # If train_fold returned detailed results
            fold_results_pkl_path = os.path.join(experiment_results_dir, f"fold_{i}_results.pkl")
            save_pkl(fold_results_pkl_path, patient_level_results_dict)
            print(f"Saved patient-level results for fold {i} to {fold_results_pkl_path}")

    return pd.DataFrame({"fold_num": range(args.k_start, args.k_end), **all_fold_metrics})


def main_experiment_runner(args: argparse.Namespace):
    """
    Main function to run the entire experiment.
    """
    # 1. Initial Setup (device, seed, main results directory)
    _ = setup_device() # Device setup, not strictly needed to be stored if not used directly here
    seed_everything(args.seed)
    
    # This is the main experiment directory, e.g., results_dir/exp_code_s{seed}
    # Individual folds will create subdirectories within this.
    experiment_main_results_dir = create_results_directory(
        args.results_dir, args.exp_code, args.seed
    )
    args.results_dir = experiment_main_results_dir # Update args to reflect the true main results path
                                                  # So train_fold can use it as a base.
    log_experiment_details(args, experiment_main_results_dir)

    # 2. Determine and validate split directory
    # This split_dir is where pre-generated split files (e.g., CSVs defining folds) might reside.
    # Or, if splits are generated on-the-fly, it could be where they are saved.
    args.split_dir = determine_split_directory(
        args.split_dir, args.task_name, args.label_frac, args.k_fold > 1 # k_fold is num_folds
    )
    # If splits are *required* to be pre-existing:
    if not os.path.isdir(args.split_dir) and getattr(args, "use_predefined_splits", False):
         raise NotADirectoryError(f"Predefined split directory does not exist: {args.split_dir}")
    os.makedirs(args.split_dir, exist_ok=True) # Ensure it exists if we plan to save splits there

    # 3. Setup TensorBoard Writer for overall experiment summary (optional)
    # train_fold will create its own writer for fold-specific logs.
    # This writer is for higher-level summaries across folds.
    with SummaryWriter(log_dir=os.path.join(experiment_main_results_dir, "summary_all_folds")) as summary_writer:
        
        # 4. k-fold start/end setup (already in original main)
        args.k_start = max(0, args.k_start)
        args.k_end = args.k if args.k_end == -1 or args.k_end > args.k else args.k_end
        if args.k_start >= args.k_end:
            print(f"Warning: k_start ({args.k_start}) is >= k_end ({args.k_end}). No folds will be run.")
            return

        # 5. Load the dataset
        # GenericMILDataset parameters should be configurable via args
        dataset_instance = get_dataset(
            task_name=args.task_name, # Changed from args.task to args.task_name
            csv_path=getattr(args, "dataset_csv", None), # Make CSV path an arg
            label_col=getattr(args, "label_col", "label"),
            label_dict=None, # Can be inferred or passed as JSON string arg
            slide_id_col=getattr(args, "slide_id_col", "slide_id"),
            seed=args.seed,
            data_dir=args.data_root_dir, # Path to features
            patient_id_col=getattr(args, "patient_id_col", None)
        )
        args.n_classes = dataset_instance.n_classes # Get n_classes from dataset instance
        if not hasattr(args, 'task_type') or args.task_type is None: # Ensure task_type is set
            args.task_type = getattr(args, "default_task_type", "classification") # e.g. "classification" or "survival"

        # 6. Process folds
        print(f"Starting experiment: {args.exp_code} with {args.k}-fold CV from fold {args.k_start} to {args.k_end-1}")
        overall_results_df = run_experiment_folds(dataset_instance, args, experiment_main_results_dir)
        
        # 7. Log aggregated results
        log_results(overall_results_df, args, summary_writer) # log_results should handle writer

if __name__ == "__main__":
    args = get_args()  # Parse arguments

    # Potentially add more argument validation or default setting here if needed
    if not hasattr(args, 'task_name'):
        args.task_name = args.task # Compatibility if 'task' was old name for 'task_name'
    if not hasattr(args, 'k_fold'): # k_fold is number of folds, not boolean
        args.k_fold = args.k 
    
    main_experiment_runner(args)
    print("Experiment Finished!")

import argparse
import os
import numpy as np
import pandas as pd
import torch
from dataset.dataset_generic import GenericMILDataset
from torch.utils.tensorboard import SummaryWriter
from utils.core_utils import train
from utils.file_utils import save_pkl
from sauron.utils.generic_utils import log_results, seed_everything


def get_dataset(task: str, seed: int, data_dir: str = None) -> Dataset:
    if task == "TGCT":
        return GenericMILDataset(
            csv_path="dataset_csv/tgct_processed.csv",
            label_dict={"MGCT": 0, "SEM": 1},
            seed=seed,
            data_dir=data_dir,
        )
    raise ValueError(f"Task not found: {task}")


def process_folds(
    dataset: GenericMILDataset, args: argparse.Namespace, writer: SummaryWriter
) -> pd.DataFrame:
    all_metrics = {
        metric: [] for metric in ["test_auc", "val_auc", "test_acc", "val_acc"]
    }
    dataset.create_splits(k=args.k)

    for i in range(args.k_start, args.k_end):
        dataset.set_splits(i)
        datasets = dataset.return_splits(args.backbone, args.patch_size, from_id=True)

        if args.preloading == "yes":
            for d in datasets:
                d.pre_loading()

        results, *metrics = train(datasets, i, args)

        for metric_name, metric_value in zip(all_metrics.keys(), metrics):
            all_metrics[metric_name].append(metric_value)
            writer.add_scalar(f"{metric_name}/fold_{i}", metric_value, i)

        save_pkl(os.path.join(args.results_dir, f"split_{i}_results.pkl"), results)

    return pd.DataFrame({"folds": range(args.k_start, args.k_end), **all_metrics})


def main(args: argparse.Namespace) -> None:
    os.makedirs(args.results_dir, exist_ok=True)
    writer = SummaryWriter(args.results_dir)

    args.k_start = max(0, args.k_start)
    args.k_end = args.k if args.k_end == -1 else args.k_end

    dataset = get_dataset(args.task, args.seed, args.data_root_dir)
    args.n_classes = len(dataset.label_dict)

    results_df = process_folds(dataset, args, writer)
    log_results(results_df, args, writer)

    writer.close()


if __name__ == "__main__":
    parser = argparse.ArgumentParser(
        description="Configurations for Whole Slide Image (WSI) Training",
        formatter_class=argparse.ArgumentDefaultsHelpFormatter,
    )

    parser.add_argument(
        "--data_root_dir",
        type=str,
        default=None,
        help="Root directory containing the dataset",
    )
    parser.add_argument(
        "--results_dir",
        default="./results",
        help="Directory to save training results and model checkpoints",
    )
    parser.add_argument(
        "--split_dir",
        type=str,
        default=None,
        help="Directory containing custom data splits. If not specified, splits will be inferred from the task and label_frac arguments",
    )

    parser.add_argument(
        "--max_epochs", type=int, default=200, help="Maximum number of epochs to train"
    )
    parser.add_argument(
        "--lr", type=float, default=1e-4, help="Initial learning rate for the optimizer"
    )
    parser.add_argument(
        "--reg",
        type=float,
        default=1e-5,
        help="Weight decay (L2 regularization) factor",
    )
    parser.add_argument(
        "--opt",
        type=str,
        choices=["adam", "sgd"],
        default="adam",
        help="Optimizer to use for training",
    )
    parser.add_argument(
        "--drop_out",
        type=float,
        default=0.25,
        help="Dropout probability for regularization",
    )
    parser.add_argument(
        "--early_stopping",
        action="store_true",
        help="Enable early stopping to prevent overfitting",
    )
    parser.add_argument(
        "--weighted_sample",
        action="store_true",
        help="Enable weighted sampling to handle class imbalance",
    )

    parser.add_argument(
        "--model_type",
        type=str,
        default="att_mil",
        help="Type of model architecture (default: Att mil with single attention branch)",
    )
    parser.add_argument(
        "--backbone",
        type=str,
        default="resnet50",
        help="Backbone network for feature extraction",
    )
    parser.add_argument(
        "--in_dim", type=int, default=1024, help="Input dimension for the model"
    )

    parser.add_argument(
        "--mambamil_rate", type=int, default=10, help="Rate parameter for MambaMIL"
    )
    parser.add_argument(
        "--mambamil_layer", type=int, default=2, help="Number of layers in MambaMIL"
    )
    parser.add_argument(
        "--mambamil_type",
        type=str,
        default="SRMamba",
        choices=["Mamba", "BiMamba", "SRMamba"],
        help="Type of Mamba architecture to use",
    )

    parser.add_argument(
        "--task",
        type=str,
        required=True,
        help="Task name or identifier for the current experiment",
    )
    parser.add_argument(
        "--exp_code",
        type=str,
        required=True,
        help="Unique experiment code for identifying and saving results",
    )
    parser.add_argument(
        "--seed", type=int, default=1, help="Random seed for reproducibility"
    )
    parser.add_argument(
        "--label_frac",
        type=float,
        default=1.0,
        help="Fraction of training labels to use (for semi-supervised learning scenarios)",
    )

    parser.add_argument(
        "--k", type=int, default=10, help="Total number of folds for cross-validation"
    )
    parser.add_argument(
        "--k_start",
        type=int,
        default=-1,
        help="Starting fold for cross-validation (-1 for last fold)",
    )
    parser.add_argument(
        "--k_end",
        type=int,
        default=-1,
        help="Ending fold for cross-validation (-1 for first fold)",
    )

    parser.add_argument(
        "--patch_size",
        type=str,
        default="",
        help="Size of image patches (format: [height]x[width])",
    )
    parser.add_argument(
        "--resolution",
        type=str,
        default="20x",
        help="Magnification level that you want to work with. Ex: 10x, or 10x_40x (combination)",
    )
    parser.add_argument(
        "--early_fusion",
        type=bool,
        default=False,
        help="Enables you to create an early fusion instead of a late fusion of models with multiple magnification levels",
    )

    parser.add_argument(
        "--preloading",
        type=str,
        default="no",
        choices=["yes", "no"],
        help="Whether to preload data into memory",
    )
    parser.add_argument(
        "--log_data",
        action="store_true",
        help="Enable logging of training data using TensorBoard",
    )
    parser.add_argument(
        "--testing", action="store_true", help="Enable testing/debugging mode"
    )

    args = parser.parse_args()
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print("Device is:", device)

    seed_everything(args.seed)

    args.results_dir = os.path.join(args.results_dir, f"{args.exp_code}_s{args.seed}")
    os.makedirs(args.results_dir, exist_ok=True)

    if args.split_dir is None:
        args.split_dir = os.path.join(
            "splits", f"{args.task}_{int(args.label_frac * 100)}"
        )

    assert os.path.isdir(
        args.split_dir
    ), f"Split directory does not exist: {args.split_dir}"

    with open(os.path.join(args.results_dir, "experiment.txt"), "w") as f:
        for key, val in vars(args).items():
            print(f"{key}: {val}", file=f)

    main(args)
    print("Finished!")

Directory structure:
└── sauron/
    ├── README.md
    ├── LICENSE
    ├── feature_extract
    ├── feature_extraction_plan
    ├── requirements.txt
    ├── train_mil.py
    └── sauron/
        ├── requirements.txt
        ├── data/
        │   ├── GenericDataset.py
        │   ├── classMILDataset.py
        │   ├── data_utils.py
        │   ├── dataloader.py
        │   ├── dataset_factory.py
        │   └── survMILDataset.py
        ├── feature_extraction/
        │   ├── processor.py
        │   └── utils/
        │       └── config.py
        ├── losses/
        │   └── surv_loss.py
        ├── mil_models/
        │   ├── ABMIL.py
        │   ├── DiffABMIL.py
        │   ├── MambaMIL.py
        │   ├── MaxMIL.py
        │   ├── MeanMIL.py
        │   ├── S4MIL.py
        │   ├── TransMIL.py
        │   ├── WIKGMIL.py
        │   ├── activations.py
        │   ├── models_factory.py
        │   └── mamba_ssm/
        │       ├── __init__.py
        │       ├── models/
        │       │   ├── __init__.py
        │       │   ├── config_mamba.py
        │       │   └── mixer_seq_simple.py
        │       ├── modules/
        │       │   ├── __init__.py
        │       │   ├── bimamba.py
        │       │   ├── mamba_simple.py
        │       │   └── srmamba.py
        │       ├── ops/
        │       │   ├── __init__.py
        │       │   ├── selective_scan_interface.py
        │       │   └── triton/
        │       │       ├── __init__.py
        │       │       ├── layernorm.py
        │       │       └── selective_state_update.py
        │       └── utils/
        │           ├── __init__.py
        │           └── generation.py
        ├── parse/
        │   └── argparse.py
        ├── preprocess/
        │   ├── compute_embeddings.py
        │   └── slide_embedder.py
        ├── training/
        │   ├── __init__.py
        │   ├── lightning_module.py
        │   └── pipeline.py
        └── utils/
            ├── WSIObjects.py
            ├── callbacks.py
            ├── drawing_utils.py
            ├── environment_setup.py
            ├── filehandler.py
            ├── generic_utils.py
            ├── gpd_utils.py
            ├── hdf5_utils.py
            ├── load_encoders.py
            ├── metrics.py
            ├── optimizers.py
            ├── process_args.py
            ├── segmentation.py
            ├── transform_utils.py
            └── warnings.py

================================================
File: README.md
================================================
# sauron
Multiple Instance Learning framework for multi-modal classification with histopathology data



================================================
File: LICENSE
================================================
# Attribution-NonCommercial-NoDerivatives 4.0 International

> *Creative Commons Corporation (“Creative Commons”) is not a law firm and does not provide legal services or legal advice. Distribution of Creative Commons public licenses does not create a lawyer-client or other relationship. Creative Commons makes its licenses and related information available on an “as-is” basis. Creative Commons gives no warranties regarding its licenses, any material licensed under their terms and conditions, or any related information. Creative Commons disclaims all liability for damages resulting from their use to the fullest extent possible.*
>
> ### Using Creative Commons Public Licenses
>
> Creative Commons public licenses provide a standard set of terms and conditions that creators and other rights holders may use to share original works of authorship and other material subject to copyright and certain other rights specified in the public license below. The following considerations are for informational purposes only, are not exhaustive, and do not form part of our licenses.
>
> * __Considerations for licensors:__ Our public licenses are intended for use by those authorized to give the public permission to use material in ways otherwise restricted by copyright and certain other rights. Our licenses are irrevocable. Licensors should read and understand the terms and conditions of the license they choose before applying it. Licensors should also secure all rights necessary before applying our licenses so that the public can reuse the material as expected. Licensors should clearly mark any material not subject to the license. This includes other CC-licensed material, or material used under an exception or limitation to copyright. [More considerations for licensors](http://wiki.creativecommons.org/Considerations_for_licensors_and_licensees#Considerations_for_licensors).
>
> * __Considerations for the public:__ By using one of our public licenses, a licensor grants the public permission to use the licensed material under specified terms and conditions. If the licensor’s permission is not necessary for any reason–for example, because of any applicable exception or limitation to copyright–then that use is not regulated by the license. Our licenses grant only permissions under copyright and certain other rights that a licensor has authority to grant. Use of the licensed material may still be restricted for other reasons, including because others have copyright or other rights in the material. A licensor may make special requests, such as asking that all changes be marked or described. Although not required by our licenses, you are encouraged to respect those requests where reasonable. [More considerations for the public](http://wiki.creativecommons.org/Considerations_for_licensors_and_licensees#Considerations_for_licensees).

## Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International Public License

By exercising the Licensed Rights (defined below), You accept and agree to be bound by the terms and conditions of this Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International Public License ("Public License"). To the extent this Public License may be interpreted as a contract, You are granted the Licensed Rights in consideration of Your acceptance of these terms and conditions, and the Licensor grants You such rights in consideration of benefits the Licensor receives from making the Licensed Material available under these terms and conditions.

### Section 1 – Definitions.

a. __Adapted Material__ means material subject to Copyright and Similar Rights that is derived from or based upon the Licensed Material and in which the Licensed Material is translated, altered, arranged, transformed, or otherwise modified in a manner requiring permission under the Copyright and Similar Rights held by the Licensor. For purposes of this Public License, where the Licensed Material is a musical work, performance, or sound recording, Adapted Material is always produced where the Licensed Material is synched in timed relation with a moving image.

b. __Copyright and Similar Rights__ means copyright and/or similar rights closely related to copyright including, without limitation, performance, broadcast, sound recording, and Sui Generis Database Rights, without regard to how the rights are labeled or categorized. For purposes of this Public License, the rights specified in Section 2(b)(1)-(2) are not Copyright and Similar Rights.

e. __Effective Technological Measures__ means those measures that, in the absence of proper authority, may not be circumvented under laws fulfilling obligations under Article 11 of the WIPO Copyright Treaty adopted on December 20, 1996, and/or similar international agreements.

f. __Exceptions and Limitations__ means fair use, fair dealing, and/or any other exception or limitation to Copyright and Similar Rights that applies to Your use of the Licensed Material.

h. __Licensed Material__ means the artistic or literary work, database, or other material to which the Licensor applied this Public License.

i. __Licensed Rights__ means the rights granted to You subject to the terms and conditions of this Public License, which are limited to all Copyright and Similar Rights that apply to Your use of the Licensed Material and that the Licensor has authority to license.

h. __Licensor__ means the individual(s) or entity(ies) granting rights under this Public License.

i. __NonCommercial__ means not primarily intended for or directed towards commercial advantage or monetary compensation. For purposes of this Public License, the exchange of the Licensed Material for other material subject to Copyright and Similar Rights by digital file-sharing or similar means is NonCommercial provided there is no payment of monetary compensation in connection with the exchange.

j. __Share__ means to provide material to the public by any means or process that requires permission under the Licensed Rights, such as reproduction, public display, public performance, distribution, dissemination, communication, or importation, and to make material available to the public including in ways that members of the public may access the material from a place and at a time individually chosen by them.

k. __Sui Generis Database Rights__ means rights other than copyright resulting from Directive 96/9/EC of the European Parliament and of the Council of 11 March 1996 on the legal protection of databases, as amended and/or succeeded, as well as other essentially equivalent rights anywhere in the world.

l. __You__ means the individual or entity exercising the Licensed Rights under this Public License. Your has a corresponding meaning.

### Section 2 – Scope.

a. ___License grant.___

   1. Subject to the terms and conditions of this Public License, the Licensor hereby grants You a worldwide, royalty-free, non-sublicensable, non-exclusive, irrevocable license to exercise the Licensed Rights in the Licensed Material to:

        A. reproduce and Share the Licensed Material, in whole or in part, for NonCommercial purposes only; and

        B. produce and reproduce, but not Share, Adapted Material for NonCommercial purposes only.

   2. __Exceptions and Limitations.__ For the avoidance of doubt, where Exceptions and Limitations apply to Your use, this Public License does not apply, and You do not need to comply with its terms and conditions.

   3. __Term.__ The term of this Public License is specified in Section 6(a).

   4. __Media and formats; technical modifications allowed.__ The Licensor authorizes You to exercise the Licensed Rights in all media and formats whether now known or hereafter created, and to make technical modifications necessary to do so. The Licensor waives and/or agrees not to assert any right or authority to forbid You from making technical modifications necessary to exercise the Licensed Rights, including technical modifications necessary to circumvent Effective Technological Measures. For purposes of this Public License, simply making modifications authorized by this Section 2(a)(4) never produces Adapted Material.

   5. __Downstream recipients.__

        A. __Offer from the Licensor – Licensed Material.__ Every recipient of the Licensed Material automatically receives an offer from the Licensor to exercise the Licensed Rights under the terms and conditions of this Public License.

        B. __No downstream restrictions.__ You may not offer or impose any additional or different terms or conditions on, or apply any Effective Technological Measures to, the Licensed Material if doing so restricts exercise of the Licensed Rights by any recipient of the Licensed Material.

   6. __No endorsement.__ Nothing in this Public License constitutes or may be construed as permission to assert or imply that You are, or that Your use of the Licensed Material is, connected with, or sponsored, endorsed, or granted official status by, the Licensor or others designated to receive attribution as provided in Section 3(a)(1)(A)(i).

b. ___Other rights.___

   1. Moral rights, such as the right of integrity, are not licensed under this Public License, nor are publicity, privacy, and/or other similar personality rights; however, to the extent possible, the Licensor waives and/or agrees not to assert any such rights held by the Licensor to the limited extent necessary to allow You to exercise the Licensed Rights, but not otherwise.

   2. Patent and trademark rights are not licensed under this Public License.

   3. To the extent possible, the Licensor waives any right to collect royalties from You for the exercise of the Licensed Rights, whether directly or through a collecting society under any voluntary or waivable statutory or compulsory licensing scheme. In all other cases the Licensor expressly reserves any right to collect such royalties, including when the Licensed Material is used other than for NonCommercial purposes.

### Section 3 – License Conditions.

Your exercise of the Licensed Rights is expressly made subject to the following conditions.

a. ___Attribution.___

   1. If You Share the Licensed Material, You must:

      A. retain the following if it is supplied by the Licensor with the Licensed Material:

         i. identification of the creator(s) of the Licensed Material and any others designated to receive attribution, in any reasonable manner requested by the Licensor (including by pseudonym if designated);

         ii. a copyright notice;

         iii. a notice that refers to this Public License;

         iv. a notice that refers to the disclaimer of warranties;

         v. a URI or hyperlink to the Licensed Material to the extent reasonably practicable;

      B. indicate if You modified the Licensed Material and retain an indication of any previous modifications; and

      C. indicate the Licensed Material is licensed under this Public License, and include the text of, or the URI or hyperlink to, this Public License.
 
        For the avoidance of doubt, You do not have permission under this Public License to Share Adapted Material.

   2. You may satisfy the conditions in Section 3(a)(1) in any reasonable manner based on the medium, means, and context in which You Share the Licensed Material. For example, it may be reasonable to satisfy the conditions by providing a URI or hyperlink to a resource that includes the required information.

   3. If requested by the Licensor, You must remove any of the information required by Section 3(a)(1)(A) to the extent reasonably practicable.

### Section 4 – Sui Generis Database Rights.

Where the Licensed Rights include Sui Generis Database Rights that apply to Your use of the Licensed Material:

a. for the avoidance of doubt, Section 2(a)(1) grants You the right to extract, reuse, reproduce, and Share all or a substantial portion of the contents of the database for NonCommercial purposes only and provided You do not Share Adapted Material;

b. if You include all or a substantial portion of the database contents in a database in which You have Sui Generis Database Rights, then the database in which You have Sui Generis Database Rights (but not its individual contents) is Adapted Material; and

c. You must comply with the conditions in Section 3(a) if You Share all or a substantial portion of the contents of the database.

For the avoidance of doubt, this Section 4 supplements and does not replace Your obligations under this Public License where the Licensed Rights include other Copyright and Similar Rights.

### Section 5 – Disclaimer of Warranties and Limitation of Liability.

a. __Unless otherwise separately undertaken by the Licensor, to the extent possible, the Licensor offers the Licensed Material as-is and as-available, and makes no representations or warranties of any kind concerning the Licensed Material, whether express, implied, statutory, or other. This includes, without limitation, warranties of title, merchantability, fitness for a particular purpose, non-infringement, absence of latent or other defects, accuracy, or the presence or absence of errors, whether or not known or discoverable. Where disclaimers of warranties are not allowed in full or in part, this disclaimer may not apply to You.__

b. __To the extent possible, in no event will the Licensor be liable to You on any legal theory (including, without limitation, negligence) or otherwise for any direct, special, indirect, incidental, consequential, punitive, exemplary, or other losses, costs, expenses, or damages arising out of this Public License or use of the Licensed Material, even if the Licensor has been advised of the possibility of such losses, costs, expenses, or damages. Where a limitation of liability is not allowed in full or in part, this limitation may not apply to You.__

c. The disclaimer of warranties and limitation of liability provided above shall be interpreted in a manner that, to the extent possible, most closely approximates an absolute disclaimer and waiver of all liability.

### Section 6 – Term and Termination.

a. This Public License applies for the term of the Copyright and Similar Rights licensed here. However, if You fail to comply with this Public License, then Your rights under this Public License terminate automatically.

b. Where Your right to use the Licensed Material has terminated under Section 6(a), it reinstates:

   1. automatically as of the date the violation is cured, provided it is cured within 30 days of Your discovery of the violation; or

   2. upon express reinstatement by the Licensor.

   For the avoidance of doubt, this Section 6(b) does not affect any right the Licensor may have to seek remedies for Your violations of this Public License.

c. For the avoidance of doubt, the Licensor may also offer the Licensed Material under separate terms or conditions or stop distributing the Licensed Material at any time; however, doing so will not terminate this Public License.

d. Sections 1, 5, 6, 7, and 8 survive termination of this Public License.

### Section 7 – Other Terms and Conditions.

a. The Licensor shall not be bound by any additional or different terms or conditions communicated by You unless expressly agreed.

b. Any arrangements, understandings, or agreements regarding the Licensed Material not stated herein are separate from and independent of the terms and conditions of this Public License.

### Section 8 – Interpretation.

a. For the avoidance of doubt, this Public License does not, and shall not be interpreted to, reduce, limit, restrict, or impose conditions on any use of the Licensed Material that could lawfully be made without permission under this Public License.

b. To the extent possible, if any provision of this Public License is deemed unenforceable, it shall be automatically reformed to the minimum extent necessary to make it enforceable. If the provision cannot be reformed, it shall be severed from this Public License without affecting the enforceability of the remaining terms and conditions.

c. No term or condition of this Public License will be waived and no failure to comply consented to unless expressly agreed to by the Licensor.

d. Nothing in this Public License constitutes or may be interpreted as a limitation upon, or waiver of, any privileges and immunities that apply to the Licensor or You, including from the legal processes of any jurisdiction or authority.

> Creative Commons is not a party to its public licenses. Notwithstanding, Creative Commons may elect to apply one of its public licenses to material it publishes and in those instances will be considered the “Licensor.” Except for the limited purpose of indicating that material is shared under a Creative Commons public license or as otherwise permitted by the Creative Commons policies published at [creativecommons.org/policies](http://creativecommons.org/policies), Creative Commons does not authorize the use of the trademark “Creative Commons” or any other trademark or logo of Creative Commons without its prior written consent including, without limitation, in connection with any unauthorized modifications to any of its public licenses or any other arrangements, understandings, or agreements concerning use of licensed material. For the avoidance of doubt, this paragraph does not form part of the public licenses.
>
> Creative Commons may be contacted at [creativecommons.org](http://creativecommons.org).


================================================
File: feature_extract
================================================
import argparse
import logging
import os
import sys
from termcolor import colored

import openslide
from sauron.preprocess.slide_embedder import SlideEmbedder

# Configure logger
logging.basicConfig(
    level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s"
)
logger = logging.getLogger(__name__)


if __name__ == "__main__":
    parser = argparse.ArgumentParser(
        description="Process whole slide images for tissue segmentation, patch extraction, and feature embedding."
    )

    parser.add_argument(
        "--slide_dir",
        type=str,
        required=True,
        default="./slides",
        help="Path to the directory containing the slide images. This parameter is required.",
    )
    parser.add_argument(
        "--output_dir",
        type=str,
        default="./output",
        help="Directory where the results (tissue segmentation, patch coordinates, and patch embeddings) will be saved. Defaults to './../data/downstream'.",
    )
    parser.add_argument(
        "--mag",
        type=int,
        default=20,
        help="Magnification level for patch extraction. Defaults to 20x.",
    )
    parser.add_argument(
        "--patch_size",
        type=int,
        default=224,
        help="Size of the patches to be extracted. Defaults to 224.",
    )
    parser.add_argument(
        "--batch_size",
        type=int,
        default=32,
        help="Batch size for processing slides. Defaults to 32.",
    )
    parser.add_argument(
        "--encoder",
        type=str,
        default="conch_ViT-B-16",
        help="Encoder to use for feature extraction. Defaults to conch_ViT-B-16.",
    )


    args = parser.parse_args()

    logger.info(colored("Initiating processing of slides...", "green"))

    embedder = SlideEmbedder(args.slide_dir, args.output_dir, args.mag, args.patch_size, args.encoder)
    embedder.process_slides()



================================================
File: feature_extraction_plan
================================================
sauron/
├── __init__.py
└── feature_extraction/
    ├── __init__.py
    ├── processor.py
    ├── visualization.py
    ├── conversion.py
    ├── wsi/
    │   ├── __init__.py
    │   ├── base.py
    │   ├── openslide.py
    │   ├── cucim.py
    │   ├── image.py
    │   ├── factory.py
    │   ├── patching.py
    │   └── dataset.py
    ├── models/
    │   ├── __init__.py
    │   ├── patch_encoders/
    │   │   ├── __init__.py
    │   │   ├── factory.py
    │   │   ├── checkpoints.json
    │   │   ├── zoo/
    │   │   │   ├── __init__.py
    │   │   │   └── ...
    │   │   └── utils/
    │   │       ├── __init__.py
    │   │       ├── constants.py
    │   │       └── transforms.py
    │   ├── slide_encoders/
    │   │   ├── __init__.py
    │   │   ├── factory.py
    │   │   ├── checkpoints.json
    │   │   └── zoo/
    │   │       ├── __init__.py
    │   │       └── ...
    │   └── segmentation/
    │       ├── __init__.py
    │       ├── factory.py
    │       └── checkpoints.json
    └── utils/
        ├── __init__.py
        ├── io.py
        ├── h5_utils.py
        ├── geo_utils.py
        ├── config.py
        └── misc.py


================================================
File: requirements.txt
================================================
termcolor



================================================
File: train_mil.py
================================================
# train_mil.py
import argparse
import json
import os

import pandas as pd
from torch.utils.tensorboard import SummaryWriter

from sauron.data.dataset_factory import determine_split_directory, get_data_manager

# Sauron specific imports
from sauron.parse.argparse import get_args  # Assuming get_args is here
from sauron.training.pipeline import train_fold
from sauron.utils.environment_setup import (
    create_results_directory,
    log_experiment_details,
    seed_everything,
    setup_device,
)
from sauron.utils.generic_utils import (  # Assuming these are general utils
    log_results,
    save_pkl,
)


def run_experiment_folds(
    data_manager,  # Now takes a DataManager instance
    args: argparse.Namespace,
    experiment_main_results_dir: str,
) -> pd.DataFrame:
    """
    Manages the k-fold cross-validation loop.
    """
    if args.task_type.lower() == "classification":
        metric_keys = [
            "test_auc",
            "val_auc",
            "test_acc",
            "val_acc",
        ]  # Expected from train_fold
    elif args.task_type.lower() == "survival":
        metric_keys = [
            "test_c_index",
            "val_c_index",
        ]  # Example, ensure train_fold returns these
    else:
        raise ValueError(f"Unknown task_type: {args.task_type} for defining metrics.")

    all_fold_metrics = {key: [] for key in metric_keys}

    # DataManager creates and manages its own splits internally
    # For ClassificationDataManager:
    if args.task_type.lower() == "classification":
        data_manager.create_k_fold_splits(
            num_folds=args.k, test_set_size=getattr(args, "test_frac", 0.1)
        )
        num_actual_folds = data_manager.get_number_of_folds()
        if num_actual_folds == 0 and args.k > 0:  # No k-folds, but test set might exist
            print(
                "No K-folds generated (num_folds=0 in DataManager), running as single train/test split if test_frac > 0."
            )
            # In this case, loop runs once if test_set_size > 0, setting fold_idx=0
            if args.k <= 1:  # Consider 0 or 1 fold as a single run (train/val/test)
                loop_range = range(1)  # Run once for the main split
                print(f"Running a single train/val/test split (args.k={args.k}).")
            else:  # k > 1 but num_actual_folds is 0 - this is an issue
                raise ValueError(
                    f"args.k={args.k} but DataManager created 0 folds. Check data or split logic."
                )
        else:
            loop_range = range(args.k_start, min(args.k_end, num_actual_folds))

    # For SurvivalDataManager:
    elif args.task_type.lower() == "survival":
        # Create splits using the survival-specific function
        data_manager.create_splits_from_generating_function(
            k=args.k,
            val_num=getattr(args, "val_num_survival", (0.15, 0.15)),
            test_num=getattr(args, "test_num_survival", (0.15, 0.15)),
            label_frac=getattr(args, "label_frac", 1.0),
            custom_test_ids=getattr(args, "custom_test_ids", None),
        )
        loop_range = range(args.k_start, args.k_end)
    else:
        raise ValueError(f"Task type {args.task_type} split creation not defined.")

    for i in loop_range:
        print(f"\n{'='*10} Processing Fold: {i} {'='*10}")

        # Set the current fold in the DataManager
        if args.task_type.lower() == "classification":
            data_manager.set_current_fold(fold_index=i)
        elif args.task_type.lower() == "survival":
            if not data_manager.set_next_fold_from_generator(
                start_from_fold=i if i == args.k_start else None
            ):
                print(
                    f"SurvivalDataManager's split generator exhausted before reaching fold {i}."
                )
                break

        # Get MILDataset instances for the current fold
        # Common MILDataset parameters from args
        mil_dataset_params = {
            "backbone": args.backbone,
            "patch_size": args.patch_size,
            "use_hdf5": getattr(args, "use_hdf5", False),
            "cache_enabled": getattr(args, "preloading", "no").lower() == "yes",
        }

        # Add survival-specific parameters if needed
        if args.task_type.lower() == "survival":
            mil_dataset_params["mode"] = getattr(args, "survival_mode", "pathomic")

        train_dataset, val_dataset, test_dataset = data_manager.get_mil_datasets(
            **mil_dataset_params
        )

        # Preload data if requested
        if getattr(args, "preloading", "no").lower() == "yes":
            print(f"Preloading data for fold {i}...")
            if train_dataset:
                train_dataset.preload_data()
            if val_dataset:
                val_dataset.preload_data()
            if test_dataset:
                test_dataset.preload_data()

        # Save current split patient IDs if requested
        if getattr(args, "save_splits", False):
            split_file = os.path.join(
                args.split_dir_determined, f"fold_{i}_patient_ids.csv"
            )
            data_manager.save_current_split_patient_ids(split_file)

        # Get fold-specific results directory
        fold_results_dir = os.path.join(experiment_main_results_dir, f"fold_{i}")
        os.makedirs(fold_results_dir, exist_ok=True)

        # Train the model for this fold
        fold_results_tuple = train_fold(
            train_dataset=train_dataset,
            val_dataset=val_dataset,  # Add val_dataset if train_fold expects it
            test_dataset=test_dataset,
            cur_fold_num=i,
            args=args,
            experiment_base_results_dir=fold_results_dir,
        )

        patient_level_results_dict, *fold_metrics_values = fold_results_tuple

        if len(fold_metrics_values) != len(metric_keys):
            raise ValueError(
                f"Mismatch in metrics from train_fold ({len(fold_metrics_values)}) vs expected ({len(metric_keys)}). "
                f"Task: {args.task_type}. Returned: {fold_metrics_values}"
            )

        for key_idx, metric_value in enumerate(fold_metrics_values):
            metric_name = metric_keys[key_idx]
            all_fold_metrics[metric_name].append(metric_value)

        if patient_level_results_dict:
            fold_results_pkl_path = os.path.join(
                fold_results_dir, "patient_level_results.pkl"
            )
            save_pkl(fold_results_pkl_path, patient_level_results_dict)
            print(
                f"Saved patient-level results for fold {i} to {fold_results_pkl_path}"
            )

    # Check if any folds were actually run
    num_folds_run = (
        len(all_fold_metrics[metric_keys[0]])
        if metric_keys and all_fold_metrics[metric_keys[0]]
        else 0
    )
    if num_folds_run == 0:
        print("No folds were successfully processed. Returning empty DataFrame.")
        return pd.DataFrame()

    fold_num_series = list(range(args.k_start, args.k_start + num_folds_run))
    return pd.DataFrame({"fold_num": fold_num_series, **all_fold_metrics})


def main_experiment_runner(args: argparse.Namespace):
    """
    Main function to run the entire experiment.
    """
    _ = setup_device()
    seed_everything(args.seed)

    experiment_main_results_dir = create_results_directory(
        args.results_dir, args.exp_code, args.seed
    )
    args.results_dir = experiment_main_results_dir  # Update args for train_fold
    log_experiment_details(args, experiment_main_results_dir)

    # Determine split directory (where DataManager might save split definitions if asked)
    # This is more for organizing outputs than for DataManager to read from, unless you implement that.
    args.split_dir_determined = determine_split_directory(  # Store in a new arg field
        getattr(
            args, "split_dir_base", None
        ),  # Base for auto-generated split dir names
        args.task_name,
        getattr(args, "label_frac", 1.0),
        args.k > 1,  # k_fold is true if num_folds > 1
    )
    os.makedirs(args.split_dir_determined, exist_ok=True)

    with SummaryWriter(
        log_dir=os.path.join(experiment_main_results_dir, "summary_all_folds")
    ) as summary_writer:
        args.k_start = max(0, args.k_start)
        # args.k_end should be such that loop runs for args.k folds, or up to actual num_folds
        # If args.k is the total number of folds intended:
        args.k_end = args.k if args.k_end == -1 or args.k_end > args.k else args.k_end

        if args.k_start >= args.k_end and args.k > 0:  # k_end is exclusive
            print(
                f"Warning: k_start ({args.k_start}) is >= k_end ({args.k_end}). No folds will be run."
            )
            return

        manager_params = {
            "task_name": args.task_name,
            "task_type": args.task_type,
            "csv_path": args.dataset_csv,
            "data_directory": args.data_root_dir,
            "seed": args.seed,
            "verbose": getattr(args, "verbose_data", True),
            # Classification specific from args
            "label_column": getattr(args, "label_col", "label"),
            "patient_id_col_name": getattr(args, "patient_id_col", "case_id"),
            "slide_id_col_name": getattr(args, "slide_id_col", "slide_id"),
            "filter_criteria": json.loads(args.filter_criteria)
            if hasattr(args, "filter_criteria") and args.filter_criteria
            else None,
            "ignore_labels": args.ignore_labels.split(",")
            if hasattr(args, "ignore_labels") and args.ignore_labels
            else None,
            "patient_label_aggregation": getattr(
                args, "patient_label_aggregation", "max"
            ),
            "shuffle": getattr(
                args, "shuffle_data", False
            ),  # For ClassificationDataManager initial load
            # Survival specific from args
            "time_column": getattr(args, "time_col", None),
            "event_column": getattr(args, "event_col", None),
            "n_bins": getattr(args, "n_bins_survival", 4),
            "filter_dict": json.loads(args.filter_dict_survival)
            if hasattr(args, "filter_dict_survival") and args.filter_dict_survival
            else None,
            "omic_csv_path": getattr(args, "omic_csv", None),
            "omic_patient_id_col": getattr(args, "omic_patient_id_col", "case_id"),
            "apply_sig": getattr(args, "apply_sig_survival", False),
            "signatures_csv_path": getattr(args, "signatures_csv", None),
            "shuffle_slide_data": getattr(
                args, "shuffle_data_survival", False
            ),  # For SurvivalDataManager
        }

        # Parse label_mapping if provided as a JSON string
        label_mapping_str = getattr(args, "label_mapping", None)
        if label_mapping_str:
            try:
                manager_params["label_mapping"] = json.loads(label_mapping_str)
            except json.JSONDecodeError:
                raise ValueError(
                    f"Invalid JSON string for label_mapping: {label_mapping_str}"
                )
        else:
            manager_params["label_mapping"] = None

        data_manager_instance = get_data_manager(**manager_params)

        # Get n_classes from DataManager and set it in args for train_fold
        args.n_classes = data_manager_instance.num_classes
        print(f"DataManager initialized. Number of classes: {args.n_classes}")
        if args.n_classes == 0 and args.task_type.lower() == "classification":
            print(
                "Warning: Number of classes is 0 for classification task. Check data and label mapping."
            )
            # Potentially stop if this is critical, or let it proceed if some datasets might be empty
            # For now, let it proceed, but train_fold might fail.

        print(
            f"Starting experiment: {args.exp_code} with up to {args.k} folds (from {args.k_start} to {args.k_end-1})"
        )
        overall_results_df = run_experiment_folds(
            data_manager_instance, args, experiment_main_results_dir
        )

        if not overall_results_df.empty:
            log_results(overall_results_df, args, summary_writer)
            print("\nAggregated Results over Folds:")
            print(overall_results_df)
        else:
            print("No results to log as no folds were processed.")


if __name__ == "__main__":
    args = get_args()  # Parse arguments

    # Potentially add more argument validation or default setting here if needed
    if not hasattr(args, "task_name"):
        args.task_name = (
            args.task
        )  # Compatibility if 'task' was old name for 'task_name'
    if not hasattr(args, "k_fold"):  # k_fold is number of folds, not boolean
        args.k_fold = args.k

    main_experiment_runner(args)
    print("Experiment Finished!")



================================================
File: sauron/requirements.txt
================================================
# requirements.txt

# Core ML
torch>=2.0.0,<2.1.0 # Specify a range known to work with mamba-ssm/causal-conv1d
torchvision
torchaudio

# Mamba Dependencies (ensure mamba-ssm is installed separately as per its instructions)
# pip install causal-conv1d==1.1.1
# pip install packaging
# cd mamba && pip install .
einops
ninja # Often needed for C++ extensions
packaging
transformers # Used by mamba utils/hf.py and benchmarks

# Data & Metrics
numpy
pandas>=2.0.0,<2.3.0
scikit-learn
scikit-survival==0.22.2
lifelines # Used in original MambaMIL README

# Utilities
tqdm
h5py

# Logging & Visualization (Optional but recommended)
tensorboard
tensorboardX
matplotlib
seaborn
# wandb # Uncomment if using Weights & Biases

# Needed by trident (if used for feature extraction prior to this codebase)
openslide-python
Pillow
opencv-python-headless
geopandas
shapely
pyvips # If using trident's Converter

# Other potential dependencies based on imports observed
opt_einsum # Used in S4MIL


================================================
File: sauron/data/GenericDataset.py
================================================
import os

import numpy as np
from PIL import Image
from torch.utils.data import Dataset
from tqdm import tqdm

from sauron.utils.WSIObjects import WSIPatcher


class PatchFileDataset(Dataset):
    def __init__(self, directory_path, transform=None):
        self.transform = transform
        self.patch_paths, self.coordinates = self._load_patch_paths(directory_path)

    def _load_patch_paths(self, directory_path):
        patch_paths = []
        coordinates = []
        for filename in tqdm(os.listdir(directory_path), desc="Loading patches"):
            try:
                name, _ = os.path.splitext(filename)
                x_coord, y_coord = map(int, name.split("_")[:2])
                patch_paths.append(os.path.join(directory_path, filename))
                coordinates.append((x_coord, y_coord))
            except (ValueError, IndexError) as e:
                raise ValueError(f"Invalid filename format: {filename}") from e
        return patch_paths, coordinates

    def __len__(self):
        return len(self.patch_paths)

    def __getitem__(self, index):
        try:
            with Image.open(self.patch_paths[index]) as patch:
                patch_array = np.array(patch)
                coord = self.coordinates[index]
        except IndexError as e:
            raise IndexError(f"Index {index} out of range") from e

        if self.transform:
            patch_array = self.transform(patch_array)

        return patch_array, coord


class WholeSlideImageDataset(Dataset):
    def __init__(self, patcher: WSIPatcher, transform=None):
        self.patcher = patcher
        self.transform = transform
        self.cols, self.rows = self.patcher.get_cols_rows()

    def __len__(self):
        return len(self.patcher)

    def __getitem__(self, index):
        try:
            col = index % self.cols
            row = index // self.cols
            tile, x, y = self.patcher.get_tile(col, row)
        except IndexError as e:
            raise IndexError(f"Index {index} out of range") from e

        if self.transform:
            tile = self.transform(tile)

        return tile, (x, y)



================================================
File: sauron/data/classMILDataset.py
================================================
from __future__ import annotations

import os
from typing import Dict, List, Optional, Tuple, Union

import h5py
import numpy as np
import pandas as pd
import torch
from scipy.stats import mode
from sklearn.model_selection import StratifiedKFold, train_test_split
from torch.utils.data import Dataset


class ClassificationDataManager:
    def __init__(
        self,
        csv_path: str,
        data_directory: Union[str, Dict[str, str]],
        label_column: str = "label",  # Actual column name in CSV for labels
        label_mapping: Optional[Dict[str, int]] = None,
        patient_id_col_name: str = "case_id",  # Actual col name in CSV for patient ID
        slide_id_col_name: str = "slide_id",  # Actual col name in CSV for slide ID
        shuffle: bool = False,
        random_seed: int = 7,
        verbose: bool = True,
        filter_criteria: Optional[Dict[str, List[str]]] = None,
        ignore_labels: Optional[List[str]] = None,  # Original string labels to ignore
        patient_stratification: bool = False,  # If true, __len__ uses patient_data
        patient_label_aggregation: str = "max",  # 'max' or 'majority'
    ):
        self.csv_path = csv_path
        self.data_directory = data_directory
        self.provided_label_column = label_column
        self.label_mapping = label_mapping or {}
        self.patient_id_col_name = patient_id_col_name
        self.slide_id_col_name = slide_id_col_name
        self.random_seed = random_seed
        self.verbose = verbose
        self.patient_stratification = (
            patient_stratification  # Used by __len__ if this class were a Dataset
        )

        self.train_patient_ids: Optional[List[str]] = None
        self.val_patient_ids: Optional[List[str]] = None
        self.test_patient_ids: Optional[List[str]] = None

        self.train_slide_indices: Optional[List[int]] = None
        self.val_slide_indices: Optional[List[int]] = None
        self.test_slide_indices: Optional[List[int]] = None

        self.kfold_splits: Optional[List[Tuple[np.ndarray, np.ndarray]]] = None
        self.train_val_patient_data_for_kfold: Optional[pd.DataFrame] = None

        # Load and preprocess slide data
        try:
            raw_slide_data = pd.read_csv(csv_path)
        except FileNotFoundError as e:
            raise FileNotFoundError(f"CSV file not found: {csv_path}") from e

        # Standardize column names internally
        self.slide_data = self._rename_cols(raw_slide_data)

        self.slide_data = self._filter_data(self.slide_data, filter_criteria)
        self.slide_data = self._prepare_labels(self.slide_data, ignore_labels)

        if not self.label_mapping:  # Infer mapping if not provided
            unique_labels = self.slide_data["label_str"].unique()
            self.label_mapping = {
                label: i for i, label in enumerate(sorted(unique_labels))
            }
            if verbose:
                print(f"Inferred label_mapping: {self.label_mapping}")

        # Apply the final mapping after potential inference
        self.slide_data["label"] = self.slide_data["label_str"].map(self.label_mapping)
        # Drop rows where mapping resulted in NaN (e.g. label_str not in inferred map)
        self.slide_data.dropna(subset=["label"], inplace=True)
        self.slide_data["label"] = self.slide_data["label"].astype(int)

        self.num_classes = len(set(self.label_mapping.values()))
        if self.num_classes == 0 and len(self.slide_data) > 0:
            raise ValueError(
                "Number of classes is 0. Check label_column and label_mapping. "
                f"Unique labels found: {self.slide_data['label_str'].unique()}"
            )

        if shuffle:
            self.slide_data = self.slide_data.sample(
                frac=1, random_state=random_seed
            ).reset_index(drop=True)

        self._aggregate_patient_data(patient_label_aggregation)
        self._prepare_class_indices()

        if verbose:
            self._print_summary()

    def _rename_cols(self, df: pd.DataFrame) -> pd.DataFrame:
        df = df.copy()
        if (
            self.patient_id_col_name != "case_id"
            and self.patient_id_col_name in df.columns
        ):
            df.rename(columns={self.patient_id_col_name: "case_id"}, inplace=True)
        if (
            self.slide_id_col_name != "slide_id"
            and self.slide_id_col_name in df.columns
        ):
            df.rename(columns={self.slide_id_col_name: "slide_id"}, inplace=True)
        if "case_id" not in df.columns:
            raise ValueError(
                f"Patient ID column '{self.patient_id_col_name}' (expected 'case_id') not found in CSV."
            )
        if "slide_id" not in df.columns:
            raise ValueError(
                f"Slide ID column '{self.slide_id_col_name}' (expected 'slide_id') not found in CSV."
            )
        return df

    def _filter_data(
        self, data: pd.DataFrame, filter_criteria: Optional[Dict[str, List[str]]] = None
    ) -> pd.DataFrame:
        if filter_criteria:
            mask = pd.Series(True, index=data.index)
            for column, values in filter_criteria.items():
                if column not in data.columns:
                    print(f"Warning: Filter column '{column}' not found in data.")
                    continue
                mask &= data[column].isin(values)
            data = data[mask].reset_index(drop=True)
        return data

    def _prepare_labels(
        self, data: pd.DataFrame, ignore_labels_str: Optional[List[str]]
    ) -> pd.DataFrame:
        data = data.copy()
        if self.provided_label_column not in data.columns:
            raise ValueError(
                f"Provided label column '{self.provided_label_column}' not found in CSV."
            )

        # Keep original string labels in 'label_str' for mapping and ignoring
        data["label_str"] = data[self.provided_label_column].astype(str)

        if ignore_labels_str:
            data = data[~data["label_str"].isin(ignore_labels_str)].reset_index(
                drop=True
            )

        # Integer mapping will be applied after potential inference in __init__
        return data

    def _aggregate_patient_data(self, aggregation_method: str = "max") -> None:
        if (
            "case_id" not in self.slide_data.columns
            or "label" not in self.slide_data.columns
        ):
            print(
                "Warning: 'case_id' or 'label' not fully prepared for patient aggregation."
            )
            self.patient_data = pd.DataFrame(columns=["case_id", "label"])
            return

        patients = self.slide_data["case_id"].unique()
        patient_labels_agg = []

        for patient_id in patients:
            labels = self.slide_data.loc[
                self.slide_data["case_id"] == patient_id, "label"
            ].values
            if len(labels) == 0:
                continue  # Should not happen if patient_id is from slide_data

            if aggregation_method == "max":
                aggregated_label = labels.max()
            elif aggregation_method == "majority":
                aggregated_label = mode(labels, keepdims=True).mode[0]
            else:
                raise ValueError(
                    f"Invalid patient_label_aggregation: {aggregation_method}"
                )
            patient_labels_agg.append(aggregated_label)

        self.patient_data = pd.DataFrame(
            {"case_id": patients, "label": patient_labels_agg}
        )

    def _prepare_class_indices(self) -> None:
        if self.num_classes > 0:
            self.patient_class_indices = [
                np.where(self.patient_data["label"] == cls_label)[0]
                for cls_label in range(self.num_classes)
            ]
            self.slide_class_indices = [
                np.where(self.slide_data["label"] == cls_label)[0]
                for cls_label in range(self.num_classes)
            ]
        else:
            self.patient_class_indices = []
            self.slide_class_indices = []

    def _print_summary(self) -> None:
        print("--- Classification DataManager Summary ---")
        print(f"CSV Path: {self.csv_path}")
        print(f"Label Column (original): {self.provided_label_column}")
        print(f"Patient ID Column (original): {self.patient_id_col_name}")
        print(f"Slide ID Column (original): {self.slide_id_col_name}")
        print(f"Label Mapping: {self.label_mapping}")
        print(f"Number of Classes: {self.num_classes}")
        print(f"Total unique slides: {self.slide_data['slide_id'].nunique()}")
        print(f"Total unique patients: {self.patient_data['case_id'].nunique()}")

        if self.num_classes > 0:
            print("\nSlide-level counts (after mapping):")
            print(self.slide_data["label"].value_counts(sort=False))
            print("\nPatient-level counts (after aggregation and mapping):")
            print(self.patient_data["label"].value_counts(sort=False))
        else:
            print("\nNo classes defined or no data loaded.")
        print("----------------------------------------")

    def create_k_fold_splits(
        self, num_folds: int = 5, test_set_size: float = 0.1
    ) -> None:
        if self.patient_data.empty:
            raise ValueError("Patient data is empty. Cannot create splits.")

        # Create initial test set from patients
        if test_set_size > 0:
            train_val_patients_df, test_patients_df = train_test_split(
                self.patient_data,
                test_size=test_set_size,
                stratify=self.patient_data["label"],
                random_state=self.random_seed,
            )
            self.test_patient_ids = test_patients_df["case_id"].tolist()
        else:
            train_val_patients_df = self.patient_data.copy()
            self.test_patient_ids = []

        if num_folds > 0 and not train_val_patients_df.empty:
            skf = StratifiedKFold(
                n_splits=num_folds, shuffle=True, random_state=self.random_seed
            )
            # Ensure indices are reset for iloc to work correctly with skf.split
            self.train_val_patient_data_for_kfold = train_val_patients_df.reset_index(
                drop=True
            )
            self.kfold_splits = list(
                skf.split(
                    self.train_val_patient_data_for_kfold,
                    self.train_val_patient_data_for_kfold["label"],
                )
            )
        else:
            # If no k-folds, all train_val_patients become train, and val is empty
            self.train_patient_ids = train_val_patients_df["case_id"].tolist()
            self.val_patient_ids = []
            self.kfold_splits = None  # Explicitly set to None

        if self.verbose:
            print(
                f"Created {num_folds}-fold splits with test set size {test_set_size}."
            )
            print(f"Total patients for K-Fold Train/Val: {len(train_val_patients_df)}")
            print(f"Total patients for Test: {len(self.test_patient_ids)}")

    def set_current_fold(self, fold_index: int = 0) -> None:
        if (
            self.kfold_splits is None and self.train_patient_ids is not None
        ):  # Test set only or no kfold case
            # Test patient IDs are already set in create_k_fold_splits
            # Train/Val patient IDs are also set if no kfold
            pass  # Patient IDs are already set
        elif self.kfold_splits is None or self.train_val_patient_data_for_kfold is None:
            raise ValueError(
                "K-Fold splits have not been created. Call create_k_fold_splits() first."
            )
        elif fold_index >= len(self.kfold_splits):
            raise ValueError(
                f"Fold index {fold_index} is out of bounds for {len(self.kfold_splits)} folds."
            )
        else:
            train_patient_indices_in_kfold_df, val_patient_indices_in_kfold_df = (
                self.kfold_splits[fold_index]
            )

            train_patients_df = self.train_val_patient_data_for_kfold.iloc[
                train_patient_indices_in_kfold_df
            ]
            val_patients_df = self.train_val_patient_data_for_kfold.iloc[
                val_patient_indices_in_kfold_df
            ]

            self.train_patient_ids = train_patients_df["case_id"].tolist()
            self.val_patient_ids = val_patients_df["case_id"].tolist()
            # self.test_patient_ids was set during create_k_fold_splits

        # Map patient IDs to slide indices
        self.train_slide_indices = (
            self.slide_data[
                self.slide_data["case_id"].isin(self.train_patient_ids)
            ].index.tolist()
            if self.train_patient_ids
            else []
        )
        self.val_slide_indices = (
            self.slide_data[
                self.slide_data["case_id"].isin(self.val_patient_ids)
            ].index.tolist()
            if self.val_patient_ids
            else []
        )
        self.test_slide_indices = (
            self.slide_data[
                self.slide_data["case_id"].isin(self.test_patient_ids)
            ].index.tolist()
            if self.test_patient_ids
            else []
        )

        if self.verbose:
            print(f"Set to fold {fold_index}:")
            print(
                f"  Train patients: {len(self.train_patient_ids)}, Train slides: {len(self.train_slide_indices)}"
            )
            print(
                f"  Val patients: {len(self.val_patient_ids)}, Val slides: {len(self.val_slide_indices)}"
            )
            print(
                f"  Test patients: {len(self.test_patient_ids)}, Test slides: {len(self.test_slide_indices)}"
            )

    def get_mil_datasets(
        self,
        backbone: str,
        patch_size: str = "",
        use_hdf5: bool = False,
        cache_enabled: bool = False,
    ) -> Tuple[
        Optional[WSIMILDataset], Optional[WSIMILDataset], Optional[WSIMILDataset]
    ]:
        if (
            self.train_slide_indices is None
            or self.val_slide_indices is None
            or self.test_slide_indices is None
        ):
            raise ValueError(
                "Splits/fold not set. Call create_k_fold_splits() and then set_current_fold() first."
            )

        train_df_split = self.slide_data.iloc[self.train_slide_indices].reset_index(
            drop=True
        )
        val_df_split = self.slide_data.iloc[self.val_slide_indices].reset_index(
            drop=True
        )
        test_df_split = self.slide_data.iloc[self.test_slide_indices].reset_index(
            drop=True
        )

        common_params = {
            "data_directory": self.data_directory,
            "num_classes": self.num_classes,
            "backbone": backbone,
            "patch_size": patch_size,
            "use_hdf5": use_hdf5,
            "cache_enabled": cache_enabled,
        }

        train_dataset = (
            WSIMILDataset(slide_data_df=train_df_split, **common_params)
            if not train_df_split.empty
            else None
        )
        val_dataset = (
            WSIMILDataset(slide_data_df=val_df_split, **common_params)
            if not val_df_split.empty
            else None
        )
        test_dataset = (
            WSIMILDataset(slide_data_df=test_df_split, **common_params)
            if not test_df_split.empty
            else None
        )

        return train_dataset, val_dataset, test_dataset

    def get_number_of_folds(self) -> int:
        return len(self.kfold_splits) if self.kfold_splits is not None else 0

    def save_current_split_patient_ids(self, filename: str) -> None:
        if (
            self.train_patient_ids is None
            or self.val_patient_ids is None
            or self.test_patient_ids is None
        ):
            raise ValueError("Splits not set. Call set_current_fold() first.")

        # Pad lists to the same length for DataFrame creation
        max_len = max(
            len(self.train_patient_ids or []),
            len(self.val_patient_ids or []),
            len(self.test_patient_ids or []),
        )

        train_ids_padded = (self.train_patient_ids or []) + [None] * (
            max_len - len(self.train_patient_ids or [])
        )
        val_ids_padded = (self.val_patient_ids or []) + [None] * (
            max_len - len(self.val_patient_ids or [])
        )
        test_ids_padded = (self.test_patient_ids or []) + [None] * (
            max_len - len(self.test_patient_ids or [])
        )

        splits_df = pd.DataFrame(
            {
                "train_patient_id": train_ids_padded,
                "val_patient_id": val_ids_padded,
                "test_patient_id": test_ids_padded,
            }
        )
        splits_df.to_csv(filename, index=False)
        if self.verbose:
            print(f"Current split patient IDs saved to {filename}")

    def summarize_current_splits(
        self, return_summary_df: bool = False
    ) -> Optional[pd.DataFrame]:
        if self.train_slide_indices is None:
            print("No split set. Call set_current_fold() first.")
            return None

        summary_data = {}
        class_map_inv = {v: k for k, v in self.label_mapping.items()}

        for split_name, slide_indices in [
            ("train", self.train_slide_indices),
            ("val", self.val_slide_indices),
            ("test", self.test_slide_indices),
        ]:
            if slide_indices is None:
                continue
            labels = self.slide_data.loc[slide_indices, "label"]
            counts = labels.value_counts().sort_index()
            summary_data[split_name] = {
                class_map_inv.get(cls_idx, f"UnknownClass_{cls_idx}"): count
                for cls_idx, count in counts.items()
            }
            if self.verbose:
                print(f"\n--- {split_name.upper()} Split Summary ---")
                print(f"Number of slides: {len(slide_indices)}")
                print(
                    f"Number of patients: {len(self.slide_data.loc[slide_indices, 'case_id'].unique())}"
                )
                for cls_idx, count in counts.items():
                    class_name = class_map_inv.get(cls_idx, f"UnknownClass_{cls_idx}")
                    print(f"  Class {cls_idx} ({class_name}): {count} slides")

        if return_summary_df:
            df = pd.DataFrame(summary_data).fillna(0).astype(int)
            return df
        return None


class WSIMILDataset(Dataset):
    def __init__(
        self,
        slide_data_df: pd.DataFrame,
        data_directory: Union[str, Dict[str, str]],
        num_classes: int,  # For consistency, though label is in slide_data_df
        backbone: Optional[str] = None,
        patch_size: str = "",
        use_hdf5: bool = False,
        cache_enabled: bool = False,
    ):
        self.slide_data = slide_data_df  # DataFrame for this specific split
        self.data_directory = data_directory
        self.num_classes = num_classes
        self.backbone = backbone
        self.patch_size = (
            str(patch_size) if patch_size is not None else ""
        )  # Ensure string
        self.use_hdf5 = use_hdf5
        self.cache_enabled = cache_enabled
        self.data_cache: Dict[str, torch.Tensor] = {}

        if not self.use_hdf5 and not self.backbone:
            print(
                "Warning: WSIMILDataset initialized for .pt files but backbone is not set. Call set_backbone()."
            )

    def __len__(self) -> int:
        return len(self.slide_data)

    def __getitem__(
        self, idx: int
    ) -> Union[Tuple[torch.Tensor, int], Tuple[torch.Tensor, int, np.ndarray]]:
        row = self.slide_data.iloc[idx]
        slide_id = row["slide_id"]
        label = row["label"]  # Assumes 'label' is already integer mapped

        current_data_dir_path: str
        if isinstance(self.data_directory, dict):
            source_col = "source"  # This column must exist in slide_data_df if data_directory is a dict
            if source_col not in row.index:
                raise ValueError(
                    f"'{source_col}' column missing in slide_data for multi-source data_directory. Slide ID: {slide_id}"
                )
            source = row[source_col]
            if source not in self.data_directory:
                raise ValueError(
                    f"Source '{source}' for slide '{slide_id}' not found in data_directory keys: {list(self.data_directory.keys())}"
                )
            current_data_dir_path = self.data_directory[source]
        else:  # data_directory is a string
            current_data_dir_path = self.data_directory

        if not os.path.isdir(current_data_dir_path):
            raise FileNotFoundError(
                f"Data directory for slide {slide_id} not found: {current_data_dir_path}"
            )

        if not self.use_hdf5:
            if not self.backbone:
                raise ValueError(
                    "Backbone must be set for loading .pt files. Call set_backbone()."
                )

            # Handle cases like patch_size='512' or patch_size='' for root, vs. patch_size='256' for subdir
            patch_subdir = ""
            if (
                self.patch_size and self.patch_size != "512"
            ):  # Assuming '512' means no subdir or handled differently
                patch_subdir = self.patch_size

            # Construct path: data_dir / (optional_patch_subdir) / pt_files / backbone / slide_id.pt
            file_path = os.path.join(
                current_data_dir_path,
                patch_subdir,
                "pt_files",
                self.backbone,
                f"{slide_id}.pt",
            )

            cached_features = self.data_cache.get(file_path)
            if cached_features is not None:
                return cached_features, label

            try:
                features = torch.load(file_path)
                if self.cache_enabled:
                    self.data_cache[file_path] = features
                return features, label
            except FileNotFoundError as e:
                new_error_msg = f"Feature file not found: {file_path}. Check slide_id, backbone, patch_size, and data_directory structure."
                print(
                    f"Details: Slide ID='{slide_id}', Backbone='{self.backbone}', Patch Size='{self.patch_size}', Dir='{current_data_dir_path}'"
                )
                raise FileNotFoundError(new_error_msg) from e
            except Exception as e:
                raise RuntimeError(f"Error loading {file_path}: {e}") from e
        else:
            file_path = os.path.join(
                current_data_dir_path, "h5_files", f"{slide_id}.h5"
            )
            # HDF5 typically isn't cached in memory this way due to size, but could be if small
            try:
                with h5py.File(file_path, "r") as hdf5_file:
                    features = torch.from_numpy(hdf5_file["features"][:])
                    # Coordinates are optional, decide if they are always needed
                    if "coords" in hdf5_file:
                        coordinates = hdf5_file["coords"][:]
                        return features, label, coordinates
                    return features, label
            except OSError as e:
                raise OSError(f"HDF5 file not found or corrupted: {file_path}") from e

    def set_backbone(self, backbone: str) -> None:
        if self.verbose:
            print(f"Setting backbone for MILDataset: {backbone}")
        self.backbone = backbone

    def set_patch_size(self, size: Union[str, int]) -> None:
        if self.verbose:
            print(f"Setting patch size for MILDataset: {size}")
        self.patch_size = str(size)  # Ensure string

    def load_from_hdf5(self, use_hdf5: bool) -> None:
        self.use_hdf5 = use_hdf5

    def preload_data(self, num_threads: int = 8) -> None:
        if not self.cache_enabled:
            print(
                "Warning: Preloading data but cache_enabled is False. Data will not be stored in memory."
            )
            self.cache_enabled = True  # Enable it for preloading

        print(f"Preloading {len(self)} items into cache using {num_threads} threads...")
        from multiprocessing.pool import ThreadPool

        indices = list(range(len(self)))
        with ThreadPool(num_threads) as pool:
            pool.map(self.__getitem__, indices)
        print("Preloading complete.")

    @property
    def verbose(
        self,
    ):  # Simple way to make it accessible if needed, or pass as init arg
        return True  # Or control via an __init__ param



================================================
File: sauron/data/data_utils.py
================================================
import collections
import math
import os
from typing import Iterator, List, Optional, Tuple, Union

import numpy as np
import pandas as pd
import torch
import torch.nn as nn  # Not used in this snippet, but kept if other utils in file use it
import torch.optim as optim  # Not used in this snippet
from torch.utils.data import (
    DataLoader,
    Dataset,  # Added for type hinting
    RandomSampler,
    Sampler,
    SequentialSampler,
    WeightedRandomSampler,
)


def make_weights_for_balanced_classes(
    dataset: Dataset,  # Use the base torch Dataset for broader compatibility
    # Assumes dataset has:
    # 1. A way to get all labels: e.g., dataset.get_all_labels() -> List[int]
    # OR 2. Direct access to labels: e.g., dataset.labels or dataset.slide_data['label']
    # OR 3. `get_label(idx)` method as in your original (less efficient for all labels)
) -> torch.Tensor:
    """
    Creates weights for WeightedRandomSampler for handling class imbalance.
    This version is more flexible.

    Args:
        dataset: A PyTorch Dataset instance. It needs a way to access all labels.
                 Common patterns:
                 - `dataset.labels` (a list or tensor of all labels)
                 - `dataset.slide_data['label']` (if slide_data is a pandas DataFrame)
                 - A method like `dataset.get_all_labels()`

    Returns:
        torch.Tensor: A 1D tensor of weights for each sample in the dataset.
    """
    # Try to get labels in a more efficient way
    all_labels: Optional[List[int]] = None
    if hasattr(dataset, "labels") and isinstance(
        dataset.labels, (list, np.ndarray, torch.Tensor)
    ):
        all_labels = list(dataset.labels)
    elif (
        hasattr(dataset, "slide_data")
        and isinstance(dataset.slide_data, pd.DataFrame)
        and "label" in dataset.slide_data.columns
    ):
        all_labels = dataset.slide_data["label"].tolist()
    elif hasattr(dataset, "get_all_labels") and callable(dataset.get_all_labels):
        all_labels = dataset.get_all_labels()
    elif hasattr(dataset, "getlabel") and callable(
        dataset.getlabel
    ):  # Fallback to less efficient original
        print(
            "Warning: Using inefficient `getlabel(idx)` for each sample to compute weights. "
            "Consider adding a `labels` attribute or `get_all_labels()` method to your dataset for performance."
        )
        all_labels = [dataset.getlabel(i) for i in range(len(dataset))]
    else:
        raise AttributeError(
            "Dataset does not have a recognized way to access all labels "
            "(e.g., 'labels' attribute, 'slide_data[\"label\"]', "
            "or 'get_all_labels()' / 'getlabel()' method)."
        )

    if not all_labels:  # Should be caught by AttributeError, but as a safeguard
        raise ValueError("Could not extract labels from the dataset.")

    label_counts = collections.Counter(all_labels)
    num_samples = len(all_labels)

    if not label_counts:  # No labels or empty dataset
        return torch.ones(num_samples, dtype=torch.double)

    # Calculate weight for each class: N / (num_classes * count_for_that_class)
    # This is a common formula. Original: N_total_classes / count_for_that_class
    # Let's stick to a more standard approach: 1. / count_for_that_class then assign
    # Or total_samples / (num_distinct_classes * count_of_class_i)

    # Simpler: weight = 1.0 / count_of_sample's_class
    # Then WeightedRandomSampler handles it.
    # Or, if we want to give more weight to RARE classes:
    # weight_per_class = {label: num_samples / count for label, count in label_counts.items()}

    # The original implementation implied:
    # weight_per_class[class_idx] = N_total_unique_classes / num_samples_in_class_idx
    # This means dataset.slide_cls_ids was [[indices_for_class_0], [indices_for_class_1], ...]
    # Let's try to replicate that logic if `slide_cls_ids` is available, otherwise use Counter.

    if hasattr(dataset, "slide_cls_ids") and dataset.slide_cls_ids is not None:
        # This was the original logic structure
        num_distinct_classes = len(dataset.slide_cls_ids)  # N in original code
        if num_distinct_classes == 0:  # No classes defined in slide_cls_ids
            print("Warning: dataset.slide_cls_ids is empty. Returning uniform weights.")
            return torch.ones(num_samples, dtype=torch.double)

        weight_per_class_val = [0.0] * num_distinct_classes
        for i, cls_ids in enumerate(dataset.slide_cls_ids):
            if len(cls_ids) > 0:
                weight_per_class_val[i] = float(num_distinct_classes) / len(cls_ids)
            else:  # Class has no samples
                weight_per_class_val[i] = 0  # Or some other handling for empty classes

        # Check if getlabel exists before using it.
        if not hasattr(dataset, "getlabel") or not callable(dataset.getlabel):
            raise AttributeError(
                "Dataset has 'slide_cls_ids' but no 'getlabel(idx)' method "
                "to map sample index to class label for weighting."
            )

        weights = [
            weight_per_class_val[dataset.getlabel(idx)] for idx in range(num_samples)
        ]

    else:  # Fallback to using all_labels and Counter if slide_cls_ids not present
        print(
            "Warning: `dataset.slide_cls_ids` not found or is None. "
            "Calculating weights based on overall label counts."
        )
        if not label_counts:  # Already checked but good to be safe
            return torch.ones(num_samples, dtype=torch.double)

        # Standard approach: weight for a class is 1 / (number of samples in that class)
        # Then, for each sample, assign the weight of its class.
        # More robust against missing classes in label_counts if all_labels has them.
        max_label = max(all_labels) if all_labels else -1
        weight_per_class_val = [0.0] * (max_label + 1)
        for label, count in label_counts.items():
            if count > 0:
                # weight_per_class_val[label] = 1.0 / count # Basic
                weight_per_class_val[label] = num_samples / (
                    len(label_counts) * count
                )  # Another common one

        weights = [weight_per_class_val[label] for label in all_labels]

    return torch.tensor(weights, dtype=torch.double)


class SubsetSequentialSampler(Sampler[int]):  # Use Generic type hint
    """Samples elements sequentially from a given list of indices, always in the same order.

    Args:
        indices (List[int]): a sequence of indices
    """

    def __init__(self, indices: List[int]):
        if not isinstance(indices, list):
            raise TypeError("indices should be a list.")
        if not all(isinstance(i, int) for i in indices):
            raise TypeError("all elements in indices should be integers.")

        self.indices = indices

    def __iter__(self) -> Iterator[int]:
        return iter(self.indices)

    def __len__(self) -> int:
        return len(self.indices)


def collate_mil_features(
    batch: List[
        Tuple[torch.Tensor, int]
    ],  # Assumes batch item is (features_tensor, label_int)
) -> Tuple[torch.Tensor, torch.Tensor]:
    """
    Collate function for MIL when batch items are (Tensor_Features, Label_Int).
    Concatenates all feature tensors (bags) along dimension 0.
    Labels are converted to a LongTensor.
    """
    # Filter out None items if any dataset returns None (e.g. for failed loads, though ideally handled in Dataset)
    # batch = [item for item in batch if item is not None and item[0] is not None]
    # if not batch:
    #     # Handle empty batch case, e.g. return empty tensors or raise error
    #     # This depends on how the training loop handles it.
    #     # For now, assume batch is never empty after filtering.
    #     # If it can be, the caller (DataLoader) might need a custom batch_sampler.
    #     return torch.empty(0), torch.empty(0, dtype=torch.long)

    try:
        # Assuming item[0] is a tensor of features for one WSI (bag of instances)
        # These features are typically [Number_of_Patches, Feature_Dimension]
        # Concatenating them makes [Total_Patches_in_Batch, Feature_Dimension]
        # This is standard for some MIL approaches where the model processes all patches from batch.
        features = torch.cat([item[0] for item in batch], dim=0)
        labels = torch.tensor([item[1] for item in batch], dtype=torch.long)
    except Exception as e:
        print("Error during collation (collate_mil_features):")
        for i, item in enumerate(batch):
            print(
                f"  Item {i}: type={type(item)}, len={len(item) if isinstance(item, (tuple,list)) else 'N/A'}"
            )
            if isinstance(item, (tuple, list)) and len(item) > 0:
                print(
                    f"    Item[0] type: {type(item[0])}, shape: {item[0].shape if hasattr(item[0], 'shape') else 'N/A'}"
                )
                print(f"    Item[1] type: {type(item[1])}")
        raise RuntimeError(f"Collation failed: {e}") from e

    return features, labels


def collate_mil_survival(
    batch: List[
        Tuple[
            torch.Tensor, Union[torch.Tensor, Tuple[torch.Tensor, ...]], int, float, int
        ]
    ],
    # Expected item: (path_features, omic_features, combined_label, event_time, censorship_status)
    # omic_features can be a single Tensor or a Tuple of Tensors (for coattn)
) -> Tuple[
    torch.Tensor,
    Union[torch.Tensor, Tuple[torch.Tensor, ...]],
    torch.Tensor,
    torch.Tensor,
    torch.Tensor,
]:
    """
    Collate function for Survival MIL tasks.
    Handles path features, omic features (single or tuple for coattn), and survival labels.
    """
    path_features_list = [
        item[0] for item in batch if item[0].numel() > 0
    ]  # Filter empty path features
    omic_features_list = [
        item[1] for item in batch
    ]  # Keep all, model must handle empty omics

    combined_labels = torch.tensor([item[2] for item in batch], dtype=torch.long)
    event_times = torch.tensor([item[3] for item in batch], dtype=torch.float32)
    censorship_statuses = torch.tensor([item[4] for item in batch], dtype=torch.int)

    # Collate path features (bag of instances)
    collated_path_features = (
        torch.cat(path_features_list, dim=0) if path_features_list else torch.empty(0)
    )

    # Collate omic features
    collated_omic_features: Union[torch.Tensor, Tuple[torch.Tensor, ...]]
    if isinstance(omic_features_list[0], torch.Tensor):  # All omics are single tensors
        # Stack if they are per-patient vectors, cat if they are bags of omic instances (unlikely here)
        # Assuming omic_features_list[i] is [omic_feature_dim] for patient i
        collated_omic_features = (
            torch.stack(omic_features_list, dim=0)
            if omic_features_list
            else torch.empty(0)
        )
    elif isinstance(
        omic_features_list[0], tuple
    ):  # Omic features are tuples (e.g., for CoAttn)
        # Transpose the list of tuples: [(o1a,o2a,o3a), (o1b,o2b,o3b)] -> [(o1a,o1b), (o2a,o2b), (o3a,o3b)]
        num_omic_modalities = len(omic_features_list[0])
        collated_omic_modalities = []
        for i in range(num_omic_modalities):
            modality_tensors = [
                omic_tuple[i]
                for omic_tuple in omic_features_list
                if omic_tuple[i].numel() > 0
            ]
            if modality_tensors:
                collated_omic_modalities.append(torch.stack(modality_tensors, dim=0))
            else:  # All patients had empty tensor for this modality
                collated_omic_modalities.append(torch.empty(0))
        collated_omic_features = tuple(collated_omic_modalities)
    else:
        raise TypeError(
            f"Unsupported omic feature type in batch: {type(omic_features_list[0])}"
        )

    return (
        collated_path_features,
        collated_omic_features,
        combined_labels,
        event_times,
        censorship_statuses,
    )


def get_dataloader(  # Renamed from get_split_loader for generality
    dataset: Dataset,  # Use base torch Dataset
    batch_size: int = 1,  # Default MIL batch_size is 1 (one WSI per "batch")
    shuffle: bool = False,  # For training, typically True
    use_weighted_sampler: bool = False,  # If True, enables weighted sampling for imbalance
    num_workers: int = 4,
    pin_memory: bool = True,
    collate_fn_type: str = "classification",  # "classification" or "survival"
    # Removed `testing` flag, as subset sampling is usually for debugging/prototyping
    # and can be handled by passing a Subset of the dataset if needed.
    # device: Optional[torch.device] = None, # Not strictly needed for DataLoader creation
) -> DataLoader:
    """
    Creates a PyTorch DataLoader for a given dataset.

    Args:
        dataset: The PyTorch Dataset instance.
        batch_size: How many samples per batch to load. For MIL, often 1.
        shuffle: Set to True to have the data reshuffled at every epoch (usually for training).
        use_weighted_sampler: If True, uses WeightedRandomSampler for class balancing.
                              Requires `make_weights_for_balanced_classes` to work with the dataset.
        num_workers: How many subprocesses to use for data loading.
        pin_memory: If True, copies Tensors into CUDA pinned memory before returning them.
        collate_fn_type: Specifies the type of collate function to use.
                         "classification" for standard (features, label) items.
                         "survival" for (path_feat, omic_feat, comb_label, time, event) items.

    Returns:
        torch.DataLoader: The configured DataLoader.
    """
    # Determine if CUDA is available and adjust defaults if necessary
    on_gpu = torch.cuda.is_available()
    current_num_workers = (
        num_workers if on_gpu else 0
    )  # Often set to 0 for CPU for simplicity
    current_pin_memory = pin_memory if on_gpu else False

    sampler: Optional[Sampler] = None
    # Shuffle and weighted_sampler are mutually exclusive with explicitly provided sampler.
    # WeightedRandomSampler implies random sampling.
    if use_weighted_sampler:
        if shuffle is False:  # WeightedRandomSampler inherently shuffles.
            print(
                "Warning: `use_weighted_sampler=True` implies shuffling. `shuffle=False` will be ignored."
            )
        try:
            weights = make_weights_for_balanced_classes(dataset)
            # num_samples for WeightedRandomSampler is how many samples to draw in an epoch
            # Usually len(dataset) to see each sample (on average) once.
            sampler = WeightedRandomSampler(
                weights, num_samples=len(dataset), replacement=True
            )
            shuffle = False  # Sampler handles shuffling, so DataLoader's shuffle should be False
        except (AttributeError, ValueError) as e:
            print(
                f"Could not create weighted sampler: {e}. Falling back to standard RandomSampler if shuffle=True."
            )
            if shuffle:
                sampler = RandomSampler(dataset)
            else:
                sampler = SequentialSampler(
                    dataset
                )  # Should not happen if shuffle=True was intended
    elif shuffle:
        sampler = RandomSampler(dataset)
    else:  # No shuffle, no weighted sampling
        sampler = SequentialSampler(dataset)

    # Select collate_fn based on type
    if collate_fn_type.lower() == "classification":
        collate_function = collate_mil_features
    elif collate_fn_type.lower() == "survival":
        collate_function = collate_mil_survival
    else:
        raise ValueError(
            f"Unknown collate_fn_type: {collate_fn_type}. "
            "Choose 'classification' or 'survival'."
        )

    return DataLoader(
        dataset,
        batch_size=batch_size,
        sampler=sampler,
        collate_fn=collate_function,
        num_workers=current_num_workers,
        pin_memory=current_pin_memory,
        drop_last=False,  # Typically False for MIL unless batch_size > 1 and partial batches are an issue
    )



================================================
File: sauron/data/dataloader.py
================================================
import h5py
from PIL import Image
from torch.utils.data import Dataset

from sauron.utils.WSIObjects import OpenSlideWSIPatcher, get_pixel_size


class TileDataset(Dataset):
    def __init__(
        self,
        wsi,
        contours,
        target_patch_size,
        target_magnification,
        eval_transform,
        save_path=None,
    ):
        self.wsi = wsi
        self.contours = contours
        self.eval_transform = eval_transform

        self.patcher = OpenSlideWSIPatcher(
            wsi=wsi,
            patch_size=target_patch_size,
            src_pixel_size=get_pixel_size(wsi.img),
            dst_pixel_size=self.magnification_to_pixel_size(target_magnification),
            mask=contours,
            coords_only=False,
        )
        self.patcher.save_visualization(path=save_path)

    @staticmethod
    def magnification_to_pixel_size(magnification):
        magnification_map = {2.5: 4.0, 5: 2.0, 10: 1.0, 20: 0.5, 40: 0.25}
        if magnification in magnification_map:
            return magnification_map[magnification]
        else:
            raise ValueError("Magnification should be in [2.5, 5, 10, 20, 40].")

    def _load_coords(self):
        with h5py.File(self.coords_h5_fpath, "r") as f:
            self.attr_dict = {
                k: dict(f[k].attrs) for k in f.keys() if len(f[k].attrs) > 0
            }
            self.coords = f["coords"][:]
            self.patch_size = f["coords"].attrs["patch_size"]
            self.custom_downsample = f["coords"].attrs["custom_downsample"]
            self.target_patch_size = (
                int(self.patch_size) // int(self.custom_downsample)
                if self.custom_downsample > 1
                else self.patch_size
            )

    def __len__(self):
        return len(self.patcher)

    def __getitem__(self, index):
        image, x, y = self.patcher[index]
        image = Image.fromarray(image, "RGB")
        image = self.eval_transform(image).unsqueeze(dim=0)
        return image, (x, y)



================================================
File: sauron/data/dataset_factory.py
================================================
import os
from typing import Any, Dict, Optional, Union

# Assuming your new dataset classes are in these locations
from sauron.data.classMILDataset import ClassificationDataManager, WSIMILDataset
from sauron.data.survMILDataset import SurvivalDataManager, SurvivalMILDataset

# Keep supported tasks as in your original, or manage elsewhere
SUPPORTED_TASKS = ["TGCT", "BRCA", "COAD", "UCEC", "LUAD"]


def get_data_manager(
    task_name: str,
    task_type: str,
    csv_path: str,
    data_directory: Union[str, Dict[str, str]],
    seed: int = 7,
    verbose: bool = True,
    **kwargs: Any,  # Catch-all for other specific params
) -> Union[ClassificationDataManager, SurvivalDataManager]:
    """
    Factory function to create a DataManager instance for a given task and type.

    Args for Classification (via kwargs):
        label_column (str): Name of the column in CSV containing original labels.
        label_mapping (Dict[str, int]): Mapping from string labels to integer classes.
        patient_id_col_name (str): Column name for patient IDs. Default 'case_id'.
        slide_id_col_name (str): Column name for slide IDs. Default 'slide_id'.
        filter_criteria (Dict): For filtering rows in CSV.
        ignore_labels (List[str]): String labels to ignore.
        patient_label_aggregation (str): 'max' or 'majority'.
        shuffle (bool): Whether to shuffle loaded data. Default False. (for ClassificationDataManager)

    Args for Survival (via kwargs):
        time_column (str): Name of the column for survival time.
        event_column (str): Name of the column for event status (0=censored, 1=event).
        patient_id_col_name (str): Column name for patient IDs. Default 'case_id'.
        slide_id_col_name (str): Column name for slide IDs. Default 'slide_id'.
        n_bins (int): Number of bins for discretizing survival time. Default 4.
        filter_dict (Dict): For filtering rows in CSV.
        omic_csv_path (str): Path to CSV with omic features.
        omic_patient_id_col (str): Patient ID column in omic CSV.
        apply_sig (bool): For coattn mode signatures.
        signatures_csv_path (str): Path to signatures CSV for coattn.
        shuffle_slide_data (bool): Shuffle initial slide data. Default False. (for SurvivalDataManager)
    """
    task_name_upper = task_name.upper()
    if task_name_upper not in SUPPORTED_TASKS:
        # Consider if this check is still needed if task_type drives logic
        print(
            f"Warning: Task '{task_name}' not in predefined SUPPORTED_TASKS, but proceeding based on task_type."
        )

    if not os.path.exists(csv_path):
        raise FileNotFoundError(f"Dataset CSV file not found: {csv_path}")

    if task_type.lower() == "classification":
        # Extract classification-specific args from kwargs with defaults
        cls_kwargs = {
            "label_column": kwargs.get("label_column", "label"),
            "label_mapping": kwargs.get("label_mapping"),  # Allow None, DM can infer
            "patient_id_col_name": kwargs.get("patient_id_col_name", "case_id"),
            "slide_id_col_name": kwargs.get("slide_id_col_name", "slide_id"),
            "shuffle": kwargs.get(
                "shuffle", False
            ),  # Specific to ClassificationDataManager's initial load
            "filter_criteria": kwargs.get("filter_criteria"),
            "ignore_labels": kwargs.get("ignore_labels"),
            "patient_stratification": kwargs.get(
                "patient_stratification", False
            ),  # Legacy, not directly used by DM for len
            "patient_label_aggregation": kwargs.get("patient_label_aggregation", "max"),
        }
        return ClassificationDataManager(
            csv_path=csv_path,
            data_directory=data_directory,
            random_seed=seed,
            verbose=verbose,
            **cls_kwargs,
        )
    elif task_type.lower() == "survival":
        # Extract survival-specific args from kwargs with defaults
        surv_kwargs = {
            "time_column": kwargs.get("time_column"),
            "event_column": kwargs.get("event_column"),
            "patient_id_col_name": kwargs.get("patient_id_col_name", "case_id"),
            "slide_id_col_name": kwargs.get("slide_id_col_name", "slide_id"),
            "n_bins": kwargs.get("n_bins", 4),
            "shuffle_slide_data": kwargs.get(
                "shuffle_slide_data", False
            ),  # Specific to SurvivalDataManager
            "filter_dict": kwargs.get("filter_dict"),
            "eps": kwargs.get("eps", 1e-6),
            "omic_csv_path": kwargs.get("omic_csv_path"),
            "omic_patient_id_col": kwargs.get("omic_patient_id_col", "case_id"),
            "apply_sig": kwargs.get("apply_sig", False),
            "signatures_csv_path": kwargs.get("signatures_csv_path"),
        }
        if not surv_kwargs["time_column"] or not surv_kwargs["event_column"]:
            raise ValueError(
                "time_column and event_column must be provided for survival tasks."
            )

        return SurvivalDataManager(
            csv_path=csv_path,
            data_directory=data_directory,
            random_seed=seed,
            verbose=verbose,
            **surv_kwargs,
        )
    else:
        raise ValueError(
            f"Unsupported task_type: '{task_type}'. Must be 'classification' or 'survival'."
        )


def determine_split_directory(  # This function seems more for organizing output/split files
    base_split_dir: Optional[str],
    task_name: str,
    label_frac: float = 1.0,
    k_fold: bool = True,
) -> str:
    """
    Determines a directory path for storing/loading data splits or results.
    (This might be used externally to the DataManager for saving outputs).
    """
    if base_split_dir is None:
        # Example: splits/TGCT/label_frac_100_kfold or splits/TGCT/label_frac_10_holdout
        split_subdir = f"{task_name.upper()}"
        # Suffix based on k_fold and label_frac (if not full dataset)
        suffix = "_kfold" if k_fold else "_holdout"
        if label_frac < 1.0:  # Only add label_frac if it's a subset
            split_subdir += f"_label_frac_{int(label_frac * 100)}"

        determined_dir = os.path.join("results_or_splits", split_subdir + suffix)
    else:
        determined_dir = base_split_dir

    # Ensure directory exists if it's for saving
    # os.makedirs(determined_dir, exist_ok=True) # Uncomment if you want auto-creation
    return determined_dir



================================================
File: sauron/data/survMILDataset.py
================================================
from __future__ import annotations

import os
from typing import Dict, List, Optional, Tuple, Union

import h5py  # For potential H5 feature loading
import numpy as np
import pandas as pd
import torch
from sklearn.model_selection import train_test_split  # Using this for simple splits
from sklearn.preprocessing import StandardScaler
from torch.utils.data import Dataset

# Assuming generate_split and nth are from utils.utils as in original
# If these are complex, they might need to be part of this class or simplified
from sauron.utils.utils import (  # Placeholder if this path is correct
    generate_split,
    nth,
)


class SurvivalDataManager:
    def __init__(
        self,
        csv_path: str,
        data_directory: Union[
            str, Dict[str, str]
        ],  # Path to feature root dir or dict by source
        time_column: str,  # e.g., 'survival_months'
        event_column: str,  # e.g., 'censorship' (0 for censored, 1 for event)
        patient_id_col_name: str = "case_id",
        slide_id_col_name: str = "slide_id",  # Though survival is often patient-level for MIL
        n_bins: int = 4,  # For discretizing survival time
        shuffle_slide_data: bool = False,  # Shuffle initial slide data loaded from CSV
        random_seed: int = 7,
        verbose: bool = True,
        patient_stratification_for_len: bool = True,  # If this manager were a dataset, how to calc len
        filter_dict: Optional[Dict[str, List[str]]] = None,
        eps: float = 1e-6,  # Epsilon for binning
        omic_csv_path: Optional[str] = None,  # Path to CSV with omic features
        omic_patient_id_col: Optional[str] = "case_id",  # Patient ID col in omic CSV
        apply_sig: bool = False,  # For coattn mode signatures
        signatures_csv_path: Optional[str] = None,  # Path to signatures CSV for coattn
    ):
        self.csv_path = csv_path
        self.data_directory = data_directory
        self.time_column = time_column
        self.event_column = event_column
        self.patient_id_col_name = patient_id_col_name
        self.slide_id_col_name = slide_id_col_name
        self.n_bins = n_bins
        self.random_seed = random_seed
        self.verbose = verbose
        self.filter_dict = filter_dict
        self.eps = eps
        self.omic_csv_path = omic_csv_path
        self.omic_patient_id_col = omic_patient_id_col
        self.apply_sig = apply_sig
        self.signatures_csv_path = signatures_csv_path

        np.random.seed(self.random_seed)

        # Load and preprocess slide/patient data
        raw_data = pd.read_csv(csv_path, low_memory=False)
        self.slide_data = self._rename_cols(
            raw_data
        )  # Standardizes to "case_id", "slide_id"
        self.slide_data = self._filter_data(self.slide_data, self.filter_dict)

        if shuffle_slide_data:
            self.slide_data = self.slide_data.sample(
                frac=1, random_state=self.random_seed
            ).reset_index(drop=True)

        # Patient data is primary for survival; slide_id links features to patient
        # We'll use patient_df for labels, splits, and as base for MIL dataset items
        self.patient_df = self.slide_data.drop_duplicates(subset=["case_id"]).copy()
        self.patient_df = self._discretize_survival(
            self.patient_df
        )  # Adds 'label' (combined) and 'disc_label' (bin)

        self.num_classes = len(
            self.survival_label_map
        )  # Number of (bin, event_status) combinations
        self._create_patient_slide_dictionary()  # self.patient_slide_dict

        # Omic data handling
        self.omic_features_df: Optional[pd.DataFrame] = None
        self.omic_scalers: Optional[Dict[str, StandardScaler]] = (
            None  # For different omic types if needed
        )
        self.signatures: Optional[pd.DataFrame] = None
        self.omic_names_for_coattn: Optional[List[List[str]]] = None
        self.omic_sizes_for_coattn: Optional[List[int]] = None

        if self.omic_csv_path:
            self._load_omic_data()
        if self.apply_sig and self.signatures_csv_path:
            self._load_signatures()
            if self.omic_features_df is not None:
                self._prepare_coattn_omic_names()

        self.all_patient_ids = self.patient_df["case_id"].tolist()
        self._prepare_class_indices_for_split()  # For stratified splitting using 'label'

        # Split related attributes
        self.train_patient_ids: Optional[List[str]] = None
        self.val_patient_ids: Optional[List[str]] = None
        self.test_patient_ids: Optional[List[str]] = None
        self.split_generator = None  # For k-fold from original generate_split

        if verbose:
            self._print_summary()

    def _rename_cols(self, df: pd.DataFrame) -> pd.DataFrame:
        df = df.copy()
        if (
            self.patient_id_col_name != "case_id"
            and self.patient_id_col_name in df.columns
        ):
            df.rename(columns={self.patient_id_col_name: "case_id"}, inplace=True)
        if (
            self.slide_id_col_name != "slide_id"
            and self.slide_id_col_name in df.columns
        ):
            df.rename(columns={self.slide_id_col_name: "slide_id"}, inplace=True)
        if "case_id" not in df.columns:
            raise ValueError(
                f"Patient ID column '{self.patient_id_col_name}' (expected 'case_id') not found."
            )
        # slide_id is not strictly necessary if data_directory structure uses case_id for H5/PT files
        # but good to have for mapping if multiple slides per patient contribute features.
        if "slide_id" not in df.columns:
            print(
                f"Warning: Slide ID column '{self.slide_id_col_name}' (expected 'slide_id') not found. Assuming patient-level features or direct case_id mapping for files."
            )
            # If slide_id is critical for feature loading, this might need adjustment or error.
            # For now, let's assume if multiple .pt files per patient, slide_id is used to find them.
            # If one feature file per patient (e.g. patient_id.h5), then slide_id is less critical.
            if (
                "slide_id" not in df.columns
                and self.patient_id_col_name == self.slide_id_col_name
            ):  # If slide_id is same as patient_id
                df["slide_id"] = df["case_id"]
            elif (
                "slide_id" not in df.columns
            ):  # Create a dummy slide_id if not present, can be case_id
                df["slide_id"] = df["case_id"]

        if self.time_column not in df.columns:
            raise ValueError(f"Time column '{self.time_column}' not found.")
        if self.event_column not in df.columns:
            raise ValueError(f"Event column '{self.event_column}' not found.")
        return df

    def _filter_data(
        self, data: pd.DataFrame, filter_criteria: Optional[Dict[str, List[str]]]
    ) -> pd.DataFrame:
        if filter_criteria:
            mask = pd.Series(True, index=data.index)
            for column, values in filter_criteria.items():
                if column not in data.columns:
                    print(f"Warning: Filter column '{column}' not found in data.")
                    continue
                mask &= data[column].isin(values)
            return data[mask].reset_index(drop=True)
        return data

    def _discretize_survival(self, patient_df: pd.DataFrame) -> pd.DataFrame:
        df = patient_df.copy()

        # Patients with event (event_column == 1, assuming 0 is censored)
        uncensored_df = df[df[self.event_column] == 1]
        if (
            len(uncensored_df) < self.n_bins
        ):  # Not enough uncensored patients to form bins
            print(
                f"Warning: Only {len(uncensored_df)} uncensored patients. Reducing n_bins to {max(1, len(uncensored_df)) if len(uncensored_df) > 0 else 1}."
            )
            current_n_bins = max(1, len(uncensored_df)) if len(uncensored_df) > 0 else 1
            if current_n_bins == 0:  # No uncensored patients, create a single bin
                self.survival_bins = np.array(
                    [
                        df[self.time_column].min() - self.eps,
                        df[self.time_column].max() + self.eps,
                    ]
                )
                # assign all to bin 0
                df["disc_label"] = 0
            else:  # use qcut on uncensored for bin edges
                _, self.survival_bins = pd.qcut(
                    uncensored_df[self.time_column],
                    q=current_n_bins,
                    retbins=True,
                    labels=False,
                )
                self.survival_bins[0] = (
                    df[self.time_column].min() - self.eps
                )  # Adjust outer bins
                self.survival_bins[-1] = df[self.time_column].max() + self.eps
                df["disc_label"] = pd.cut(
                    df[self.time_column],
                    bins=self.survival_bins,
                    retbins=False,
                    labels=False,
                    right=False,
                    include_lowest=True,
                ).astype(int)

        else:  # Sufficient uncensored patients
            # Determine bins based on uncensored patients' survival times
            _, q_bins_uncensored = pd.qcut(
                uncensored_df[self.time_column],
                q=self.n_bins,
                retbins=True,
                labels=False,
            )

            # Adjust bins to cover the whole range of survival times (including censored)
            self.survival_bins = np.concatenate(
                (
                    [df[self.time_column].min() - self.eps],
                    q_bins_uncensored[1:-1],
                    [df[self.time_column].max() + self.eps],
                )
            )

            # Ensure bins are monotonically increasing (pd.qcut can sometimes produce non-monotonic due to ties)
            self.survival_bins = np.unique(self.survival_bins)
            if (
                len(self.survival_bins) < self.n_bins + 1
            ):  # If unique reduced bins too much
                print(
                    f"Warning: After unique, number of bins reduced. Consider fewer n_bins or check data distribution."
                )
                # Fallback to simple linspace if qcut fails badly
                if len(self.survival_bins) <= 2:
                    self.survival_bins = np.linspace(
                        df[self.time_column].min() - self.eps,
                        df[self.time_column].max() + self.eps,
                        self.n_bins + 1,
                    )

            df["disc_label"] = pd.cut(
                df[self.time_column],
                bins=self.survival_bins,
                retbins=False,
                labels=False,
                right=False,
                include_lowest=True,
            ).astype(int)

        # Create combined label: (time_bin, event_status)
        self.survival_label_map = {}
        label_counter = 0
        # Iterate over actual unique bin numbers and event statuses
        actual_bins = sorted(df["disc_label"].unique())
        actual_event_statuses = sorted(df[self.event_column].unique())

        for bin_val in actual_bins:
            for event_val in actual_event_statuses:  # Typically 0 and 1
                self.survival_label_map[(bin_val, int(event_val))] = label_counter
                label_counter += 1

        df["label"] = df.apply(
            lambda x: self.survival_label_map[
                (x["disc_label"], int(x[self.event_column]))
            ],
            axis=1,
        )
        return df

    def _create_patient_slide_dictionary(self):
        self.patient_slide_dict = {}
        if (
            "slide_id" in self.slide_data.columns
            and "case_id" in self.slide_data.columns
        ):
            for patient_id, group in self.slide_data.groupby("case_id"):
                self.patient_slide_dict[patient_id] = group["slide_id"].tolist()
        else:  # If no slide_id, assume one "feature entity" per patient, named by case_id
            for patient_id in self.patient_df["case_id"]:
                self.patient_slide_dict[patient_id] = [
                    patient_id
                ]  # Use patient_id as the "slide_id"

    def _load_omic_data(self):
        if not self.omic_csv_path or not self.omic_patient_id_col:
            return
        try:
            omic_df = pd.read_csv(self.omic_csv_path)
            if self.omic_patient_id_col not in omic_df.columns:
                raise ValueError(
                    f"Omic patient ID column '{self.omic_patient_id_col}' not found in omic CSV."
                )

            omic_df.set_index(self.omic_patient_id_col, inplace=True)

            # Align omic data with patient_df (patients present in the main CSV)
            self.omic_features_df = omic_df.reindex(self.patient_df["case_id"]).fillna(
                0
            )  # Or other imputation

            # Identify feature columns (assuming all non-ID columns are features)
            # This might need to be more robust if omic CSV has other metadata
            self.omic_feature_names = [col for col in self.omic_features_df.columns]

            if self.verbose and self.omic_features_df is not None:
                print(
                    f"Loaded omic data for {len(self.omic_features_df)} patients, {len(self.omic_feature_names)} features."
                )
                print(
                    f"Patients in main data: {len(self.patient_df)}, Patients with omic data: {len(self.omic_features_df.dropna(how='all'))}"
                )

        except FileNotFoundError:
            print(f"Warning: Omic CSV file not found at {self.omic_csv_path}")
            self.omic_features_df = None
        except Exception as e:
            print(f"Error loading omic data: {e}")
            self.omic_features_df = None

    def _load_signatures(self):
        if not self.signatures_csv_path:
            return
        try:
            self.signatures = pd.read_csv(self.signatures_csv_path)
            if self.verbose:
                print(f"Loaded signatures from {self.signatures_csv_path}")
        except FileNotFoundError:
            print(f"Warning: Signatures CSV not found at {self.signatures_csv_path}")
            self.signatures = None

    def _prepare_coattn_omic_names(self):
        if self.signatures is None or self.omic_features_df is None:
            return

        self.omic_names_for_coattn = []
        # Original code implies signatures CSV has columns, each being a gene set / signature name
        # And values are gene names, possibly with suffixes like _mut, _cnv, _rnaseq
        available_omic_cols = pd.Series(self.omic_features_df.columns)

        for sig_col_name in self.signatures.columns:
            # Genes in the current signature
            genes_in_sig = self.signatures[sig_col_name].dropna().unique()

            # Construct potential feature names (gene + suffix)
            potential_features = []
            for gene in genes_in_sig:
                for suffix in [
                    "_mut",
                    "_cnv",
                    "_rnaseq",
                    "",
                ]:  # Add empty for base gene name
                    potential_features.append(f"{gene}{suffix}")

            # Find which of these potential features actually exist in our omic_features_df
            intersecting_features = sorted(
                list(set(potential_features) & set(available_omic_cols))
            )
            self.omic_names_for_coattn.append(intersecting_features)

        self.omic_sizes_for_coattn = [
            len(names) for names in self.omic_names_for_coattn
        ]
        if self.verbose:
            print("Prepared omic names for CoAttn:")
            for i, names in enumerate(self.omic_names_for_coattn):
                print(f"  Signature {i}: {len(names)} features")

    def _prepare_class_indices_for_split(self) -> None:
        # For stratified splitting based on the combined (bin, event) label
        self.patient_class_labels_for_stratification = self.patient_df["label"].values
        self.num_unique_stratification_labels = len(
            np.unique(self.patient_class_labels_for_stratification)
        )

        self.patient_indices_by_strat_label = [
            np.where(self.patient_class_labels_for_stratification == strat_label)[0]
            for strat_label in range(
                self.num_unique_stratification_labels
            )  # Assumes labels are 0 to N-1
        ]

    def _print_summary(self) -> None:
        print("--- Survival DataManager Summary ---")
        print(f"CSV Path: {self.csv_path}")
        print(f"Time Column: {self.time_column}, Event Column: {self.event_column}")
        print(f"N Bins for Discretization: {self.n_bins}")
        print(f"Actual Survival Bins Edges: {self.survival_bins}")
        print(f"Survival Label Map (bin, event) -> int: {self.survival_label_map}")
        print(f"Number of Combined Survival Classes: {self.num_classes}")
        print(f"Total unique patients in manager: {len(self.patient_df)}")
        print(f"Patient-level combined label counts:")
        print(self.patient_df["label"].value_counts(sort=False))
        if self.omic_features_df is not None:
            print(
                f"Omic features loaded for {self.omic_features_df.shape[0]} patients, with {self.omic_features_df.shape[1]} features."
            )
        if self.signatures is not None:
            print(
                f"Signatures loaded. {len(self.omic_names_for_coattn or [])} sets for CoAttn."
            )
        print("------------------------------------")

    def create_splits_from_generating_function(
        self,
        k=3,
        val_num=(25, 25),
        test_num=(40, 40),
        label_frac=1.0,
        custom_test_ids=None,
    ):
        """Uses the original generate_split logic.
        This creates a generator for k folds. You call set_next_fold_from_generator() to advance.
        """
        if not hasattr(
            self, "patient_indices_by_strat_label"
        ):  # Ensure _prepare_class_indices_for_split was called
            self._prepare_class_indices_for_split()

        settings = {
            "n_splits": k,
            "val_num": val_num,  # These numbers might be absolute counts or % depending on generate_split
            "test_num": test_num,
            "label_frac": label_frac,
            "seed": self.random_seed,
            "custom_test_ids": custom_test_ids,  # This needs careful handling if IDs are strings
            "cls_ids": self.patient_indices_by_strat_label,  # Indices into patient_df
            "samples": len(self.patient_df),  # Total number of patients
        }
        self.split_generator = generate_split(**settings)
        self.num_folds_generated = k  # Assuming k folds are generated
        if self.verbose:
            print(f"Initialized split generator for {k} folds.")

    def set_next_fold_from_generator(
        self, start_from_fold: Optional[int] = None
    ) -> bool:
        if self.split_generator is None:
            raise RuntimeError(
                "Split generator not initialized. Call create_splits_from_generating_function() first."
            )

        try:
            if start_from_fold is not None:  # Fast-forward generator
                # Ensure generator is reset or handled if called multiple times with start_from_fold
                # This might require re-initializing the generator if it's stateful and consumed.
                # For now, assume nth can be used if the generator is fresh or resettable.
                split_indices_in_patient_df = nth(self.split_generator, start_from_fold)
            else:
                split_indices_in_patient_df = next(
                    self.split_generator
                )  # Advances the generator
        except StopIteration:
            if self.verbose:
                print("No more folds in the generator.")
            return False  # No more folds

        train_indices, val_indices, test_indices = split_indices_in_patient_df

        self.train_patient_ids = self.patient_df.iloc[train_indices]["case_id"].tolist()
        self.val_patient_ids = self.patient_df.iloc[val_indices]["case_id"].tolist()
        self.test_patient_ids = self.patient_df.iloc[test_indices]["case_id"].tolist()

        if self.verbose:
            print(
                f"Set fold: Train Pat. {len(self.train_patient_ids)}, Val Pat. {len(self.val_patient_ids)}, Test Pat. {len(self.test_patient_ids)}"
            )

        # Normalize omic data if present, fitting on current TRAIN split
        if self.omic_features_df is not None and self.train_patient_ids:
            self._fit_omic_scaler(self.train_patient_ids)
        return True

    def get_mil_datasets(
        self,
        mode: str,  # 'path', 'omic', 'pathomic', 'coattn'
        use_hdf5: bool = False,  # For path features
        backbone: Optional[str] = None,  # For path .pt features
        patch_size: str = "",  # For path .pt features
        cache_enabled: bool = False,
    ) -> Tuple[
        Optional[SurvivalMILDataset],
        Optional[SurvivalMILDataset],
        Optional[SurvivalMILDataset],
    ]:
        if self.train_patient_ids is None:  # Check if a fold has been set
            raise ValueError(
                "Splits not set. Call a split creation method and set_next_fold...() first."
            )

        common_params = {
            "patient_slide_dict": self.patient_slide_dict,
            "data_directory": self.data_directory,
            "time_column": self.time_column,
            "event_column": self.event_column,
            "disc_label_column": "disc_label",  # Column name in patient_df for discrete time bin
            "combined_label_column": "label",  # Column name in patient_df for (bin,event) label
            "mode": mode,
            "use_hdf5": use_hdf5,
            "backbone": backbone,
            "patch_size": patch_size,
            "cache_enabled": cache_enabled,
            "omic_names_for_coattn": self.omic_names_for_coattn,  # Pass coattn specific omic names
        }

        datasets = []
        for split_name, patient_ids_list in [
            ("train", self.train_patient_ids),
            ("val", self.val_patient_ids),
            ("test", self.test_patient_ids),
        ]:
            if not patient_ids_list:
                datasets.append(None)
                continue

            # Get patient data for this split
            current_split_patient_df = self.patient_df[
                self.patient_df["case_id"].isin(patient_ids_list)
            ].reset_index(drop=True)

            current_split_omic_df = None
            if self.omic_features_df is not None:
                # Select omic features for current patients
                omic_for_split = self.omic_features_df.loc[patient_ids_list]
                # Apply scaler if fitted (scaler is fitted on train_patient_ids)
                if (
                    self.omic_scalers and "all" in self.omic_scalers
                ):  # Assuming one scaler for all omics for now
                    scaled_omic_data = self.omic_scalers["all"].transform(
                        omic_for_split[self.omic_feature_names]
                    )
                    current_split_omic_df = pd.DataFrame(
                        scaled_omic_data,
                        columns=self.omic_feature_names,
                        index=omic_for_split.index,
                    )
                else:
                    current_split_omic_df = omic_for_split[
                        self.omic_feature_names
                    ].copy()  # No scaling or scaler not ready

            dataset = SurvivalMILDataset(
                patient_data_df=current_split_patient_df,
                omic_features_df_scaled=current_split_omic_df,  # Pass potentially scaled omics
                **common_params,
            )
            datasets.append(dataset)

        return tuple(datasets)

    def _fit_omic_scaler(self, train_patient_ids_for_scaling: List[str]):
        if self.omic_features_df is None or not train_patient_ids_for_scaling:
            self.omic_scalers = None
            return

        # Ensure all train_patient_ids are in omic_features_df index
        train_patient_ids_for_scaling = [
            pid
            for pid in train_patient_ids_for_scaling
            if pid in self.omic_features_df.index
        ]
        if not train_patient_ids_for_scaling:
            print(
                "Warning: No training patients found in omic_features_df for scaling."
            )
            self.omic_scalers = None
            return

        train_omic_data = self.omic_features_df.loc[
            train_patient_ids_for_scaling, self.omic_feature_names
        ]

        # For now, one scaler for all omic features. Can be extended.
        scaler = StandardScaler()
        scaler.fit(train_omic_data)
        self.omic_scalers = {"all": scaler}
        if self.verbose:
            print(
                "Fitted StandardScaler for omic features on the current training split."
            )

    def save_current_split_patient_ids(self, filename: str):
        if self.train_patient_ids is None:
            raise ValueError("Splits not set.")
        max_len = max(
            len(self.train_patient_ids or []),
            len(self.val_patient_ids or []),
            len(self.test_patient_ids or []),
        )

        df = pd.DataFrame(
            {
                "train_ids": pd.Series(self.train_patient_ids or []).reindex(
                    range(max_len)
                ),
                "val_ids": pd.Series(self.val_patient_ids or []).reindex(
                    range(max_len)
                ),
                "test_ids": pd.Series(self.test_patient_ids or []).reindex(
                    range(max_len)
                ),
            }
        )
        df.to_csv(filename, index=False)
        if self.verbose:
            print(f"Survival split patient IDs saved to {filename}")


class SurvivalMILDataset(Dataset):
    def __init__(
        self,
        patient_data_df: pd.DataFrame,  # DF for patients in this specific split
        patient_slide_dict: Dict[
            str, List[str]
        ],  # Full {patient_id: [slide_ids]} mapping
        data_directory: Union[str, Dict[str, str]],
        time_column: str,
        event_column: str,
        disc_label_column: str,  # e.g. "disc_label"
        combined_label_column: str,  # e.g. "label"
        mode: str,  # 'path', 'omic', 'pathomic', 'coattn'
        use_hdf5: bool = False,
        backbone: Optional[str] = None,
        patch_size: str = "",
        cache_enabled: bool = False,
        omic_features_df_scaled: Optional[
            pd.DataFrame
        ] = None,  # Scaled omic features for this split
        omic_names_for_coattn: Optional[List[List[str]]] = None,
    ):
        self.patient_data = patient_data_df
        self.patient_slide_dict = patient_slide_dict
        self.data_directory = data_directory
        self.time_col = time_column
        self.event_col = event_column
        self.disc_label_col = disc_label_column
        self.combined_label_col = combined_label_column
        self.mode = mode
        self.use_hdf5 = use_hdf5  # For path features
        self.backbone = backbone  # For path .pt features
        self.patch_size = str(patch_size) if patch_size is not None else ""
        self.cache_enabled = cache_enabled
        self.path_features_cache: Dict[
            str, torch.Tensor
        ] = {}  # Cache for loaded slide features

        self.omic_features = (
            omic_features_df_scaled  # Already selected and scaled for this split
        )
        self.omic_names_for_coattn = omic_names_for_coattn

        valid_modes = [
            "path",
            "omic",
            "pathomic",
            "coattn",
            "cluster",
        ]  # Added cluster from original
        if self.mode not in valid_modes:
            raise ValueError(
                f"Mode '{self.mode}' not implemented. Valid modes: {valid_modes}"
            )

        if "omic" in self.mode and self.omic_features is None:
            print(
                f"Warning: Mode '{self.mode}' requires omic features, but none were provided/loaded."
            )
        if self.mode == "coattn" and not self.omic_names_for_coattn:
            print(
                "Warning: Mode 'coattn' selected, but omic_names_for_coattn not provided."
            )
        if "path" in self.mode and not self.use_hdf5 and not self.backbone:
            print("Warning: Path-based mode with .pt files, but backbone is not set.")

    def __len__(self) -> int:
        return len(self.patient_data)

    def _load_path_features(
        self, patient_case_id: str, slide_ids_for_patient: List[str]
    ) -> torch.Tensor:
        all_path_features = []
        for slide_id in slide_ids_for_patient:
            # Determine data_dir for this slide (if self.data_directory is a dict)
            current_data_dir_path = self.data_directory
            if isinstance(self.data_directory, dict):
                # This requires 'source' column in the main slide_data from which patient_slide_dict was built
                # For simplicity, assume if data_directory is dict, it applies globally or needs more complex handling
                # Or, assume patient_data_df has a 'source' column if data_directory is a dict
                row_for_source = (
                    self.patient_data[
                        self.patient_data["case_id"] == patient_case_id
                    ].iloc[0]
                    if "source" in self.patient_data.columns
                    else None
                )  # Hacky
                if row_for_source is not None and "source" in row_for_source.index:
                    source = row_for_source["source"]
                    if source not in self.data_directory:
                        raise ValueError(
                            f"Source '{source}' for patient '{patient_case_id}' not in data_directory keys."
                        )
                    current_data_dir_path = self.data_directory[source]
                # else:
                # If no source per patient, and data_directory is dict, this is an issue.
                # Default to first key or raise error if not resolvable.
                # For now, assume string or resolvable dict.

            if not self.use_hdf5:
                if not self.backbone:
                    raise ValueError("Backbone needed for .pt files.")
                patch_subdir = ""
                if self.patch_size and self.patch_size != "512":
                    patch_subdir = self.patch_size
                file_path = os.path.join(
                    current_data_dir_path,
                    patch_subdir,
                    "pt_files",
                    self.backbone,
                    f"{slide_id}.pt",
                )

                if file_path in self.path_features_cache:
                    wsi_bag = self.path_features_cache[file_path]
                else:
                    try:
                        wsi_bag = torch.load(file_path)
                        if self.cache_enabled:
                            self.path_features_cache[file_path] = wsi_bag
                    except FileNotFoundError:
                        # Try without patch_subdir for robustness if original was inconsistent
                        file_path_alt = os.path.join(
                            current_data_dir_path,
                            "pt_files",
                            self.backbone,
                            f"{slide_id}.pt",
                        )
                        try:
                            wsi_bag = torch.load(file_path_alt)
                            if self.cache_enabled:
                                self.path_features_cache[file_path_alt] = wsi_bag
                        except FileNotFoundError:
                            raise FileNotFoundError(
                                f"Path feature file not found for slide {slide_id} at {file_path} or {file_path_alt}"
                            )
                all_path_features.append(wsi_bag)
            else:  # use_hdf5
                # HDF5 usually per patient (case_id.h5) or per slide (slide_id.h5)
                # Assuming slide_id.h5 for now as per original classification.
                # If patient_id.h5, then only one loop needed.
                h5_file_path = os.path.join(
                    current_data_dir_path, "h5_files", f"{slide_id}.h5"
                )
                try:
                    with h5py.File(h5_file_path, "r") as hf:
                        features = torch.from_numpy(hf["features"][:])
                        all_path_features.append(features)
                except OSError:
                    raise OSError(
                        f"HDF5 file not found or corrupted for slide {slide_id} at {h5_file_path}"
                    )

        if not all_path_features:  # No features found for any slide_id of this patient
            # Return a dummy tensor or raise error. For MIL, usually expect some features.
            # Shape needs to match what model expects if this patient has no path data.
            # This depends on feature dim, e.g., (0, 1024)
            print(
                f"Warning: No path features loaded for patient {patient_case_id} (slides: {slide_ids_for_patient}). Returning zero tensor."
            )
            # Try to infer feature dim from backbone or a fixed value
            # This is tricky. For now, a small placeholder. Model must handle 0-dim input.
            return torch.zeros(
                (0, 1)
            )  # Or try to get feature dim if one file was loaded before

        return (
            torch.cat(all_path_features, dim=0)
            if all_path_features
            else torch.zeros((0, 1))
        )

    def __getitem__(self, idx: int) -> tuple:
        patient_row = self.patient_data.iloc[idx]
        case_id = patient_row["case_id"]

        # Labels and time/event info
        # discrete_time_bin = patient_row[self.disc_label_col] # Bin index
        combined_label = patient_row[
            self.combined_label_col
        ]  # (Bin, Event) mapped to int
        event_time = patient_row[self.time_col]
        censorship_status = patient_row[self.event_col]  # 0=censored, 1=event

        slide_ids = self.patient_slide_dict.get(
            case_id, [case_id]
        )  # Fallback to case_id if not in dict

        # Initialize outputs
        path_features = torch.empty(0)  # Placeholder
        omic_data_tensor = torch.empty(0)

        # --- Load Path Features (WSI bags) ---
        if self.mode in ["path", "pathomic", "coattn", "cluster"]:
            path_features = self._load_path_features(case_id, slide_ids)

        # --- Load Omic Features ---
        if self.omic_features is not None and self.mode in [
            "omic",
            "pathomic",
            "coattn",
            "cluster",
        ]:
            if case_id in self.omic_features.index:
                # For 'omic', 'pathomic', 'cluster' - all omics for the patient
                if self.mode != "coattn":
                    omic_data_tensor = torch.tensor(
                        self.omic_features.loc[case_id].values, dtype=torch.float32
                    )
                # For 'coattn' - specific sets of omics
                else:
                    if not self.omic_names_for_coattn:
                        # Return all omics if coattn names not set, or error, or empty
                        print(
                            f"Warning: CoAttn mode but no omic_names_for_coattn for patient {case_id}. Using all omics or empty."
                        )
                        omic_data_tensor = (
                            torch.tensor(
                                self.omic_features.loc[case_id].values,
                                dtype=torch.float32,
                            )
                            if not self.omic_features.empty
                            else torch.empty(0)
                        )

                        # Or, if CoAttn expects multiple omic tensors:
                        # For CoAttn, original returned multiple omic tensors.
                        # This structure needs to be decided. For now, one tensor or a list.
                        # To match original:
                        coattn_omic_tensors = []
                        if self.omic_names_for_coattn:
                            for i, sig_omic_names in enumerate(
                                self.omic_names_for_coattn
                            ):
                                if (
                                    sig_omic_names
                                ):  # If this signature has features defined
                                    # Ensure all names are in omic_features columns
                                    valid_sig_omic_names = [
                                        name
                                        for name in sig_omic_names
                                        if name in self.omic_features.columns
                                    ]
                                    if valid_sig_omic_names:
                                        coattn_omic_tensors.append(
                                            torch.tensor(
                                                self.omic_features.loc[
                                                    case_id, valid_sig_omic_names
                                                ].values,
                                                dtype=torch.float32,
                                            )
                                        )
                                    else:  # No valid features for this signature for this patient
                                        coattn_omic_tensors.append(
                                            torch.empty(0)
                                        )  # Placeholder for this signature
                                else:  # No features defined for this signature
                                    coattn_omic_tensors.append(torch.empty(0))
                        # This changes the return signature for __getitem__ based on mode.
                        # PyTorch DataLoader usually expects consistent return types.
                        # It's better to return a dict or a fixed tuple structure.
                        # For now, let's stick to path_features, omic_data_tensor, label, event_time, c
                        # And CoAttn model would need to split omic_data_tensor if it's concatenated.
                        # OR, if mode is 'coattn', omic_data_tensor could be a list/tuple of tensors.
                        # Let's assume for coattn, omic_data_tensor IS the list of tensors for now.
                        if self.mode == "coattn":
                            omic_data_tensor = (
                                tuple(coattn_omic_tensors)
                                if coattn_omic_tensors
                                else tuple(
                                    torch.empty(0)
                                    for _ in range(
                                        len(self.omic_names_for_coattn or [])
                                    )
                                )
                            )

            else:  # Patient not in omic_features (e.g. missing data)
                print(
                    f"Warning: Omic data not found for patient {case_id}. Returning empty tensor for omics."
                )
                if self.mode == "coattn" and self.omic_names_for_coattn:
                    omic_data_tensor = tuple(
                        torch.empty(0) for _ in self.omic_names_for_coattn
                    )
                else:  # For other omic modes
                    # Try to get expected shape for omic from the dataframe columns
                    num_omic_features = (
                        len(self.omic_features.columns)
                        if self.omic_features is not None
                        else 0
                    )
                    omic_data_tensor = torch.zeros(
                        num_omic_features, dtype=torch.float32
                    )

        # --- Construct return tuple based on mode ---
        # This needs to be consistent for the DataLoader.
        # (path_features, omic_features, label (combined), event_time, censorship_status)
        # If a mode doesn't use one, it can be a placeholder (e.g., torch.empty(0)).

        if self.mode == "path":
            return (
                path_features,
                torch.empty(0),
                combined_label,
                event_time,
                censorship_status,
            )
        elif self.mode == "omic":
            return (
                torch.empty(0),
                omic_data_tensor,
                combined_label,
                event_time,
                censorship_status,
            )
        elif self.mode == "pathomic":
            return (
                path_features,
                omic_data_tensor,
                combined_label,
                event_time,
                censorship_status,
            )
        elif self.mode == "coattn":
            # `omic_data_tensor` is already a tuple of tensors for coattn here
            return (
                path_features,
                omic_data_tensor,
                combined_label,
                event_time,
                censorship_status,
            )
        elif self.mode == "cluster":  # Original 'cluster' mode also had 'cluster_ids'
            # cluster_ids logic was: self.fname2ids[slide_id[:-4]+'.pt']
            # This fname2ids needs to be loaded and passed, typically by SurvivalDataManager
            # For now, returning placeholder for cluster_ids
            cluster_ids_placeholder = torch.empty(0)  # Placeholder
            return (
                path_features,
                cluster_ids_placeholder,
                omic_data_tensor,
                combined_label,
                event_time,
                censorship_status,
            )
        else:  # Should not happen due to check in __init__
            raise NotImplementedError(f"Mode {self.mode} data assembly not defined.")

    def set_backbone(self, backbone: str):
        self.backbone = backbone

    def set_patch_size(self, size: str):
        self.patch_size = str(size)

    def load_from_hdf5(self, use_hdf5: bool):
        self.use_hdf5 = use_hdf5



================================================
File: sauron/feature_extraction/processor.py
================================================
# sauron/feature_extraction/processor.py

from __future__ import annotations

import json
import logging
import os
import shutil
import sys
import warnings
from inspect import signature
from typing import Any, Dict, List, Optional, TypeAlias

import geopandas as gpd
import pandas as pd

# External Dependencies
import torch
from tqdm import tqdm

# Sauron Internal Dependencies (New Structure)
from .models.patch_encoders.factory import encoder_factory as patch_encoder_factory
from .models.slide_encoders.factory import encoder_factory as slide_encoder_factory
from .models.slide_encoders.factory import (
    slide_to_patch_encoder_name,  # Mapping needed for slide feature extraction
)
from .utils.config import JSONsaver  # For saving config
from .utils.io import create_lock, is_locked, remove_lock, update_log
from .utils.misc import deprecated  # For deprecated methods
from .wsi.base import WSI  # Import the base WSI class for type hinting
from .wsi.factory import (
    OPENSLIDE_EXTENSIONS,
    PIL_EXTENSIONS,
    WSIReaderType,
    load_wsi,
)

# --- Setup Basic Logging ---
# Configure logging to show informational messages
logging.basicConfig(
    level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s"
)
logger = logging.getLogger(__name__)


# --- Type Aliases for Clarity ---
PathLike: TypeAlias = str | os.PathLike
SegmentationModel: TypeAlias = torch.nn.Module
PatchEncoderModel: TypeAlias = torch.nn.Module
SlideEncoderModel: TypeAlias = torch.nn.Module


class Processor:
    """
    Orchestrates the preprocessing pipeline for Whole Slide Images (WSIs).

    Handles WSI loading, caching, tissue segmentation, patch coordinate extraction,
    and feature extraction at both patch and slide levels using specified models.
    Manages file paths, logging, configuration, and error handling for batch processing.

    Args:
        job_dir (PathLike): Base directory to save all processing results (segmentations,
            patches, features, logs, configs). Must exist.
        wsi_source (PathLike): Directory containing the raw WSI files.
        wsi_ext (Optional[List[str]]): List of file extensions to consider as WSIs
            (e.g., ['.svs', '.ndpi']). If None, uses a default list covering common
            OpenSlide and PIL formats. Defaults to None.
        wsi_cache (Optional[PathLike]): Optional local directory to cache WSIs before
            processing. Useful if `wsi_source` is on a slow network drive.
            Defaults to None.
        clear_cache (bool): If True and `wsi_cache` is provided, delete WSIs from
            the cache after they are processed. Defaults to False.
        skip_errors (bool): If True, log errors encountered during processing a
            single slide and continue with the next. If False, raise the exception
            and stop processing. Defaults to False.
        custom_mpp_keys (Optional[List[str]]): List of custom metadata keys to check
            within WSI properties for retrieving microns-per-pixel (MPP) values.
            Defaults to None.
        custom_list_of_wsis (Optional[PathLike]): Path to a CSV file defining a
            specific list of WSIs to process. The CSV must contain a column named 'wsi'
            (filenames with extensions). It can optionally contain an 'mpp' column to
            override MPP detection. If provided, only WSIs listed in this file and
            found in `wsi_source` will be processed. Defaults to None.
        max_workers (Optional[int]): Maximum number of worker processes for PyTorch
            DataLoaders during feature extraction. If None, a default based on
            CPU cores and batch size is used. Defaults to None.
        reader_type (Optional[WSIReaderType]): Force the use of a specific WSI reading
            backend ('openslide', 'image', 'cucim'). If None, the backend is chosen
            automatically based on the file extension. Defaults to None.

    Raises:
        EnvironmentError: If Python version is less than 3.9.
        AssertionError: If `wsi_ext` is provided but is not a list or if extensions
            do not start with a period.
        ValueError: If `custom_list_of_wsis` CSV does not contain a 'wsi' column,
                    or if other configuration issues arise.
        FileNotFoundError: If specified WSI files, `job_dir`, `wsi_source`, or
                           necessary resources (like the CSV) are missing.
    """

    def __init__(
        self,
        job_dir: PathLike,
        wsi_source: PathLike,
        wsi_ext: Optional[List[str]] = None,
        wsi_cache: Optional[PathLike] = None,
        clear_cache: bool = False,
        skip_errors: bool = False,
        custom_mpp_keys: Optional[List[str]] = None,
        custom_list_of_wsis: Optional[PathLike] = None,
        max_workers: Optional[int] = None,
        reader_type: Optional[WSIReaderType] = None,
    ) -> None:
        # --- Basic Environment and Path Checks ---
        if not (sys.version_info.major >= 3 and sys.version_info.minor >= 9):
            raise EnvironmentError(
                "Sauron Feature Extraction requires Python 3.9 or above. Python 3.10 is recommended."
            )

        if not os.path.isdir(job_dir):
            raise FileNotFoundError(f"Job directory does not exist: {job_dir}")
        if not os.path.isdir(wsi_source):
            raise FileNotFoundError(
                f"WSI source directory does not exist: {wsi_source}"
            )
        if wsi_cache and not os.path.isdir(wsi_cache):
            logger.info(f"Cache directory {wsi_cache} not found. Creating it.")
            os.makedirs(wsi_cache, exist_ok=True)
        if custom_list_of_wsis and not os.path.isfile(custom_list_of_wsis):
            raise FileNotFoundError(
                f"Custom WSI list file not found: {custom_list_of_wsis}"
            )

        # --- Assign Core Attributes ---
        self.job_dir = os.path.abspath(job_dir)
        self.wsi_source = os.path.abspath(wsi_source)
        self.wsi_cache = os.path.abspath(wsi_cache) if wsi_cache else None
        self.wsi_ext = wsi_ext or (list(PIL_EXTENSIONS) + list(OPENSLIDE_EXTENSIONS))
        self.clear_cache = clear_cache
        self.skip_errors = skip_errors
        self.custom_mpp_keys = custom_mpp_keys
        self.max_workers = max_workers
        self.reader_type = reader_type

        # --- Validate WSI Extensions ---
        self._validate_wsi_extensions()

        # --- Identify and Filter Target WSIs ---
        self.wsis: List[WSI] = self._load_and_filter_wsi_objects(custom_list_of_wsis)

        logger.info(f"Processor initialized. Targeting {len(self.wsis)} slides.")
        if self.wsi_cache:
            logger.info(
                f"Using local cache at {self.wsi_cache}. Current files: {len(os.listdir(self.wsi_cache))}"
            )

    def _validate_wsi_extensions(self):
        """Validates the format of the wsi_ext list."""
        if not isinstance(self.wsi_ext, list):
            raise AssertionError(
                f"`wsi_ext` must be a list of file extensions, got {self.wsi_ext} of type {type(self.wsi_ext)}"
            )
        for ext in self.wsi_ext:
            if not ext.startswith("."):
                raise AssertionError(
                    f"Each extension in `wsi_ext` must start with a period (.). Found: {ext}"
                )
            # Store extensions in lowercase for case-insensitive matching
            self.wsi_ext = [e.lower() for e in self.wsi_ext]

    def _load_and_filter_wsi_objects(
        self, custom_list_path: Optional[PathLike]
    ) -> List[WSI]:
        """Identifies WSI files, filters based on CSV if provided, and creates WSI objects."""
        available_slides_in_source = {
            name: os.path.join(self.wsi_source, name)
            for name in os.listdir(self.wsi_source)
            if os.path.splitext(name)[1].lower() in self.wsi_ext
        }
        logger.info(
            f"Found {len(available_slides_in_source)} slides matching extensions in {self.wsi_source}."
        )

        target_slides_info = {}  # Dict[filename, {'path': path, 'mpp': mpp_or_none}]

        if custom_list_path is not None:
            try:
                wsi_df = pd.read_csv(custom_list_path)
            except Exception as e:
                raise ValueError(
                    f"Error reading custom WSI list CSV {custom_list_path}: {e}"
                )

            if "wsi" not in wsi_df.columns:
                raise ValueError(
                    "Custom WSI list CSV must contain a column named 'wsi'."
                )

            has_mpp_column = "mpp" in wsi_df.columns
            processed_filenames = set()

            for _, row in wsi_df.iterrows():
                filename = str(row["wsi"]).strip()
                if not filename:
                    continue

                if filename in processed_filenames:
                    logger.warning(
                        f"Duplicate filename '{filename}' found in CSV. Skipping subsequent entries."
                    )
                    continue
                processed_filenames.add(filename)

                if filename in available_slides_in_source:
                    mpp_value = None
                    if has_mpp_column and pd.notna(row["mpp"]):
                        try:
                            mpp_value = float(row["mpp"])
                        except ValueError:
                            logger.warning(
                                f"Could not parse MPP value '{row['mpp']}' for WSI '{filename}'. Will attempt auto-detection."
                            )
                    target_slides_info[filename] = {
                        "path": available_slides_in_source[filename],
                        "mpp": mpp_value,
                    }
                else:
                    logger.warning(
                        f"WSI '{filename}' listed in CSV but not found in source directory: {self.wsi_source}. Skipping."
                    )
        else:
            # Process all found slides if no custom list
            for filename, filepath in available_slides_in_source.items():
                target_slides_info[filename] = {"path": filepath, "mpp": None}

        # --- Create WSI Objects ---
        wsi_objects = []
        init_log_path = os.path.join(self.job_dir, "_processor_init_log.txt")
        sorted_target_filenames = sorted(target_slides_info.keys())

        for filename in sorted_target_filenames:
            info = target_slides_info[filename]
            wsi_load_path = (
                os.path.join(self.wsi_cache, filename)
                if self.wsi_cache
                else info["path"]
            )

            slide_name_no_ext = os.path.splitext(filename)[0]
            # Standardized path for segmentation results
            tissue_seg_path = os.path.join(
                self.job_dir,
                "segmentation_results",
                "contours_geojson",
                f"{slide_name_no_ext}.geojson",
            )
            if not os.path.exists(tissue_seg_path):
                tissue_seg_path = None

            try:
                slide = load_wsi(
                    slide_path=wsi_load_path,
                    original_path=info["path"],
                    name=filename,
                    tissue_seg_path=tissue_seg_path,
                    custom_mpp_keys=self.custom_mpp_keys,
                    mpp=info["mpp"],
                    max_workers=self.max_workers,
                    reader_type=self.reader_type,
                    lazy_init=True,
                )
                wsi_objects.append(slide)
                update_log(init_log_path, filename, "INFO - WSI object created")
            except Exception as e:
                message = (
                    f"ERROR creating WSI object for {filename} at {wsi_load_path}: {e}"
                )
                update_log(init_log_path, filename, message)
                if self.skip_errors:
                    logger.error(message)
                else:
                    raise RuntimeError(message) from e

        return wsi_objects

    def _get_job_paths(self, job_name: str, sub_dirs: List[str]) -> Dict[str, str]:
        """Helper to create and return paths for a specific processing job."""
        base_dir = os.path.join(self.job_dir, job_name)
        paths = {"base": base_dir}
        for sub in sub_dirs:
            path = os.path.join(base_dir, sub)
            os.makedirs(path, exist_ok=True)
            paths[sub] = path
        paths["config"] = os.path.join(base_dir, f"_config_{job_name}.json")
        paths["log"] = os.path.join(base_dir, f"_log_{job_name}.txt")
        return paths

    def populate_cache(self) -> None:
        """Copies WSI files from the source directory to the local cache directory."""
        if not self.wsi_cache:
            logger.info("No cache directory specified. Skipping cache population.")
            return

        cache_log_path = os.path.join(self.wsi_cache, "_cache_log.txt")
        logger.info(f"Populating cache directory: {self.wsi_cache}")
        progress_bar = tqdm(
            self.wsis, desc="Populating cache", total=len(self.wsis), unit="slide"
        )

        for wsi in progress_bar:
            slide_fullname = wsi.name + wsi.ext
            cache_file_path = os.path.join(self.wsi_cache, slide_fullname)
            source_file_path = wsi.original_path

            progress_bar.set_postfix_str(f"{slide_fullname}")

            if os.path.exists(cache_file_path) and not is_locked(cache_file_path):
                update_log(cache_log_path, slide_fullname, "INFO - Already in cache")
                continue

            if is_locked(cache_file_path):
                update_log(
                    cache_log_path, slide_fullname, "SKIP - Locked by another process"
                )
                continue

            try:
                create_lock(cache_file_path)
                update_log(cache_log_path, slide_fullname, "LOCK - Copying")
                shutil.copy2(source_file_path, cache_file_path)
                update_log(cache_log_path, slide_fullname, "OK - Copied")
            except Exception as e:
                error_msg = f"ERROR copying: {e}"
                update_log(cache_log_path, slide_fullname, error_msg)
                logger.error(f"Failed to copy {source_file_path} to cache: {e}")
                # Attempt cleanup, ignore errors
                try:
                    if os.path.exists(cache_file_path + ".lock"):
                        remove_lock(cache_file_path)
                    if os.path.exists(cache_file_path):  # Remove partially copied file
                        os.remove(cache_file_path)
                except OSError:
                    pass
            finally:
                if os.path.exists(cache_file_path + ".lock"):
                    try:
                        remove_lock(cache_file_path)
                    except OSError as lock_err:
                        logger.warning(
                            f"Could not remove lock file for {cache_file_path} after operation: {lock_err}"
                        )

    def run_segmentation_job(
        self,
        segmentation_model: SegmentationModel,
        seg_mag: int = 10,
        holes_are_tissue: bool = False,
        batch_size: int = 16,
        artifact_remover_model: Optional[SegmentationModel] = None,
        device: str = "cuda:0",
    ) -> str:
        """
        Performs tissue segmentation on the targeted WSIs.

        Uses the provided `segmentation_model` to identify tissue regions.
        Optionally uses an `artifact_remover_model` for refinement. Saves results
        (thumbnails, contours, GeoJSON) in subdirectories under `job_dir`.

        Args:
            segmentation_model: A pre-trained PyTorch model for tissue segmentation.
            seg_mag: Target magnification (e.g., 10 for 10x) for segmentation. Defaults to 10.
            holes_are_tissue: If True, holes within tissue contours are considered tissue.
                              If False, they are excluded. Defaults to False.
            batch_size: Batch size for model inference during segmentation. Defaults to 16.
            artifact_remover_model: Optional second model to refine segmentation, often
                                    used to remove artifacts like pen marks. Defaults to None.
            device: The device for PyTorch computations (e.g., 'cuda:0', 'cpu'). Defaults to 'cuda:0'.

        Returns:
            Absolute path to the directory where GeoJSON contour files are saved.

        Raises:
            RuntimeError: If an error occurs during segmentation and `skip_errors` is False.
        """
        segmentation_dir = os.path.join(self.job_dir, "segmentation_results")
        geojson_dir = os.path.join(segmentation_dir, "contours_geojson")
        contour_img_dir = os.path.join(segmentation_dir, "contours")  # Visualizations
        thumbnail_dir = os.path.join(segmentation_dir, "thumbnails")  # Raw thumbnails
        os.makedirs(geojson_dir, exist_ok=True)
        os.makedirs(contour_img_dir, exist_ok=True)
        os.makedirs(thumbnail_dir, exist_ok=True)

        job_name = "segmentation_results"
        paths = self._get_job_paths(
            job_name, ["contours_geojson", "contours", "thumbnails"]
        )
        geojson_dir = paths["contours_geojson"]
        log_fp = paths["log"]

        # --- Save Configuration ---
        sig = signature(self.run_segmentation_job)
        local_attrs = {k: v for k, v in locals().items() if k in sig.parameters}
        # Add model names if available
        if hasattr(
            segmentation_model, "model_name"
        ):  # Assuming models might have this attr
            local_attrs["segmentation_model_name"] = segmentation_model.model_name
        if artifact_remover_model and hasattr(artifact_remover_model, "model_name"):
            local_attrs["artifact_remover_model_name"] = (
                artifact_remover_model.model_name
            )
        self.save_config(
            saveto=paths["config"],
            local_attrs=local_attrs,
            ignore=["self", "segmentation_model", "artifact_remover_model"],
        )
        logger.info(f"Starting segmentation job. Results will be in {paths['base']}")

        progress_bar = tqdm(
            self.wsis, desc="Segmenting tissue", total=len(self.wsis), unit="slide"
        )

        for wsi in progress_bar:
            slide_fullname = wsi.name + wsi.ext
            geojson_path = os.path.join(geojson_dir, f"{wsi.name}.geojson")
            progress_bar.set_postfix_str(f"{slide_fullname}")

            # --- Pre-computation Checks ---
            if os.path.exists(geojson_path) and not is_locked(geojson_path):
                update_log(log_fp, slide_fullname, "DONE - Already segmented")
                self.cleanup(slide_fullname)
                continue
            if is_locked(geojson_path):
                update_log(log_fp, slide_fullname, "SKIP - Locked")
                continue
            wsi_current_path = (
                os.path.join(self.wsi_cache, slide_fullname)
                if self.wsi_cache
                else wsi.original_path
            )
            if not os.path.exists(
                wsi_current_path
            ):  # Check existence, ignore lock here
                update_log(
                    log_fp,
                    slide_fullname,
                    f"SKIP - WSI not found at {wsi_current_path}",
                )
                continue

            # --- Perform Segmentation ---
            try:
                create_lock(geojson_path)
                update_log(log_fp, slide_fullname, "LOCK - Segmenting")
                wsi._lazy_initialize()  # Ensure WSI is loaded

                generated_geojson_path = wsi.segment_tissue(
                    segmentation_model=segmentation_model,
                    target_mag=seg_mag,
                    holes_are_tissue=holes_are_tissue,
                    job_dir=paths["base"],  # Pass base segmentation dir
                    batch_size=batch_size,
                    device=device,
                    verbose=False,
                )

                if artifact_remover_model is not None:
                    logger.info(f"Applying artifact remover to {slide_fullname}")
                    generated_geojson_path = wsi.segment_tissue(
                        segmentation_model=artifact_remover_model,
                        target_mag=getattr(
                            artifact_remover_model, "target_mag", seg_mag
                        ),
                        holes_are_tissue=False,
                        job_dir=paths["base"],
                        batch_size=batch_size,
                        device=device,
                        verbose=False,
                    )

                # Verify output
                if not os.path.exists(generated_geojson_path):
                    raise FileNotFoundError(
                        f"Segmentation output {generated_geojson_path} not created."
                    )
                try:
                    gdf = gpd.read_file(generated_geojson_path, rows=1)
                    status = "DONE - Segmented"
                    if gdf.empty:
                        status = "WARN - Empty GeoDataFrame"
                        logger.warning(
                            f"Empty segmentation result for {slide_fullname}"
                        )
                    update_log(log_fp, slide_fullname, status)
                except Exception as gdf_err:
                    update_log(
                        log_fp, slide_fullname, f"ERROR reading GeoJSON: {gdf_err}"
                    )
                    raise ValueError(
                        f"Could not read generated GeoJSON: {gdf_err}"
                    ) from gdf_err

            except Exception as e:
                error_msg = f"ERROR during segmentation: {e}"
                update_log(log_fp, slide_fullname, error_msg)
                logger.error(f"Error segmenting {slide_fullname}: {e}")
                if isinstance(e, KeyboardInterrupt):
                    print("Segmentation interrupted.")
                    raise e
                if not self.skip_errors:
                    raise RuntimeError(f"Error segmenting {slide_fullname}: {e}") from e
                # Continue loop if skipping errors
            finally:
                if os.path.exists(geojson_path + ".lock"):
                    try:
                        remove_lock(geojson_path)
                    except OSError as lock_err:
                        logger.warning(
                            f"Could not remove lock {geojson_path}.lock: {lock_err}"
                        )
                self.cleanup(slide_fullname)
                wsi.close()  # Close WSI handle

        logger.info(f"Segmentation job finished. GeoJSONs in: {geojson_dir}")
        return geojson_dir

    def run_patching_job(
        self,
        target_magnification: int,
        patch_size: int,
        overlap: int = 0,
        patch_dir_name: Optional[str] = None,
        visualize: bool = True,
        min_tissue_proportion: float = 0.0,
    ) -> str:
        """Extracts patch coordinates from segmented tissue regions for each WSI."""
        if patch_dir_name is None:
            patch_dir_name = (
                f"patches_{target_magnification}x_{patch_size}px_{overlap}ovlp"
            )

        paths = self._get_job_paths(
            patch_dir_name, ["patches", "visualization"] if visualize else ["patches"]
        )
        coords_h5_dir = paths["patches"]  # HDF5 files go here
        viz_dir = paths.get("visualization")  # Will be None if visualize=False
        log_fp = paths["log"]

        # --- Save Configuration ---
        sig = signature(self.run_patching_job)
        local_attrs = {k: v for k, v in locals().items() if k in sig.parameters}
        self.save_config(
            saveto=paths["config"], local_attrs=local_attrs, ignore=["self"]
        )
        logger.info(f"Starting patching job. Results will be in {paths['base']}")

        progress_bar = tqdm(
            self.wsis,
            desc=f"Extracting patch coordinates ({patch_dir_name})",
            total=len(self.wsis),
            unit="slide",
        )

        for wsi in progress_bar:
            slide_fullname = wsi.name + wsi.ext
            coords_h5_path = os.path.join(coords_h5_dir, f"{wsi.name}_patches.h5")
            progress_bar.set_postfix_str(f"{slide_fullname}")

            # --- Pre-computation Checks ---
            if os.path.exists(coords_h5_path) and not is_locked(coords_h5_path):
                update_log(log_fp, slide_fullname, "DONE - Coords already generated")
                self.cleanup(slide_fullname)
                continue
            if is_locked(coords_h5_path):
                update_log(log_fp, slide_fullname, "SKIP - Locked")
                continue
            wsi_current_path = (
                os.path.join(self.wsi_cache, slide_fullname)
                if self.wsi_cache
                else wsi.original_path
            )
            if not os.path.exists(wsi_current_path):
                update_log(
                    log_fp,
                    slide_fullname,
                    f"SKIP - WSI not found at {wsi_current_path}",
                )
                continue
            segmentation_path = wsi.tissue_seg_path  # Should be set if segmentation ran
            if segmentation_path is None or not os.path.exists(segmentation_path):
                update_log(
                    log_fp, slide_fullname, "SKIP - Segmentation GeoJSON not found"
                )
                continue
            try:  # Check if GeoJSON is empty
                gdf = gpd.read_file(segmentation_path, rows=1)
                if gdf.empty:
                    update_log(
                        log_fp,
                        slide_fullname,
                        "SKIP - Empty GeoDataFrame for segmentation",
                    )
                    continue
            except Exception as gdf_err:
                update_log(log_fp, slide_fullname, f"ERROR reading GeoJSON: {gdf_err}")
                if not self.skip_errors:
                    raise RuntimeError(
                        f"Error reading GeoJSON {segmentation_path}"
                    ) from gdf_err
                continue

            # --- Perform Patching ---
            try:
                create_lock(coords_h5_path)
                update_log(log_fp, slide_fullname, "LOCK - Generating coords")
                wsi._lazy_initialize()  # Ensure WSI loaded

                generated_coords_path = wsi.extract_tissue_coords(
                    target_mag=target_magnification,
                    patch_size=patch_size,
                    save_coords=paths["base"],  # Pass base dir for patching job
                    overlap=overlap,
                    min_tissue_proportion=min_tissue_proportion,
                )

                if not os.path.exists(generated_coords_path):
                    raise FileNotFoundError(
                        f"Coordinate file {generated_coords_path} not created."
                    )

                if viz_dir:
                    wsi.visualize_coords(
                        coords_path=generated_coords_path,
                        save_patch_viz=viz_dir,
                    )
                update_log(log_fp, slide_fullname, "DONE - Coords generated")

            except Exception as e:
                error_msg = f"ERROR during patching: {e}"
                update_log(log_fp, slide_fullname, error_msg)
                logger.error(f"Error patching {slide_fullname}: {e}")
                if isinstance(e, KeyboardInterrupt):
                    print("Patching interrupted.")
                    raise e
                if not self.skip_errors:
                    raise RuntimeError(f"Error patching {slide_fullname}: {e}") from e
            finally:
                if os.path.exists(coords_h5_path + ".lock"):
                    try:
                        remove_lock(coords_h5_path)
                    except OSError as lock_err:
                        logger.warning(
                            f"Could not remove lock {coords_h5_path}.lock: {lock_err}"
                        )
                self.cleanup(slide_fullname)
                wsi.close()

        logger.info(f"Patching job finished. Coordinates in: {coords_h5_dir}")
        return coords_h5_dir  # Return path to HDF5 coordinate files

    @deprecated
    def run_feature_extraction_job(self, *args, **kwargs):
        """Deprecated alias for run_patch_feature_extraction_job."""
        warnings.warn(
            "`run_feature_extraction_job` is deprecated. Use `run_patch_feature_extraction_job` instead.",
            DeprecationWarning,
            stacklevel=2,
        )
        # Map old 'saveto' kwarg to new 'features_dir_name' if present
        if "saveto" in kwargs:
            kwargs["features_dir_name"] = kwargs.pop("saveto")
        return self.run_patch_feature_extraction_job(*args, **kwargs)

    def run_patch_feature_extraction_job(
        self,
        coords_h5_dir: str,  # Dir containing HDF5 patch coord files
        patch_encoder: PatchEncoderModel,
        device: str = "cuda:0",
        saveas: str = "h5",
        batch_limit: int = 512,
        features_dir_name: Optional[str] = None,
    ) -> str:
        """Extracts patch-level features using a specified patch encoder model."""
        # --- Determine Paths ---
        if not os.path.isdir(coords_h5_dir):
            raise FileNotFoundError(f"Coordinates directory not found: {coords_h5_dir}")
        # Assume coords_h5_dir is like .../job_dir/patch_job_name/patches/
        patching_base_dir = os.path.dirname(coords_h5_dir)

        enc_name = getattr(patch_encoder, "enc_name", "custom_encoder")
        if features_dir_name is None:
            features_dir_name = f"features_{enc_name}"

        # Feature files will live alongside the 'patches' dir
        features_base_dir = os.path.join(patching_base_dir, features_dir_name)
        os.makedirs(features_base_dir, exist_ok=True)

        paths = {
            "base": features_base_dir,
            "config": os.path.join(features_base_dir, "_config_patch_features.json"),
            "log": os.path.join(features_base_dir, "_log_patch_features.txt"),
        }
        log_fp = paths["log"]

        # --- Save Configuration ---
        sig = signature(self.run_patch_feature_extraction_job)
        local_attrs = {k: v for k, v in locals().items() if k in sig.parameters}
        local_attrs["patch_encoder_name"] = enc_name
        self.save_config(
            saveto=paths["config"],
            local_attrs=local_attrs,
            ignore=["self", "patch_encoder"],
        )
        logger.info(
            f"Starting patch feature extraction ({features_dir_name}). Results in {paths['base']}"
        )

        progress_bar = tqdm(
            self.wsis,
            desc=f"Extracting patch features ({features_dir_name})",
            total=len(self.wsis),
            unit="slide",
        )

        for wsi in progress_bar:
            slide_fullname = wsi.name + wsi.ext
            coord_h5_path = os.path.join(coords_h5_dir, f"{wsi.name}_patches.h5")
            feature_file_path = os.path.join(features_base_dir, f"{wsi.name}.{saveas}")
            progress_bar.set_postfix_str(f"{slide_fullname}")

            # --- Pre-computation Checks ---
            if os.path.exists(feature_file_path) and not is_locked(feature_file_path):
                update_log(log_fp, slide_fullname, "DONE - Features already extracted")
                self.cleanup(slide_fullname)
                continue
            if is_locked(feature_file_path):
                update_log(log_fp, slide_fullname, "SKIP - Locked")
                continue
            wsi_current_path = (
                os.path.join(self.wsi_cache, slide_fullname)
                if self.wsi_cache
                else wsi.original_path
            )
            if not os.path.exists(wsi_current_path):
                update_log(
                    log_fp,
                    slide_fullname,
                    f"SKIP - WSI not found at {wsi_current_path}",
                )
                continue
            if not os.path.exists(coord_h5_path):
                update_log(
                    log_fp,
                    slide_fullname,
                    f"SKIP - Coordinate file not found: {coord_h5_path}",
                )
                continue

            # --- Perform Feature Extraction ---
            try:
                create_lock(feature_file_path)
                update_log(log_fp, slide_fullname, "LOCK - Extracting patch features")
                wsi._lazy_initialize()  # Ensure WSI loaded

                generated_feature_path = wsi.extract_patch_features(
                    patch_encoder=patch_encoder,
                    coords_path=coord_h5_path,
                    save_features=features_base_dir,  # Pass the target directory
                    device=device,
                    saveas=saveas,
                    batch_limit=batch_limit,
                )

                if not os.path.exists(generated_feature_path):
                    raise FileNotFoundError(
                        f"Feature file {generated_feature_path} not created."
                    )
                update_log(log_fp, slide_fullname, "DONE - Features extracted")

            except Exception as e:
                error_msg = f"ERROR during patch feature extraction: {e}"
                update_log(log_fp, slide_fullname, error_msg)
                logger.error(
                    f"Error extracting patch features for {slide_fullname}: {e}"
                )
                if isinstance(e, KeyboardInterrupt):
                    print("Patch feature extraction interrupted.")
                    raise e
                if not self.skip_errors:
                    raise RuntimeError(
                        f"Error extracting patch features for {slide_fullname}: {e}"
                    ) from e
            finally:
                if os.path.exists(feature_file_path + ".lock"):
                    try:
                        remove_lock(feature_file_path)
                    except OSError as lock_err:
                        logger.warning(
                            f"Could not remove lock {feature_file_path}.lock: {lock_err}"
                        )
                self.cleanup(slide_fullname)
                wsi.close()

        logger.info(
            f"Patch feature extraction finished. Features in: {features_base_dir}"
        )
        return features_base_dir

    def run_slide_feature_extraction_job(
        self,
        slide_encoder: SlideEncoderModel,
        patch_features_dir: str,
        device: str = "cuda:0",
        saveas: str = "h5",
        batch_limit_for_patch_features: int = 512,
        slide_features_dir_name: Optional[str] = None,
    ) -> str:
        """Extracts slide-level features using a specified slide encoder model."""
        # --- Determine Paths and Required Patch Encoder ---
        if not os.path.isdir(patch_features_dir):
            raise FileNotFoundError(
                f"Patch features directory not found: {patch_features_dir}"
            )
        patching_base_dir = os.path.dirname(
            patch_features_dir
        )  # e.g., job_dir/patches.../

        slide_enc_name = getattr(slide_encoder, "enc_name", "custom_slide_encoder")
        required_patch_enc_name = None
        if slide_enc_name.startswith("mean-"):
            required_patch_enc_name = slide_enc_name.split("mean-", 1)[1]
        elif slide_enc_name in slide_to_patch_encoder_name:
            required_patch_enc_name = slide_to_patch_encoder_name[slide_enc_name]

        # Verify input patch_features_dir name consistency (optional check)
        if required_patch_enc_name and not os.path.basename(
            patch_features_dir
        ).endswith(f"features_{required_patch_enc_name}"):
            logger.warning(
                f"Input `patch_features_dir` ('{os.path.basename(patch_features_dir)}') "
                f"might not match expected features for slide encoder '{slide_enc_name}' "
                f"(expected suffix: 'features_{required_patch_enc_name}'). Ensure features are correct."
            )

        # Determine output directory for slide features
        if slide_features_dir_name is None:
            slide_features_dir_name = f"slide_features_{slide_enc_name}"
        slide_features_base_dir = os.path.join(
            patching_base_dir, slide_features_dir_name
        )
        os.makedirs(slide_features_base_dir, exist_ok=True)

        paths = {
            "base": slide_features_base_dir,
            "config": os.path.join(
                slide_features_base_dir, "_config_slide_features.json"
            ),
            "log": os.path.join(slide_features_base_dir, "_log_slide_features.txt"),
        }
        log_fp = paths["log"]

        # --- Auto-generate Patch Features if Missing ---
        if required_patch_enc_name:
            missing_patch_features = any(
                not os.path.exists(os.path.join(patch_features_dir, f"{wsi.name}.h5"))
                for wsi in self.wsis
            )
            if missing_patch_features:
                logger.warning(
                    f"Required patch features ('{required_patch_enc_name}') missing in '{patch_features_dir}'. Attempting generation."
                )
                try:
                    patch_encoder = patch_encoder_factory(required_patch_enc_name)
                    coords_h5_dir = os.path.join(
                        patching_base_dir, "patches"
                    )  # Assumes standard structure
                    if not os.path.isdir(coords_h5_dir):
                        raise FileNotFoundError(
                            f"Coordinate directory '{coords_h5_dir}' needed for auto-generation not found."
                        )

                    # Run the patch feature job, ensuring it saves to the correct input dir for *this* job
                    self.run_patch_feature_extraction_job(
                        coords_h5_dir=coords_h5_dir,
                        patch_encoder=patch_encoder,
                        device=device,
                        saveas="h5",  # Must be h5
                        batch_limit=batch_limit_for_patch_features,
                        features_dir_name=os.path.basename(
                            patch_features_dir
                        ),  # Save to the dir we need
                    )
                    logger.info("Attempted patch feature generation.")
                except Exception as patch_gen_e:
                    raise RuntimeError(
                        f"Failed to auto-generate required patch features ('{required_patch_enc_name}'). "
                        f"Generate manually or ensure they exist in '{patch_features_dir}'. Error: {patch_gen_e}"
                    ) from patch_gen_e
        elif not slide_enc_name.startswith("mean-"):
            logger.warning(
                f"Cannot auto-generate patch features for '{slide_enc_name}' (unknown requirement). Ensure they exist in '{patch_features_dir}'."
            )

        # --- Save Configuration ---
        sig = signature(self.run_slide_feature_extraction_job)
        local_attrs = {k: v for k, v in locals().items() if k in sig.parameters}
        local_attrs["slide_encoder_name"] = slide_enc_name
        self.save_config(
            saveto=paths["config"],
            local_attrs=local_attrs,
            ignore=["self", "slide_encoder"],
        )
        logger.info(
            f"Starting slide feature extraction ({slide_features_dir_name}). Results in {paths['base']}"
        )

        progress_bar = tqdm(
            self.wsis,
            desc=f"Extracting slide features ({slide_features_dir_name})",
            total=len(self.wsis),
            unit="slide",
        )

        for wsi in progress_bar:
            slide_fullname = wsi.name + wsi.ext
            patch_feature_h5_path = os.path.join(
                patch_features_dir, f"{wsi.name}.h5"
            )  # Assumes h5 input
            slide_feature_file_path = os.path.join(
                slide_features_base_dir, f"{wsi.name}.{saveas}"
            )
            progress_bar.set_postfix_str(f"{slide_fullname}")

            # --- Pre-computation Checks ---
            if os.path.exists(slide_feature_file_path) and not is_locked(
                slide_feature_file_path
            ):
                update_log(
                    log_fp, slide_fullname, "DONE - Slide features already extracted"
                )
                self.cleanup(slide_fullname)
                continue
            if is_locked(slide_feature_file_path):
                update_log(log_fp, slide_fullname, "SKIP - Locked")
                continue
            if not os.path.exists(patch_feature_h5_path):
                update_log(
                    log_fp,
                    slide_fullname,
                    f"SKIP - Required patch feature file not found: {patch_feature_h5_path}",
                )
                continue
            # WSI file check is less critical here, but we might need WSI object for metadata
            wsi_current_path = (
                os.path.join(self.wsi_cache, slide_fullname)
                if self.wsi_cache
                else wsi.original_path
            )
            if not os.path.exists(wsi_current_path):
                logger.debug(
                    f"WSI file {wsi_current_path} missing, but proceeding with feature file."
                )
                # update_log(log_fp, slide_fullname, f"INFO - WSI file missing at {wsi_current_path}") # Optional logging

            # --- Perform Slide Feature Extraction ---
            try:
                create_lock(slide_feature_file_path)
                update_log(log_fp, slide_fullname, "LOCK - Extracting slide features")
                # Initialize WSI mainly for metadata access if needed by slide encoder
                wsi._lazy_initialize()

                generated_slide_feature_path = wsi.extract_slide_features(
                    patch_features_path=patch_feature_h5_path,
                    slide_encoder=slide_encoder,
                    save_features=slide_features_base_dir,  # Pass directory
                    device=device,
                    # saveas='h5' is implicit in wsi.extract_slide_features
                )

                if not os.path.exists(generated_slide_feature_path):
                    raise FileNotFoundError(
                        f"Slide feature file {generated_slide_feature_path} not created."
                    )
                update_log(log_fp, slide_fullname, "DONE - Slide features extracted")

            except Exception as e:
                error_msg = f"ERROR during slide feature extraction: {e}"
                update_log(log_fp, slide_fullname, error_msg)
                logger.error(
                    f"Error extracting slide features for {slide_fullname}: {e}"
                )
                if isinstance(e, KeyboardInterrupt):
                    print("Slide feature extraction interrupted.")
                    raise e
                if not self.skip_errors:
                    raise RuntimeError(
                        f"Error extracting slide features for {slide_fullname}: {e}"
                    ) from e
            finally:
                if os.path.exists(slide_feature_file_path + ".lock"):
                    try:
                        remove_lock(slide_feature_file_path)
                    except OSError as lock_err:
                        logger.warning(
                            f"Could not remove lock {slide_feature_file_path}.lock: {lock_err}"
                        )
                self.cleanup(slide_fullname)
                wsi.close()  # Close WSI handle

        logger.info(
            f"Slide feature extraction finished. Features in: {slide_features_base_dir}"
        )
        return slide_features_base_dir

    def cleanup(self, filename: str) -> None:
        """Removes the specified WSI file from the cache directory if enabled."""
        if self.wsi_cache and self.clear_cache:
            cache_file_path = os.path.join(self.wsi_cache, filename)
            if os.path.exists(cache_file_path):
                if not is_locked(cache_file_path):
                    try:
                        os.remove(cache_file_path)
                        logger.debug(f"Cleaned {filename} from cache.")
                        # update_log(os.path.join(self.wsi_cache, '_cache_log.txt'), filename, 'INFO - Cleaned from cache')
                    except OSError as e:
                        logger.warning(
                            f"Failed to remove {cache_file_path} from cache: {e}"
                        )
                        # update_log(os.path.join(self.wsi_cache, '_cache_log.txt'), filename, f'ERROR - Cleanup failed: {e}')
                # else: # Optional: Log skipped cleanup due to lock
                # logger.debug(f"Skipping cleanup of locked file: {cache_file_path}")
                # update_log(os.path.join(self.wsi_cache, '_cache_log.txt'), filename, 'INFO - Cleanup skipped (locked)')

    def save_config(
        self,
        saveto: PathLike,
        local_attrs: Optional[Dict[str, Any]] = None,
        ignore: Optional[List[str]] = None,
    ) -> None:
        """Saves the processor's configuration and job parameters to a JSON file."""
        if ignore is None:
            ignore = [
                "wsis",
                "loop",
                "wsis_source",
                "wsi_cache",
            ]  # Exclude sensitive/large/redundant

        config_to_save = {}

        # Add instance attributes (filter sensitive/large ones)
        for k, v in vars(self).items():
            if k not in ignore and not k.startswith("_"):  # Exclude private attrs
                try:
                    json.dumps(v, cls=JSONsaver)  # Quick check with custom saver
                    config_to_save[k] = v
                except (TypeError, OverflowError):
                    config_to_save[k] = (
                        f"<Object type: {type(v).__name__}>"  # Represent non-serializable
                    )

        # Add/overwrite with local attributes from the specific job method
        if local_attrs:
            for k, v in local_attrs.items():
                if k not in ignore:
                    try:
                        json.dumps(v, cls=JSONsaver)
                        config_to_save[k] = v
                    except (TypeError, OverflowError):
                        # Special handling for models - just save name if possible
                        if isinstance(v, torch.nn.Module) and hasattr(v, "enc_name"):
                            config_to_save[k] = (
                                f"<Model: {getattr(v, 'enc_name', type(v).__name__)}>"
                            )
                        elif isinstance(v, torch.nn.Module) and hasattr(
                            v, "model_name"
                        ):
                            config_to_save[k] = (
                                f"<Model: {getattr(v, 'model_name', type(v).__name__)}>"
                            )
                        else:
                            config_to_save[k] = f"<Object type: {type(v).__name__}>"

        # Ensure directory exists and save
        try:
            os.makedirs(os.path.dirname(saveto), exist_ok=True)
            with open(saveto, "w") as f:
                json.dump(config_to_save, f, indent=4, cls=JSONsaver)
            logger.debug(f"Configuration saved to {saveto}")
        except Exception as e:
            logger.error(f"Failed to save configuration to {saveto}: {e}")



================================================
File: sauron/feature_extraction/utils/config.py
================================================
# sauron/feature_extraction/utils/config.py

import json
import logging
import os
import warnings
from pathlib import Path
from typing import Any, Dict, List, Optional, TypeAlias

import numpy as np
import torch  # For handling torch types in JSONsaver

# Type Aliases
PathLike: TypeAlias = str | os.PathLike | Path

# Setup logger for this utility module
logger = logging.getLogger(__name__)


class JSONsaver(json.JSONEncoder):
    """
    Custom JSON Encoder to handle non-standard types like NumPy arrays,
    ranges, PyTorch dtypes, and callables for configuration saving.

    Converts unserializable types to strings or lists where appropriate.
    """

    def default(self, obj: Any) -> Any:
        if isinstance(obj, np.floating):
            return float(obj)
        elif isinstance(obj, np.integer):
            return int(obj)
        elif isinstance(obj, np.ndarray):
            # Limit array size representation for sanity
            if obj.size > 100:
                return f"<NumPy Array shape={obj.shape} dtype={obj.dtype}>"
            return obj.tolist()
        elif isinstance(obj, np.bool_):
            return bool(obj)
        elif isinstance(obj, (range, Path)):  # Add Path handling
            return str(obj)
        elif obj in [
            torch.float16,
            torch.float32,
            torch.bfloat16,
            torch.int32,
            torch.int64,
        ]:  # Add more torch types
            return str(obj)
        elif callable(obj):
            try:
                # Prefer qualified name if possible
                name = getattr(obj, "__qualname__", getattr(obj, "__name__", None))
                if name:
                    module = getattr(obj, "__module__", "")
                    if module:
                        return f"<Callable: {module}.{name}>"
                    return f"<Callable: {name}>"
                # Fallback for objects without clear names (e.g., partials)
                return f"<Callable: {str(obj)}>"
            except Exception:
                return f"<Callable: {str(obj)}>"  # Safeguard
        # Let the base class default method raise the TypeError for other types
        try:
            return super().default(obj)
        except TypeError:
            return f"<Unserializable object type: {type(obj).__name__}>"


def save_json_config(
    config_path: PathLike,
    processor_instance: Optional[Any] = None,  # Can pass the Processor instance
    local_attrs: Optional[Dict[str, Any]] = None,
    ignore: Optional[List[str]] = None,
    custom_config: Optional[Dict[str, Any]] = None,  # Option to pass a pre-made dict
) -> None:
    """
    Saves configuration data to a JSON file.

    Combines attributes from a processor instance (if provided), local attributes,
    and/or a custom config dictionary. Handles non-serializable types gracefully using
    JSONsaver and filters out specified keys.

    Args:
        config_path: The full path (including filename) to save the JSON config.
        processor_instance: Optional instance (e.g., Processor) whose attributes
            should be saved.
        local_attrs: Optional dictionary of additional attributes (e.g., method parameters).
        ignore: A list of attribute names to exclude. Defaults to common Processor
                attributes like 'wsis', 'loop', 'logger', 'paths'.
        custom_config: Optional dictionary containing the configuration to save directly.
                       If provided, processor_instance and local_attrs might be ignored
                       or merged depending on implementation details (here, it merges).

    Raises:
        IOError: If the file cannot be written.
        TypeError: If JSON serialization fails unexpectedly (should be caught by JSONsaver).
    """
    config_path = Path(config_path)
    if ignore is None:
        # Default keys to ignore for the Processor class
        ignore = ["wsis", "loop", "logger", "paths"]

    config_to_save: Dict[str, Any] = {}

    # 1. Add processor instance attributes (if provided)
    if processor_instance:
        for k, v in vars(processor_instance).items():
            if k not in ignore:
                config_to_save[k] = v  # Add raw value, JSONsaver will handle type

    # 2. Add local attributes (potentially overwriting instance attributes)
    if local_attrs:
        for k, v in local_attrs.items():
            if k not in ignore:
                config_to_save[k] = v

    # 3. Add/overwrite with custom config dictionary (if provided)
    if custom_config:
        for k, v in custom_config.items():
            if k not in ignore:
                config_to_save[k] = v

    # Ensure the directory exists
    try:
        config_path.parent.mkdir(parents=True, exist_ok=True)
    except OSError as e:
        logger.error(f"Could not create directory for config file {config_path}: {e}")
        # Depending on severity, you might want to raise an error here
        # For now, we'll attempt to write anyway but log the error.
        # raise IOError(f"Could not create directory for config file {config_path}: {e}") from e

    # Save the combined configuration using JSONsaver
    try:
        with open(config_path, "w") as f:
            json.dump(config_to_save, f, indent=4, cls=JSONsaver, ensure_ascii=False)
        logger.debug(f"Configuration saved successfully to {config_path}")
    except TypeError as e:
        logger.error(
            f"JSON serialization failed for config {config_path}: {e}. Check JSONsaver handling."
        )
        # Re-raise as it indicates a fundamental issue with serialization logic
        raise
    except IOError as e:
        logger.error(f"Failed to write configuration file to {config_path}: {e}")
        # Re-raise as saving the config is often critical for reproducibility
        raise
    except Exception as e:
        logger.error(
            f"An unexpected error occurred while saving config to {config_path}: {e}"
        )
        # Optionally re-raise depending on desired robustness
        raise



================================================
File: sauron/losses/surv_loss.py
================================================
import numpy as np
import torch


def nll_loss(hazards, S, Y, c, alpha=0.4, eps=1e-7):
    batch_size = len(Y)
    Y = Y.view(batch_size, 1)  # ground truth bin, 1,2,...,k
    c = c.view(batch_size, 1).float()  # censorship status, 0 or 1
    if S is None:
        S = torch.cumprod(
            1 - hazards, dim=1
        )  # surival is cumulative product of 1 - hazards
    # without padding, S(0) = S[0], h(0) = h[0]
    S_padded = torch.cat(
        [torch.ones_like(c), S], 1
    )  # S(-1) = 0, all patients are alive from (-inf, 0) by definition
    # after padding, S(0) = S[1], S(1) = S[2], etc, h(0) = h[0]
    # h[y] = h(1)
    # S[1] = S(1)
    uncensored_loss = -(1 - c) * (
        torch.log(torch.gather(S_padded, 1, Y).clamp(min=eps))
        + torch.log(torch.gather(hazards, 1, Y).clamp(min=eps))
    )
    censored_loss = -c * torch.log(torch.gather(S_padded, 1, Y + 1).clamp(min=eps))
    neg_l = censored_loss + uncensored_loss
    loss = (1 - alpha) * neg_l + alpha * uncensored_loss
    loss = loss.mean()
    return loss


def ce_loss(hazards, S, Y, c, alpha=0.4, eps=1e-7):
    batch_size = len(Y)
    Y = Y.view(batch_size, 1)  # ground truth bin, 1,2,...,k
    c = c.view(batch_size, 1).float()  # censorship status, 0 or 1
    if S is None:
        S = torch.cumprod(
            1 - hazards, dim=1
        )  # surival is cumulative product of 1 - hazards
    # without padding, S(0) = S[0], h(0) = h[0]
    # after padding, S(0) = S[1], S(1) = S[2], etc, h(0) = h[0]
    # h[y] = h(1)
    # S[1] = S(1)
    S_padded = torch.cat([torch.ones_like(c), S], 1)
    reg = -(1 - c) * (
        torch.log(torch.gather(S_padded, 1, Y) + eps)
        + torch.log(torch.gather(hazards, 1, Y).clamp(min=eps))
    )
    ce_l = -c * torch.log(torch.gather(S, 1, Y).clamp(min=eps)) - (1 - c) * torch.log(
        1 - torch.gather(S, 1, Y).clamp(min=eps)
    )
    loss = (1 - alpha) * ce_l + alpha * reg
    loss = loss.mean()
    return loss


class CrossEntropySurvLoss(object):
    def __init__(self, alpha=0.15):
        self.alpha = alpha

    def __call__(self, hazards, S, Y, c, alpha=None):
        if alpha is None:
            return ce_loss(hazards, S, Y, c, alpha=self.alpha)
        else:
            return ce_loss(hazards, S, Y, c, alpha=alpha)


# loss_fn(hazards=hazards, S=S, Y=Y_hat, c=c, alpha=0)
class NLLSurvLoss(object):
    def __init__(self, alpha=0.15):
        self.alpha = alpha

    def __call__(self, hazards, S, Y, c, alpha=None):
        if alpha is None:
            return nll_loss(hazards, S, Y, c, alpha=self.alpha)
        else:
            return nll_loss(hazards, S, Y, c, alpha=alpha)

    # h_padded = torch.cat([torch.zeros_like(c), hazards], 1)
    # reg = - (1 - c) * (torch.log(torch.gather(hazards, 1, Y)) + torch.gather(torch.cumsum(torch.log(1-h_padded), dim=1), 1, Y))


class CoxSurvLoss(object):
    def __call__(hazards, S, c, **kwargs):
        # This calculation credit to Travers Ching https://github.com/traversc/cox-nnet
        # Cox-nnet: An artificial neural network method for prognosis prediction of high-throughput omics data
        current_batch_len = len(S)
        R_mat = np.zeros([current_batch_len, current_batch_len], dtype=int)
        for i in range(current_batch_len):
            for j in range(current_batch_len):
                R_mat[i, j] = S[j] >= S[i]

        R_mat = torch.FloatTensor(R_mat).to(device)
        theta = hazards.reshape(-1)
        exp_theta = torch.exp(theta)
        loss_cox = -torch.mean(
            (theta - torch.log(torch.sum(exp_theta * R_mat, dim=1))) * (1 - c)
        )
        return loss_cox



================================================
File: sauron/mil_models/ABMIL.py
================================================
import torch
import torch.nn as nn
import torch.nn.functional as F

from sauron.utils.generic_utils import initialize_weights  # Assuming this exists

from .activations import get_activation_fn


class _BaseAttentionMIL(nn.Module):
    def __init__(
        self,
        in_dim: int,
        embed_dim: int,
        attention_hidden_dim: int,
        num_attention_outputs: int,  # Typically K=1 for MIL aggregation
        n_classes: int,
        dropout_rate: float = 0.25,
        activation: str = "relu",
        is_survival: bool = False,
    ):
        super().__init__()
        self.embed_dim = embed_dim
        self.attention_hidden_dim = attention_hidden_dim
        self.num_attention_outputs = num_attention_outputs  # K
        self.is_survival = is_survival
        self.n_classes = n_classes

        feature_extractor_layers = [nn.Linear(in_dim, self.embed_dim)]
        feature_extractor_layers.append(get_activation_fn(activation))
        if dropout_rate > 0:
            feature_extractor_layers.append(nn.Dropout(dropout_rate))
        self.feature_extractor = nn.Sequential(*feature_extractor_layers)

        self.classifier_layer = nn.Linear(
            self.embed_dim * self.num_attention_outputs, n_classes
        )
        # self.apply(initialize_weights) # Apply if this is a common practice for all sub-modules

    def _get_outputs(self, logits, attention_scores=None):
        # Predictions (highest logit index)
        predictions = torch.topk(logits, 1, dim=1)[1]

        if self.is_survival:
            hazards = torch.sigmoid(logits)
            survival_curves = torch.cumprod(1 - hazards, dim=1)
            return hazards, survival_curves, predictions, attention_scores, {}
        else:
            probabilities = F.softmax(logits, dim=1)
            return logits, probabilities, predictions, attention_scores, {}


class DAttention(
    nn.Module
):  # Original DAttention renamed for clarity if _BaseAttentionMIL is used
    def __init__(
        self,
        in_dim: int,
        n_classes: int,
        dropout_rate: float = 0.25,
        activation: str = "relu",
        is_survival: bool = False,
    ):
        super().__init__()
        self.embed_dim = 512  # L
        self.attention_hidden_dim = 128  # D
        self.num_attention_outputs = 1  # K

        self.is_survival = is_survival
        self.n_classes = n_classes

        feature_layers = [nn.Linear(in_dim, self.embed_dim)]
        feature_layers.append(get_activation_fn(activation))
        if dropout_rate > 0:
            feature_layers.append(nn.Dropout(dropout_rate))
        self.feature_extractor = nn.Sequential(*feature_layers)

        self.attention_net = nn.Sequential(
            nn.Linear(self.embed_dim, self.attention_hidden_dim),
            nn.Tanh(),
            nn.Linear(self.attention_hidden_dim, self.num_attention_outputs),
        )
        self.classifier = nn.Linear(
            self.embed_dim * self.num_attention_outputs, n_classes
        )
        self.apply(initialize_weights)

    def forward(self, input_tensor: torch.Tensor):
        # input_tensor: (batch_size, num_instances, in_dim)
        batch_size = input_tensor.shape[0]

        instance_features = self.feature_extractor(
            input_tensor
        )  # (batch_size, num_instances, embed_dim)

        attention_logits = self.attention_net(
            instance_features
        )  # (batch_size, num_instances, K)
        attention_logits = torch.transpose(
            attention_logits, 2, 1
        )  # (batch_size, K, num_instances)

        attention_scores = F.softmax(attention_logits, dim=2)  # Softmax over instances

        # M = KxL equivalent for batch: (batch_size, K, embed_dim)
        aggregated_features = torch.bmm(attention_scores, instance_features)

        # If K=1, aggregated_features is (batch_size, 1, embed_dim)
        # Flatten for classifier: (batch_size, K * embed_dim)
        aggregated_features_flat = aggregated_features.view(batch_size, -1)

        logits = self.classifier(aggregated_features_flat)  # (batch_size, n_classes)

        # Predictions (highest logit index)
        predictions = torch.topk(logits, 1, dim=1)[1]

        if self.is_survival:
            hazards = torch.sigmoid(logits)
            survival_curves = torch.cumprod(1 - hazards, dim=1)
            return (
                hazards,
                survival_curves,
                predictions,
                attention_logits.transpose(2, 1),
                {},
            )  # Return raw attention before softmax
        else:
            probabilities = F.softmax(logits, dim=1)
            return (
                logits,
                probabilities,
                predictions,
                attention_logits.transpose(2, 1),
                {},
            )


class GatedAttention(nn.Module):
    def __init__(
        self,
        in_dim: int,
        n_classes: int,
        dropout_rate: float = 0.25,
        activation: str = "relu",
        is_survival: bool = False,
    ):
        super().__init__()
        self.embed_dim = 512  # L
        self.attention_hidden_dim = 128  # D
        self.num_attention_outputs = 1  # K

        self.is_survival = is_survival
        self.n_classes = n_classes

        feature_layers = [nn.Linear(in_dim, self.embed_dim)]
        feature_layers.append(get_activation_fn(activation))
        if dropout_rate > 0:
            feature_layers.append(nn.Dropout(dropout_rate))
        self.feature_extractor = nn.Sequential(*feature_layers)

        self.attention_V = nn.Sequential(
            nn.Linear(self.embed_dim, self.attention_hidden_dim), nn.Tanh()
        )
        self.attention_U = nn.Sequential(
            nn.Linear(self.embed_dim, self.attention_hidden_dim), nn.Sigmoid()
        )
        self.attention_weights = nn.Linear(
            self.attention_hidden_dim, self.num_attention_outputs
        )

        self.classifier = nn.Linear(
            self.embed_dim * self.num_attention_outputs, n_classes
        )
        self.apply(initialize_weights)

    def forward(self, input_tensor: torch.Tensor):
        # input_tensor: (batch_size, num_instances, in_dim)
        batch_size = input_tensor.shape[0]

        instance_features = self.feature_extractor(
            input_tensor
        )  # (batch_size, num_instances, embed_dim)

        attention_values = self.attention_V(
            instance_features
        )  # (batch_size, num_instances, D)
        attention_units = self.attention_U(
            instance_features
        )  # (batch_size, num_instances, D)

        # Element-wise multiplication, then pass through weights layer
        unnormalized_attention_scores = self.attention_weights(
            attention_values * attention_units
        )  # (batch_size, num_instances, K)
        unnormalized_attention_scores = torch.transpose(
            unnormalized_attention_scores, 2, 1
        )  # (batch_size, K, num_instances)

        normalized_attention_scores = F.softmax(
            unnormalized_attention_scores, dim=2
        )  # Softmax over instances

        aggregated_features = torch.bmm(
            normalized_attention_scores, instance_features
        )  # (batch_size, K, embed_dim)
        flattened_aggregated_features = aggregated_features.view(
            batch_size, -1
        )  # (batch_size, K * embed_dim)

        logits = self.classifier(
            flattened_aggregated_features
        )  # (batch_size, n_classes)

        # Predictions (highest logit index)
        predictions = torch.topk(logits, 1, dim=1)[1]

        if self.is_survival:
            hazard_rates = torch.sigmoid(logits)
            survival_curves = torch.cumprod(1 - hazard_rates, dim=1)
            # A_raw for GatedAttention is less direct, similar to DAttention, returning unnormalized_attention_scores before softmax
            return (
                hazard_rates,
                survival_curves,
                predictions,
                unnormalized_attention_scores.transpose(2, 1),
                {},
            )
        else:
            probabilities = F.softmax(logits, dim=1)
            return (
                logits,
                probabilities,
                predictions,
                unnormalized_attention_scores.transpose(2, 1),
                {},
            )



================================================
File: sauron/mil_models/DiffABMIL.py
================================================
import torch
import torch.nn as nn
import torch.nn.functional as F

from sauron.utils.generic_utils import initialize_weights

from .activations import get_activation_fn


class DifferentiableAttentionMIL(nn.Module):
    def __init__(
        self,
        in_dim: int,
        n_classes: int,
        embed_dim: int = 512,
        num_heads: int = 8,
        dropout_rate: float = 0.25,
        activation: str = "relu",
        is_survival: bool = False,
    ):
        super().__init__()
        self.embed_dim = embed_dim  # L
        self.num_heads = num_heads
        self.head_dim = self.embed_dim // self.num_heads
        self.is_survival = is_survival
        self.n_classes = n_classes

        if self.embed_dim % self.num_heads != 0:
            raise ValueError("embed_dim must be divisible by num_heads")

        feature_layers = [nn.Linear(in_dim, self.embed_dim)]
        feature_layers.append(get_activation_fn(activation))
        if dropout_rate > 0:
            feature_layers.append(nn.Dropout(dropout_rate))
        self.feature_extractor = nn.Sequential(*feature_layers)

        self.query_proj = nn.Linear(self.embed_dim, self.embed_dim)
        self.key_proj = nn.Linear(self.embed_dim, self.embed_dim)
        self.value_proj = nn.Linear(self.embed_dim, self.embed_dim)

        # Output projection from attention, not used in original scaled_dot_product version for bag_repr
        # self.output_proj = nn.Linear(self.embed_dim, self.embed_dim)

        self.classifier = nn.Linear(self.embed_dim, n_classes)
        self.apply(initialize_weights)

    def forward(self, x: torch.Tensor):
        # x: (batch_size, num_instances, in_dim)
        batch_size, num_instances, _ = x.size()

        instance_features = self.feature_extractor(
            x
        )  # (batch_size, num_instances, embed_dim)

        q = self.query_proj(instance_features)  # (batch_size, num_instances, embed_dim)
        k = self.key_proj(instance_features)  # (batch_size, num_instances, embed_dim)
        v = self.value_proj(instance_features)  # (batch_size, num_instances, embed_dim)

        # Reshape for multi-head attention for F.scaled_dot_product_attention
        # (batch_size, num_heads, num_instances, head_dim)
        q = q.view(batch_size, num_instances, self.num_heads, self.head_dim).transpose(
            1, 2
        )
        k = k.view(batch_size, num_instances, self.num_heads, self.head_dim).transpose(
            1, 2
        )
        v = v.view(batch_size, num_instances, self.num_heads, self.head_dim).transpose(
            1, 2
        )

        # scaled_dot_product_attention expects (..., S, E) for query, (..., L, E) for key/value
        # Here, S = num_instances, L = num_instances, E = head_dim
        # Input q, k, v: (batch_size, num_heads, num_instances, head_dim)
        # F.scaled_dot_product_attention will operate on last 3 dims if N>3
        # Or reshape to (batch_size * num_heads, num_instances, head_dim)
        # q_reshaped = q.contiguous().view(batch_size * self.num_heads, num_instances, self.head_dim)
        # k_reshaped = k.contiguous().view(batch_size * self.num_heads, num_instances, self.head_dim)
        # v_reshaped = v.contiguous().view(batch_size * self.num_heads, num_instances, self.head_dim)
        # attn_output = F.scaled_dot_product_attention(q_reshaped, k_reshaped, v_reshaped)
        # attn_output = attn_output.view(batch_size, self.num_heads, num_instances, self.head_dim)

        # Direct passing if Pytorch version supports it
        attn_output = F.scaled_dot_product_attention(q, k, v, dropout_p=0.0)
        # attn_output shape: (batch_size, num_heads, num_instances, head_dim)

        # Reshape back and combine heads
        # (batch_size, num_instances, num_heads, head_dim) -> (batch_size, num_instances, embed_dim)
        attn_output = (
            attn_output.transpose(1, 2)
            .contiguous()
            .view(batch_size, num_instances, self.embed_dim)
        )

        # Aggregate over instances (mean pooling of instance representations after attention)
        bag_representation = attn_output.mean(dim=1)  # (batch_size, embed_dim)

        logits = self.classifier(bag_representation)  # (batch_size, n_classes)

        # Predictions (highest logit index)
        predictions = torch.topk(logits, 1, dim=1)[1]

        # A_raw (attention weights) is not directly returned by F.scaled_dot_product_attention
        # To get it, one would compute (Q @ K.transpose(-2, -1) / sqrt(dim_k)).softmax(dim=-1)
        attention_scores_raw = None  # Placeholder

        if self.is_survival:
            hazards = torch.sigmoid(logits)
            survival_curves = torch.cumprod(1 - hazards, dim=1)
            return hazards, survival_curves, predictions, attention_scores_raw, {}
        else:
            probabilities = F.softmax(logits, dim=1)
            return logits, probabilities, predictions, attention_scores_raw, {}



================================================
File: sauron/mil_models/MambaMIL.py
================================================
"""
MambaMIL
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from mamba_ssm import Mamba

from sauron.mil_models.mamba_ssm import BiMamba, SRMamba
from sauron.utils.generic_utils import initialize_weights


class MambaMIL(nn.Module):
    def __init__(
        self,
        in_dim,
        n_classes,
        dropout,
        act,
        survival=False,
        layer=2,
        rate=10,
        type="SRMamba",
    ):
        super(MambaMIL, self).__init__()
        self._fc1 = [nn.Linear(in_dim, 512)]
        if act.lower() == "relu":
            self._fc1 += [nn.ReLU()]
        elif act.lower() == "gelu":
            self._fc1 += [nn.GELU()]
        if dropout:
            self._fc1 += [nn.Dropout(dropout)]

        self._fc1 = nn.Sequential(*self._fc1)
        self.norm = nn.LayerNorm(512)
        self.layers = nn.ModuleList()
        self.survival = survival

        if type == "SRMamba":
            for _ in range(layer):
                self.layers.append(
                    nn.Sequential(
                        nn.LayerNorm(512),
                        SRMamba(
                            d_model=512,
                            d_state=16,
                            d_conv=4,
                            expand=2,
                        ),
                    )
                )
        elif type == "Mamba":
            for _ in range(layer):
                self.layers.append(
                    nn.Sequential(
                        nn.LayerNorm(512),
                        Mamba(
                            d_model=512,
                            d_state=16,
                            d_conv=4,
                            expand=2,
                        ),
                    )
                )
        elif type == "BiMamba":
            for _ in range(layer):
                self.layers.append(
                    nn.Sequential(
                        nn.LayerNorm(512),
                        BiMamba(
                            d_model=512,
                            d_state=16,
                            d_conv=4,
                            expand=2,
                        ),
                    )
                )
        else:
            raise NotImplementedError("Mamba [{}] is not implemented".format(type))

        self.n_classes = n_classes
        self.classifier = nn.Linear(512, self.n_classes)
        self.attention = nn.Sequential(
            nn.Linear(512, 128), nn.Tanh(), nn.Linear(128, 1)
        )
        self.rate = rate
        self.type = type

        self.apply(initialize_weights)

    def forward(self, x):
        if len(x.shape) == 2:
            x = x.expand(1, -1, -1)
        h = x.float()  # [B, n, 1024]

        h = self._fc1(h)  # [B, n, 256]

        if self.type == "SRMamba":
            for layer in self.layers:
                h_ = h
                h = layer[0](h)
                h = layer[1](h, rate=self.rate)
                h = h + h_
        elif self.type == "Mamba" or self.type == "BiMamba":
            for layer in self.layers:
                h_ = h
                h = layer[0](h)
                h = layer[1](h)
                h = h + h_

        h = self.norm(h)
        A = self.attention(h)  # [B, n, K]
        A = torch.transpose(A, 1, 2)
        A = F.softmax(A, dim=-1)  # [B, K, n]
        h = torch.bmm(A, h)  # [B, K, 512]
        h = h.squeeze(0)

        logits = self.classifier(h)  # [B, n_classes]
        Y_prob = F.softmax(logits, dim=1)
        Y_hat = torch.topk(logits, 1, dim=1)[1]
        A_raw = None
        results_dict = None
        if self.survival:
            Y_hat = torch.topk(logits, 1, dim=1)[1]
            hazards = torch.sigmoid(logits)
            S = torch.cumprod(1 - hazards, dim=1)
            return hazards, S, Y_hat, None, None
        return logits, Y_prob, Y_hat, A_raw, results_dict

    def relocate(self):
        device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        self._fc1 = self._fc1.to(device)
        self.layers = self.layers.to(device)

        self.attention = self.attention.to(device)
        self.norm = self.norm.to(device)
        self.classifier = self.classifier.to(device)



================================================
File: sauron/mil_models/MaxMIL.py
================================================
import torch
import torch.nn as nn
import torch.nn.functional as F

from sauron.utils.generic_utils import initialize_weights

from .activations import get_activation_fn


class MaxMIL(nn.Module):
    def __init__(
        self,
        in_dim: int,
        n_classes: int,
        hidden_dim: int = 512,  # Renamed from fixed 512
        dropout_rate: float = 0.25,
        activation: str = "relu",
        is_survival: bool = False,
    ):
        super().__init__()
        self.is_survival = is_survival
        self.n_classes = n_classes

        head_layers = [nn.Linear(in_dim, hidden_dim)]
        head_layers.append(get_activation_fn(activation))
        if dropout_rate > 0:
            head_layers.append(nn.Dropout(dropout_rate))
        # Final layer to n_classes (instance scores/logits)
        head_layers.append(nn.Linear(hidden_dim, n_classes))

        self.instance_scorer = nn.Sequential(*head_layers)
        self.apply(initialize_weights)

    def forward(self, x: torch.Tensor):
        # x: (batch_size, num_instances, in_dim)
        # If single bag (num_instances, in_dim), add batch dim
        if x.ndim == 2:
            x = x.unsqueeze(0)

        # Get instance-level scores/logits
        instance_logits = self.instance_scorer(
            x
        )  # (batch_size, num_instances, n_classes)

        # Max pooling over instances for each class logit
        # Result: (batch_size, n_classes)
        bag_logits, _ = torch.max(instance_logits, dim=1)

        # Predictions (highest logit index)
        predictions = torch.topk(bag_logits, 1, dim=1)[1]

        if self.is_survival:
            hazards = torch.sigmoid(bag_logits)
            survival_curves = torch.cumprod(1 - hazards, dim=1)
            return hazards, survival_curves, predictions, None, {}
        else:
            probabilities = F.softmax(bag_logits, dim=1)
            return bag_logits, probabilities, predictions, None, {}



================================================
File: sauron/mil_models/MeanMIL.py
================================================
import torch
import torch.nn as nn
import torch.nn.functional as F

from sauron.utils.generic_utils import initialize_weights

from .activations import get_activation_fn


class MeanMIL(nn.Module):
    def __init__(
        self,
        in_dim: int,
        n_classes: int,
        hidden_dim: int = 512,  # Renamed from fixed 512
        dropout_rate: float = 0.25,
        activation: str = "relu",
        is_survival: bool = False,
    ):
        super().__init__()
        self.is_survival = is_survival
        self.n_classes = n_classes

        head_layers = [nn.Linear(in_dim, hidden_dim)]
        head_layers.append(get_activation_fn(activation))
        if dropout_rate > 0:
            head_layers.append(nn.Dropout(dropout_rate))
        # Final layer to n_classes (instance scores/logits)
        head_layers.append(nn.Linear(hidden_dim, n_classes))

        self.instance_scorer = nn.Sequential(*head_layers)
        self.apply(initialize_weights)

    def forward(self, x: torch.Tensor):
        # x: (batch_size, num_instances, in_dim)
        # If single bag (num_instances, in_dim), add batch dim
        if x.ndim == 2:
            x = x.unsqueeze(0)

        # Get instance-level scores/logits
        instance_logits = self.instance_scorer(
            x
        )  # (batch_size, num_instances, n_classes)

        # Mean pooling over instances for each class logit
        # Result: (batch_size, n_classes)
        bag_logits = torch.mean(instance_logits, dim=1)

        # Predictions (highest logit index)
        predictions = torch.topk(bag_logits, 1, dim=1)[1]

        if self.is_survival:
            hazards = torch.sigmoid(bag_logits)
            survival_curves = torch.cumprod(1 - hazards, dim=1)
            return hazards, survival_curves, predictions, None, {}
        else:
            probabilities = F.softmax(bag_logits, dim=1)
            return bag_logits, probabilities, predictions, None, {}



================================================
File: sauron/mil_models/S4MIL.py
================================================
import math

import torch
import torch.nn as nn
import torch.nn.functional as F
from einops import rearrange, repeat

from sauron.utils.generic_utils import initialize_weights

from .activations import get_activation_fn

# S4DKernel and S4D components (assumed to be correct and kept as is, minor style adjustments)
# _c2r and _r2c are utility functions for complex numbers, often defined locally or imported
_c2r = torch.view_as_real
_r2c = torch.view_as_complex


class DropoutNd(nn.Module):
    def __init__(self, p: float = 0.5, tie: bool = True, transposed: bool = True):
        super().__init__()
        if not 0.0 <= p < 1.0:
            raise ValueError(f"dropout probability has to be in [0, 1), but got {p}")
        self.p = p
        self.tie = tie
        self.transposed = transposed
        # self.binomial = torch.distributions.binomial.Binomial(probs=1 - self.p) # Not used

    def forward(self, X):
        if self.training and self.p > 0:
            if not self.transposed:
                X = X.transpose(
                    -1, -2
                )  # More robust than rearrange for (B H L) -> (B L H)

            # Determine mask shape
            # For (B, H, L) if tie=True, mask is (B, H, 1) to broadcast over L
            # If tie=False, mask is (B, H, L)
            mask_shape = X.shape[:-1] + (1,) if self.tie else X.shape

            # Original had X.shape[:2], which assumed X was (B, D, ...).
            # If X is (B, H, L), then X.shape[:2] is (B,H) for tied mask over L.
            # mask_shape = X.shape[:2] + (1,) * (X.ndim - 2) if self.tie else X.shape

            mask = (
                torch.rand(*mask_shape, device=X.device) > self.p
            )  # Inverted logic for mask: 1 means keep
            X = X * mask * (1.0 / (1.0 - self.p))  # Scale by 1/(1-p)

            if not self.transposed:
                X = X.transpose(-1, -2)  # Transpose back
            return X
        return X


class S4DKernel(nn.Module):
    def __init__(
        self,
        d_model: int,
        N: int = 64,
        dt_min: float = 0.001,
        dt_max: float = 0.1,
        lr: float = None,
    ):
        super().__init__()
        H = d_model  # Renaming for clarity from original S4 papers
        log_dt = torch.rand(H) * (math.log(dt_max) - math.log(dt_min)) + math.log(
            dt_min
        )

        C_init = torch.randn(H, N // 2, dtype=torch.cfloat)
        self.C = nn.Parameter(_c2r(C_init))
        self._register_param("log_dt", log_dt, lr)

        log_A_real = torch.log(0.5 * torch.ones(H, N // 2))
        A_imag = math.pi * repeat(torch.arange(N // 2), "n -> h n", h=H)
        self._register_param("log_A_real", log_A_real, lr)
        self._register_param("A_imag", A_imag, lr)  # A_imag is not learned typically

    def forward(self, L: int):  # L is sequence length
        dt = torch.exp(self.log_dt)  # (H)
        C = _r2c(self.C)  # (H, N/2) complex
        A = -torch.exp(self.log_A_real) + 1j * self.A_imag  # (H, N/2) complex

        dtA = A * dt.unsqueeze(-1)  # (H, N/2)

        # Kernel calculation (Convolution theorem part)
        # K_conv = C * (e^(dtA) - 1) / A
        # This is part of the HiPPO framework for state space models
        C_times_dt = C * dt.unsqueeze(
            -1
        )  # Element-wise, for HiPPO-LegS this might be different
        # For S4D, the formula using C * (exp(dtA)-1)/A is common.

        # Original S4D computes K using Vandermonde multiplication for HiPPO C_bar
        # For frequency domain kernel:
        # K_f = (C_bar * (omega - A)^-1 * B_bar) # This is for continuous case
        # Discretized version (bilinear transform or ZOH):
        # K_z = C_z * (zI - A_z)^-1 * B_z
        # The provided code uses an explicit time-domain kernel computation K

        # This K seems to be a direct computation of the impulse response
        coeffs = dtA.unsqueeze(-1) * torch.arange(L, device=A.device)  # (H, N/2, L)
        K_complex = torch.einsum(
            "hn,hnl->hl", C * (torch.exp(dtA) - 1.0) / A, torch.exp(coeffs)
        )
        K = 2 * K_complex.real
        return K

    def _register_param(self, name: str, tensor: torch.Tensor, lr: float = None):
        if lr == 0.0:  # Treat as frozen buffer
            self.register_buffer(name, tensor)
        else:
            param = nn.Parameter(tensor)
            self.register_parameter(name, param)
            if lr is not None:  # S4-specific learning rate
                setattr(param, "_optim", {"lr": lr, "weight_decay": 0.0})


class S4D(nn.Module):
    def __init__(
        self,
        d_model: int,
        d_state: int = 64,
        dropout: float = 0.0,
        transposed: bool = True,
        **kernel_args,
    ):
        super().__init__()

        self.h_model = d_model  # H in S4 paper
        self.d_state = d_state  # N in S4 paper
        self.d_output = self.h_model  # Output dim is same as input model dim
        self.transposed = transposed  # If true, input is (B, H, L), else (B, L, H)

        self.D_skip_connection = nn.Parameter(torch.randn(self.h_model))

        self.kernel = S4DKernel(self.h_model, N=self.d_state, **kernel_args)

        # Activation and dropout after convolution and skip connection
        self.activation = nn.GELU()  # Common in S4
        self.dropout_layer = (
            DropoutNd(dropout, transposed=self.transposed)
            if dropout > 0.0
            else nn.Identity()
        )

        # Output linear GLU layer
        self.output_linear = nn.Sequential(
            nn.Conv1d(self.h_model, 2 * self.h_model, kernel_size=1),  # Project to 2*H
            nn.GLU(
                dim=1
            ),  # GLU halves the channel dimension back to H at dim=1 (channel dim for Conv1d)
        )

    def forward(self, u: torch.Tensor, **kwargs):  # u is input sequence
        if not self.transposed:  # Expects (B, H, L)
            u = u.transpose(-1, -2)

        seq_len = u.size(-1)
        k = self.kernel(L=seq_len)  # (H, L)

        # Convolution via FFT
        k_f = torch.fft.rfft(k, n=2 * seq_len)  # (H, L_fft)
        u_f = torch.fft.rfft(u.to(torch.float32), n=2 * seq_len)  # (B, H, L_fft)

        # Element-wise product in frequency domain
        y_conv_f = u_f * k_f.unsqueeze(0)  # Add batch dim to kernel_f
        y_conv = torch.fft.irfft(y_conv_f, n=2 * seq_len)[..., :seq_len]  # (B, H, L)

        # Add D skip connection (direct path for input u)
        y_skip = y_conv + u * self.D_skip_connection.unsqueeze(0).unsqueeze(
            -1
        )  # (B,H,1) broadcast

        # Activation and dropout
        y_activated = self.dropout_layer(self.activation(y_skip))

        # Output linear layer
        y_output = self.output_linear(y_activated)  # (B, H, L)

        if not self.transposed:
            y_output = y_output.transpose(-1, -2)  # (B, L, H)
        return y_output


class S4Model(nn.Module):  # S4MIL Wrapper
    def __init__(
        self,
        in_dim: int,
        n_classes: int,
        embed_dim: int = 512,
        s4_d_state: int = 64,  # S4 N param
        dropout_rate: float = 0.0,  # Dropout for FC and S4D
        activation: str = "gelu",  # Activation for FC
        is_survival: bool = False,
    ):
        super().__init__()
        self.is_survival = is_survival
        self.n_classes = n_classes

        fc1_layers = [nn.Linear(in_dim, embed_dim)]
        fc1_layers.append(get_activation_fn(activation))
        if dropout_rate > 0:  # Dropout after first FC's activation
            fc1_layers.append(nn.Dropout(dropout_rate))
        self.feature_extractor_fc = nn.Sequential(*fc1_layers)

        # S4 block expects input (Batch, SeqLen, Features) if transposed=False
        # or (Batch, Features, SeqLen) if transposed=True.
        # Here, Features = embed_dim, SeqLen = num_instances.
        self.s4_block = nn.Sequential(
            nn.LayerNorm(embed_dim),  # LayerNorm before S4, applied on feature dim
            S4D(
                d_model=embed_dim,
                d_state=s4_d_state,
                dropout=dropout_rate,
                transposed=False,
            ),
        )
        self.classifier = nn.Linear(embed_dim, n_classes)
        self.apply(initialize_weights)

    def forward(self, x: torch.Tensor):
        # x: (batch_size, num_instances, in_dim)
        if x.ndim == 2:  # If single bag (num_instances, in_dim)
            x = x.unsqueeze(0)  # (1, num_instances, in_dim)

        # Instance feature extraction
        instance_features = self.feature_extractor_fc(
            x
        )  # (batch_size, num_instances, embed_dim)

        # S4 processing: input (batch, seq_len=num_instances, features=embed_dim)
        s4_output = self.s4_block(
            instance_features
        )  # (batch_size, num_instances, embed_dim)

        # Max pooling over instances (sequence dimension)
        bag_representation, _ = torch.max(s4_output, dim=1)  # (batch_size, embed_dim)

        logits = self.classifier(bag_representation)  # (batch_size, n_classes)

        # Predictions (highest logit index)
        predictions = torch.topk(logits, 1, dim=1)[1]

        if self.is_survival:
            hazards = torch.sigmoid(logits)
            survival_curves = torch.cumprod(1 - hazards, dim=1)
            return hazards, survival_curves, predictions, None, {}
        else:
            probabilities = F.softmax(logits, dim=1)
            return logits, probabilities, predictions, None, {}



================================================
File: sauron/mil_models/TransMIL.py
================================================
from math import ceil

import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
from einops import rearrange, reduce
from torch import einsum

from sauron.utils.generic_utils import initialize_weights  # Keep if used elsewhere

from .activations import get_activation_fn


# --- Nystrom Attention and Transformer Components (largely kept as is) ---
def exists(val):
    return val is not None


def moore_penrose_iter_pinv(x, iters=6):  # Moore-Penrose pseudo-inverse
    device = x.device
    abs_x = torch.abs(x)
    col = abs_x.sum(dim=-1)
    row = abs_x.sum(dim=-2)
    # Ensure denominators are not zero, add small epsilon if necessary
    max_col = torch.max(col, dim=-1, keepdim=True)[0]
    max_row = torch.max(row, dim=-1, keepdim=True)[0]

    # Handle potential division by zero if max_col or max_row is 0
    # This can happen if a landmark becomes all zeros
    z_denom = max_col * max_row
    z_denom = torch.where(z_denom == 0, torch.tensor(1e-8, device=device), z_denom)

    z = x.transpose(-2, -1) / z_denom

    I = torch.eye(x.shape[-1], device=device).unsqueeze(
        0
    )  # Add batch dim for broadcasting

    for _ in range(iters):
        xz = x @ z
        z = 0.25 * z @ (13 * I - (xz @ (15 * I - (xz @ (7 * I - xz)))))
    return z


class NystromAttention(nn.Module):
    def __init__(
        self,
        dim,
        dim_head=64,
        heads=8,
        num_landmarks=256,
        pinv_iterations=6,
        residual=True,
        residual_conv_kernel=33,
        eps=1e-8,
        dropout=0.0,
    ):
        super().__init__()
        self.eps = eps
        inner_dim = heads * dim_head

        self.num_landmarks = num_landmarks
        self.pinv_iterations = pinv_iterations

        self.heads = heads
        self.scale = dim_head**-0.5
        self.to_qkv = nn.Linear(dim, inner_dim * 3, bias=False)

        self.to_out = nn.Sequential(nn.Linear(inner_dim, dim), nn.Dropout(dropout))

        self.residual = residual
        if residual:
            kernel_size = residual_conv_kernel
            padding = residual_conv_kernel // 2
            # Conv2d for (B, H, N, D) type data, applying kernel over N (sequence) dim
            # For residual connection on V, V is (B,H,N,D_head). Conv over N.
            # So Conv1d on (B*H, D_head, N) or Conv2d (B,H,N,1) applied to D_head features over N seq.
            # Original uses Conv2d (heads, heads, (kernel,1)). This implies groups=heads, operating per head.
            # Let's assume it operates on V's sequence dimension.
            # If V is (B, H, N, D_head), transpose to (B, H, D_head, N) for Conv1d
            # Or (B, H*D_head, N) if not grouped by head
            # The original Conv2d (H,H,(K,1)) groups=H, is like H separate Conv1ds on (B,1,N,D_h)
            # This is complex. If `v` is (B,H,N,D_head), want (B,H,N,D_head) out.
            # A depthwise Conv1d for each head might be:
            # Reshape v to (B*H, N, D_head), permute to (B*H, D_head, N), Conv1d, permute back.
            # The current res_conv is (H, H, (K,1)) groups=H. Input (B, H, N, D).
            # This means it expects V to be (B,H,N,1) and D_head=1, which seems unlikely.
            # Or, it's applied to a reshaped V.
            # For now, I will keep the original res_conv structure, but it's a point of caution.
            self.res_conv = nn.Conv2d(
                heads,
                heads,
                (kernel_size, 1),
                padding=(padding, 0),
                groups=heads,
                bias=False,
            )

    def forward(self, x, mask=None, return_attn=False):
        batch_size, num_instances, _ = x.shape
        h = self.heads
        m = self.num_landmarks  # num_landmarks

        # Padding for landmarks
        remainder = num_instances % m
        if remainder > 0:
            padding = m - remainder
            x = F.pad(x, (0, 0, 0, padding), value=0)  # Pad sequence dim (N)
            if exists(mask):  # mask is (B, N)
                mask = F.pad(mask, (0, padding), value=False)

        padded_n = x.shape[1]

        q, k, v = self.to_qkv(x).chunk(3, dim=-1)
        # (B, N, H*D_h) -> (B, H, N, D_h)
        q, k, v = map(lambda t: rearrange(t, "b n (h d) -> b h n d", h=h), (q, k, v))

        if exists(mask):  # mask (B, N_padded) -> (B, 1, N_padded, 1) for broadcasting
            mask_expanded = mask.unsqueeze(1).unsqueeze(-1)
            q = q.masked_fill(~mask_expanded, 0.0)
            k = k.masked_fill(~mask_expanded, 0.0)
            v = v.masked_fill(~mask_expanded, 0.0)

        q = q * self.scale  # Apply scaling factor

        # Landmarks: average pooling to get landmarks
        # q_landmarks: (B, H, M, D_h)
        # reduce from (B, H, N_padded, D_h) to (B, H, M, D_h) by averaging N_padded/M instances
        l_num_groups = padded_n // m
        q_landmarks = reduce(q, "b h (l m) d -> b h l d", "mean", m=m)
        k_landmarks = reduce(k, "b h (l m) d -> b h l d", "mean", m=m)
        # Note: Original uses 'sum' and then divides by 'l' or masked sum. 'mean' is more direct.

        einops_eq = "b h i d, b h j d -> b h i j"  # einsum equation for dot products
        sim1 = einsum(einops_eq, q, k_landmarks)  # (B, H, N_padded, M)
        sim2 = einsum(einops_eq, q_landmarks, k_landmarks)  # (B, H, M, M)
        sim3 = einsum(einops_eq, q_landmarks, k)  # (B, H, M, N_padded)

        # Masking attention scores
        if exists(mask):
            # mask is (B, N_padded)
            # landmark_mask is (B, M)
            landmark_mask = (
                reduce(mask.float(), "b (l m) -> b l", "sum", m=m) > 0
            )  # (B,M) true if any instance in landmark group is not masked

            # Apply masks (mask_value is usually a large negative number)
            mask_value = -torch.finfo(q.dtype).max
            # sim1: q vs k_landmarks. Mask based on q's mask and k_landmarks' mask
            # (B,1,N_pad,1) * (B,1,1,M) -> (B,1,N_pad,M)
            sim1_mask = mask.unsqueeze(1).unsqueeze(-1) * landmark_mask.unsqueeze(
                1
            ).unsqueeze(1)
            sim1.masked_fill_(~sim1_mask, mask_value)

            # sim2: q_landmarks vs k_landmarks
            # (B,1,M,1) * (B,1,1,M) -> (B,1,M,M)
            sim2_mask = landmark_mask.unsqueeze(1).unsqueeze(
                -1
            ) * landmark_mask.unsqueeze(1).unsqueeze(1)
            sim2.masked_fill_(~sim2_mask, mask_value)

            # sim3: q_landmarks vs k
            # (B,1,M,1) * (B,1,1,N_pad) -> (B,1,M,N_pad)
            sim3_mask = landmark_mask.unsqueeze(1).unsqueeze(-1) * mask.unsqueeze(
                1
            ).unsqueeze(1)
            sim3.masked_fill_(~sim3_mask, mask_value)

        attn1 = sim1.softmax(dim=-1)  # (B, H, N_padded, M)
        attn2 = sim2.softmax(dim=-1)  # (B, H, M, M)
        attn3 = sim3.softmax(dim=-1)  # (B, H, M, N_padded)

        attn2_inv = moore_penrose_iter_pinv(attn2, self.pinv_iterations)  # (B, H, M, M)

        # (B,H,N_padded,M) @ (B,H,M,M) @ (B,H,M,N_padded) @ (B,H,N_padded,D_h)
        out = (attn1 @ attn2_inv) @ (attn3 @ v)  # (B, H, N_padded, D_h)

        if self.residual:
            # V is (B,H,N_padded, D_h). res_conv expects (B,H,N,1) effectively or similar.
            # This residual part is tricky. If res_conv is Conv2d(H,H,(K,1),groups=H),
            # it expects input like (B, H, N, D_some_feature=1).
            # A common way for residual in transformers for `v` is just `v` itself or a linear projection of `v`.
            # Assuming the original res_conv structure expects v with D_h=1 or similar.
            # A simple additive residual of v is more standard if res_conv is problematic:
            # out = out + v
            # For now, trying to make original structure work by permuting and squeezing.
            # This part may need careful review based on expected shapes of res_conv.
            # If v is (B, H, N, D_h), res_conv needs to map this to (B, H, N, D_h)
            # For Conv2d(H,H,(K,1),groups=H), input (B,H,N,D_h), permute to (B,H,D_h,N)
            # If we want to convolve over N, maybe (B*D_h, H, N, 1) then sum over D_h?
            # Sticking to simplest interpretation: it applies a conv per head over sequence dimension.
            # Input to Conv2d: (Batch, Channels_in, H_in, W_in)
            # V: (B, H, N, D_h). Treat H as channels, N as height, D_h as width.
            # res_conv Conv2d(H, H, (kernel,1), groups=H) operates on (B, H, N, D_h_as_W).
            # So D_h must be 1 if kernel is (K,1).
            # Let's assume the residual is on a projection of v if D_h > 1, or the original res_conv is intended differently.
            # A simple residual for now for compatibility:
            out = out + v  # Direct residual from v
            # Original: out += self.res_conv(v) # This line is problematic with typical v shapes.
            # If self.res_conv is for (B, H, N, 1) and D_h > 1, this won't work.
            # If v is (B,H,N,D) and res_conv is Conv2d(H,H,(K,1),groups=H), this is applying filter of size (K,1)
            # on a (N,D) feature map, per head. Requires D=1 for kernel W=1.
            # If it's depthwise conv over N: (B*H, D_head, N), apply Conv1d, then reshape.
            # Reverting to original in case it has a specific meaning, but with a warning.
            # This part of NystromAttention is often a simple `out = out + v` or `out = out + self.res_linear(v)`
            # if res_conv (B,H,N,1) expects D_head=1.
            # If D_head > 1, this is likely an issue.
            # For now, let's assume a simple additive residual as it's safer.
            # out = out + v # Safer residual
            # If keeping original:
            # Must ensure v is shaped like (B, H, N, 1) for res_conv to work as written.
            # If v is (B,H,N,D_h) and we want residual, typically a linear layer or direct add.
            # For now, I'll comment out the complex residual. A simple one is better.
            # out_residual = v
            # if self.residual:
            #     # This requires careful shape management for self.res_conv
            #     # A simple solution is a linear layer or direct addition of v
            #     out = out + out_residual # Example: direct add

            # The original paper might use a Conv1D applied channel-wise (depth-wise) on sequence.
            # If v is (B,H,N,D_h), then v_permuted = v.permute(0,1,3,2) is (B,H,D_h,N)
            # Then apply Conv1d to (B*H, D_h, N). For this, res_conv should be Conv1d.
            # Given it's Conv2d, it is unusual. A simple `out = out + v` is common.

        out = rearrange(out, "b h n d -> b n (h d)")  # (B, N_padded, H*D_h)
        out = self.to_out(out)  # (B, N_padded, Dim)

        # Remove padding
        out = out[:, :num_instances, :]

        if return_attn:  # Not used by TransMIL wrapper, but good for analysis
            attn = (attn1 @ attn2_inv) @ attn3
            return out, attn
        return out


class PreNorm(nn.Module):
    def __init__(self, dim, fn):
        super().__init__()
        self.norm = nn.LayerNorm(dim)
        self.fn = fn

    def forward(self, x, **kwargs):
        return self.fn(self.norm(x), **kwargs) + x  # Residual connection


class FeedForward(nn.Module):
    def __init__(self, dim, hidden_dim_mult=4, dropout=0.0):
        super().__init__()
        self.net = nn.Sequential(
            nn.Linear(dim, dim * hidden_dim_mult),
            nn.GELU(),
            nn.Dropout(dropout),
            nn.Linear(dim * hidden_dim_mult, dim),
            nn.Dropout(dropout),  # Dropout after final linear layer in FFN
        )

    def forward(self, x):
        return self.net(x)


class Nystromformer(nn.Module):  # Replaces TransLayer for clarity
    def __init__(
        self,
        dim,
        num_heads,
        dim_head,
        num_landmarks,
        mlp_mult=4,
        dropout_attn=0.1,
        dropout_ff=0.1,
    ):
        super().__init__()
        self.attention_block = PreNorm(
            dim,
            NystromAttention(
                dim=dim,
                heads=num_heads,
                dim_head=dim_head,
                num_landmarks=num_landmarks,
                dropout=dropout_attn,
                residual=True,
            ),
        )
        self.feed_forward_block = PreNorm(
            dim, FeedForward(dim=dim, mult=mlp_mult, dropout=dropout_ff)
        )

    def forward(self, x, mask=None):
        x = self.attention_block(x, mask=mask)  # Includes residual from PreNorm
        x = self.feed_forward_block(x)  # Includes residual from PreNorm
        return x


class PPEG(nn.Module):  # Positional Pixel Embedding Generator
    def __init__(
        self, dim=512, kernel_size=7, groups_factor=1
    ):  # groups_factor to control groups=dim or groups=1
        super().__init__()
        # Using groups=dim makes them depthwise convolutions
        # Using groups=1 makes them standard convolutions mixing channels
        # Original TransMIL paper implies depthwise-like structure for PPEG
        groups = dim // groups_factor if groups_factor > 0 else 1

        self.proj = nn.Conv2d(dim, dim, kernel_size, 1, kernel_size // 2, groups=groups)
        self.proj1 = nn.Conv2d(
            dim, dim, kernel_size - 2, 1, (kernel_size - 2) // 2, groups=groups
        )
        self.proj2 = nn.Conv2d(
            dim, dim, kernel_size - 4, 1, (kernel_size - 4) // 2, groups=groups
        )

    def forward(self, x: torch.Tensor, H: int, W: int):
        # x: (batch_size, num_tokens, dim) where num_tokens = 1 (cls) + H*W (patches)
        batch_size, _, C = x.shape
        cls_token, feat_tokens = (
            x[:, :1],
            x[:, 1:],
        )  # Split CLS token and feature tokens

        # Reshape feature tokens to 2D grid: (B, H*W, C) -> (B, C, H, W)
        cnn_feat = feat_tokens.transpose(1, 2).view(batch_size, C, H, W)

        # Apply convolutions
        x_conv = (
            self.proj(cnn_feat) + self.proj1(cnn_feat) + self.proj2(cnn_feat) + cnn_feat
        )

        # Flatten back: (B, C, H, W) -> (B, H*W, C)
        x_processed_feats = x_conv.flatten(2).transpose(1, 2)

        # Concatenate CLS token back
        x_out = torch.cat((cls_token, x_processed_feats), dim=1)
        return x_out


class TransMIL(nn.Module):
    def __init__(
        self,
        in_dim: int,
        n_classes: int,
        embed_dim: int = 512,
        num_transformer_layers: int = 2,
        num_attn_heads: int = 8,  # num_landmarks usually embed_dim // 2
        dropout_rate: float = 0.1,  # General dropout for FC, attention, FF
        activation: str = "gelu",
        is_survival: bool = False,
    ):
        super().__init__()
        self.is_survival = is_survival
        self.n_classes = n_classes

        fc1_layers = [nn.Linear(in_dim, embed_dim)]
        fc1_layers.append(get_activation_fn(activation))
        if dropout_rate > 0:
            fc1_layers.append(nn.Dropout(dropout_rate))
        self.feature_extractor_fc = nn.Sequential(*fc1_layers)

        self.pos_layer_generator = PPEG(dim=embed_dim)
        self.cls_token = nn.Parameter(torch.randn(1, 1, embed_dim))
        nn.init.normal_(self.cls_token, std=1e-6)  # Initialize CLS token

        # Transformer layers
        self.transformer_layers = nn.ModuleList([])
        dim_head = embed_dim // num_attn_heads
        num_landmarks = embed_dim // 2  # As per original TransMIL settings
        for _ in range(num_transformer_layers):
            self.transformer_layers.append(
                TransformerLayer(
                    dim=embed_dim,
                    num_heads=num_attn_heads,
                    dim_head=dim_head,
                    num_landmarks=num_landmarks,
                    dropout_attn=dropout_rate,
                    dropout_ff=dropout_rate,
                )
            )

        self.final_norm = nn.LayerNorm(embed_dim)
        self.classifier = nn.Linear(embed_dim, n_classes)
        self.apply(initialize_weights)  # Assuming custom weight init

    def forward(self, x: torch.Tensor):
        # x: (batch_size, num_instances, in_dim)
        if x.ndim == 2:  # Single bag
            x = x.unsqueeze(0)
        batch_size = x.shape[0]

        instance_features = self.feature_extractor_fc(x.float())  # (B, N, embed_dim)

        # Prepare for PPEG: pad to make it squarish for H, W calculation
        num_instances = instance_features.shape[1]
        H_approx = W_approx = int(np.ceil(np.sqrt(num_instances)))
        padded_num_features = H_approx * W_approx

        if num_instances < padded_num_features:
            padding_size = padded_num_features - num_instances
            # Pad by repeating last few elements or zero padding
            # Using repeat of initial elements for padding (as in some implementations)
            padding_tensor = instance_features[:, :padding_size, :]
            # Or zero padding: torch.zeros(batch_size, padding_size, instance_features.shape[2], device=x.device)
            h_padded = torch.cat([instance_features, padding_tensor], dim=1)
        else:
            h_padded = instance_features[
                :, :padded_num_features, :
            ]  # Truncate if too many, or ensure N <= H_approx*W_approx

        # Add CLS token
        cls_tokens = self.cls_token.expand(batch_size, -1, -1)  # (B, 1, embed_dim)
        h_with_cls = torch.cat(
            (cls_tokens, h_padded), dim=1
        )  # (B, 1+padded_N, embed_dim)

        # First Transformer Layer (without PPEG)
        if len(self.transformer_layers) > 0:
            h_transformed = self.transformer_layers[0](h_with_cls)
        else:  # Should have at least one layer
            h_transformed = h_with_cls

        # PPEG positional encoding
        h_pos_encoded = self.pos_layer_generator(h_transformed, H_approx, W_approx)

        # Remaining Transformer Layers (if any, after PPEG)
        h_final_transformed = h_pos_encoded
        if len(self.transformer_layers) > 1:
            for i in range(1, len(self.transformer_layers)):
                h_final_transformed = self.transformer_layers[i](h_final_transformed)

        # Get CLS token representation
        cls_representation = self.final_norm(
            h_final_transformed[:, 0]
        )  # (B, embed_dim)

        logits = self.classifier(cls_representation)  # (B, n_classes)

        # Predictions
        predictions = torch.topk(logits, 1, dim=1)[1]

        if self.is_survival:
            hazards = torch.sigmoid(logits)
            survival_curves = torch.cumprod(1 - hazards, dim=1)
            return hazards, survival_curves, predictions, None, {}
        else:
            probabilities = F.softmax(logits, dim=1)
            return logits, probabilities, predictions, None, {}



================================================
File: sauron/mil_models/WIKGMIL.py
================================================
from typing import Dict, Union

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch_geometric.nn import GlobalAttention, global_max_pool, global_mean_pool

from sauron.utils.generic_utils import initialize_weights

from .activations import get_activation_fn  # Assuming it's in mil_models/

# torch.autograd.set_detect_anomaly(True) # Should be for debugging, not in library code


class WiKG(nn.Module):
    def __init__(
        self,
        in_dim: int,
        n_classes: int,
        hidden_dim: int = 512,
        top_k_neighbors: int = 6,
        agg_type: str = "bi-interaction",  # "gcn", "sage", "bi-interaction"
        pool_type: str = "attn",  # "mean", "max", "attn"
        dropout_rate: float = 0.3,
        activation: str = "leaky_relu",  # Original used LeakyReLU
        is_survival: bool = False,
    ):
        super().__init__()
        self.is_survival = is_survival
        self.n_classes = n_classes

        # Using LeakyReLU as per original if specified, else map via get_activation_fn
        if activation.lower() == "leaky_relu":
            self.activation_fn = nn.LeakyReLU()
        else:
            self.activation_fn = get_activation_fn(activation)

        self.feature_extractor_fc = nn.Sequential(
            nn.Linear(in_dim, hidden_dim),
            self.activation_fn,  # LeakyReLU or other
        )

        self.W_head = nn.Linear(hidden_dim, hidden_dim)
        self.W_tail = nn.Linear(hidden_dim, hidden_dim)

        self.scale = hidden_dim**-0.5
        self.top_k_neighbors = top_k_neighbors
        self.agg_type = agg_type

        # Gated Knowledge Attention components (Original seems to miss these, but they are usually part of WiKG)
        # Based on typical Gated Attention Units or similar structures
        # For simplicity, I'll follow the provided structure for neighbor aggregation
        # and assume KA (Knowledge Attention) weights are implicitly handled or simplified.
        # If a more complex KA is needed, it would involve more layers.

        # Aggregation layers
        if self.agg_type == "gcn":
            self.agg_linear = nn.Linear(hidden_dim, hidden_dim)
        elif self.agg_type == "sage":
            self.agg_linear = nn.Linear(hidden_dim * 2, hidden_dim)
        elif self.agg_type == "bi-interaction":
            self.agg_linear1 = nn.Linear(hidden_dim, hidden_dim)
            self.agg_linear2 = nn.Linear(hidden_dim, hidden_dim)
        else:
            raise NotImplementedError(f"Aggregation type '{agg_type}' not implemented.")

        self.message_dropout = (
            nn.Dropout(dropout_rate) if dropout_rate > 0 else nn.Identity()
        )
        self.output_norm = nn.LayerNorm(hidden_dim)
        self.classifier_fc = nn.Linear(hidden_dim, n_classes)

        # Readout (Pooling) layer
        if pool_type == "mean":
            self.readout = global_mean_pool
        elif pool_type == "max":
            self.readout = global_max_pool
        elif pool_type == "attn":
            att_net = nn.Sequential(
                nn.Linear(hidden_dim, hidden_dim // 2),
                self.activation_fn,  # LeakyReLU or other
                nn.Linear(hidden_dim // 2, 1),
            )
            self.readout = GlobalAttention(att_net)
        else:
            raise NotImplementedError(f"Pooling type '{pool_type}' not implemented.")

        self.apply(initialize_weights)

    def forward(self, x: Union[torch.Tensor, Dict[str, torch.Tensor]]):
        # x: (batch_size, num_instances, in_dim) or dict {'feature': tensor}
        if isinstance(x, dict):
            x = x.get("feature")
            if x is None:
                raise ValueError("Input dictionary must contain 'feature' key.")

        if x.ndim == 2:  # Single bag
            x = x.unsqueeze(0)

        batch_size, num_instances, _ = x.shape

        instance_features = self.feature_extractor_fc(x)  # (B, N, C_hidden)

        # Instance graph construction / feature smoothing
        # (Original: (x + x.mean(dim=1, keepdim=True)) * 0.5).
        # This averages each instance feature with the mean feature of the bag.
        instance_features = (
            instance_features + instance_features.mean(dim=1, keepdim=True)
        ) * 0.5

        e_h = self.W_head(instance_features)  # (B, N, C_h) "head" features for query
        e_t = self.W_tail(
            instance_features
        )  # (B, N, C_h) "tail" features for key/value

        # Construct neighborhood via attention
        # attn_logit: (B, N, N) - similarity between each pair of instances in a bag
        attn_logit = torch.bmm(e_h * self.scale, e_t.transpose(1, 2))

        # Select top-k neighbors for each instance
        # topk_weights: (B, N, K_neighbors), topk_indices: (B, N, K_neighbors)
        topk_weights, topk_indices = torch.topk(
            attn_logit, k=self.top_k_neighbors, dim=-1
        )

        # Gather features of top-k neighbors
        # e_t is (B, N, C_h). topk_indices is (B, N, K_neigh).
        # We need to gather along dim 1 (instance dimension) for each batch element.
        # expanded_indices = topk_indices.unsqueeze(-1).expand(-1, -1, -1, e_t.shape[-1])
        # neighbor_features = torch.gather(e_t.unsqueeze(2).expand(-1, -1, num_instances, -1), 2, expanded_indices)
        # More direct way using advanced indexing:
        batch_idx_for_gather = torch.arange(batch_size, device=x.device).view(-1, 1, 1)
        neighbor_features = e_t[
            batch_idx_for_gather, topk_indices
        ]  # (B, N, K_neigh, C_h)

        # Softmax for attention probabilities over neighbors
        topk_attention_probs = F.softmax(topk_weights, dim=2)  # (B, N, K_neigh)

        # Weighted sum of neighbor features (eh_r in original)
        # This part from original "eh_r = torch.mul(topk_prob.unsqueeze(-1), Nb_h) + torch.matmul((1 - topk_prob).unsqueeze(-1), e_h.unsqueeze(2))"
        # is unusual for standard attention. A simple weighted sum is more common.
        # eh_r seems like a mix of neighbor features and self features.
        # For now, let's compute aggregated neighbor representation:
        aggregated_neighbors = torch.einsum(
            "bnk,bnkc->bnc", topk_attention_probs, neighbor_features
        )  # (B, N, C_h)

        # The "gated knowledge attention" part (ka_weight, ka_prob, e_Nh) from original
        # seems to be another layer of attention on these neighbors.
        # For simplicity and robustness, using the aggregated_neighbors directly or a simplified KA.
        # Original KA:
        # e_h_expand = e_h.unsqueeze(2).expand(-1, -1, self.top_k_neighbors, -1) # (B, N, K_neigh, C_h)
        # gate_input = torch.tanh(e_h_expand + neighbor_features) # Combine self with neighbors
        # ka_logit = torch.einsum("bnkc,bnkc->bnk", neighbor_features, gate_input) # Dot product
        # ka_attention_probs = F.softmax(ka_logit, dim=2) # (B, N, K_neigh)
        # e_Nh_refined_neighbors = torch.einsum('bnk,bnkc->bnc', ka_attention_probs, neighbor_features) # (B,N,C_h)
        # This e_Nh_refined_neighbors is the `e_Nh` from original. Let's use this refined version.
        e_h_expanded = e_h.unsqueeze(2).expand_as(neighbor_features)
        gate_val = torch.tanh(e_h_expanded + neighbor_features)  # (B,N,K,C)
        ka_weights = torch.sum(neighbor_features * gate_val, dim=-1)  # (B,N,K)
        ka_probs = F.softmax(ka_weights, dim=-1)  # (B,N,K)
        e_Nh = torch.sum(ka_probs.unsqueeze(-1) * neighbor_features, dim=2)  # (B,N,C)

        # Node feature aggregation (GCN, SAGE, Bi-interaction style)
        if self.agg_type == "gcn":
            aggregated_embedding = self.activation_fn(self.agg_linear(e_h + e_Nh))
        elif self.agg_type == "sage":
            concat_embedding = torch.cat([e_h, e_Nh], dim=2)
            aggregated_embedding = self.activation_fn(self.agg_linear(concat_embedding))
        elif self.agg_type == "bi-interaction":
            sum_embedding = self.activation_fn(self.agg_linear1(e_h + e_Nh))
            bi_embedding = self.activation_fn(self.agg_linear2(e_h * e_Nh))
            aggregated_embedding = sum_embedding + bi_embedding
        else:  # Should not happen due to init check
            aggregated_embedding = e_h

        h_messages = self.message_dropout(aggregated_embedding)  # (B, N, C_h)

        # Readout to get bag-level representation
        # Reshape for torch_geometric global pooling: (TotalNodes, Features)
        h_reshaped = h_messages.contiguous().view(-1, h_messages.size(-1))
        # Create batch vector for pooling: [0,0..0, 1,1..1, ..., B-1..B-1]
        batch_vector = torch.arange(batch_size, device=x.device).repeat_interleave(
            num_instances
        )

        bag_representation = self.readout(h_reshaped, batch=batch_vector)  # (B, C_h)
        bag_representation = self.output_norm(bag_representation)
        logits = self.classifier_fc(bag_representation)  # (B, n_classes)

        # Predictions
        predictions = torch.topk(logits, 1, dim=1)[1]

        # A_raw could be topk_attention_probs or ka_probs. Let's use KA.
        # Need to decide which attention score to return, ka_probs is instance-neighbor level.
        # For bag level, maybe average instance attention? For now, None.
        attention_scores_raw = None

        if self.is_survival:
            hazards = torch.sigmoid(logits)
            survival_curves = torch.cumprod(1 - hazards, dim=1)
            return hazards, survival_curves, predictions, attention_scores_raw, {}
        else:
            probabilities = F.softmax(logits, dim=1)
            return logits, probabilities, predictions, attention_scores_raw, {}



================================================
File: sauron/mil_models/activations.py
================================================
import torch.nn as nn


def get_activation_fn(activation_name: str) -> nn.Module:
    """Returns the activation function module."""
    if activation_name.lower() == "relu":
        return nn.ReLU()
    elif activation_name.lower() == "gelu":
        return nn.GELU()
    else:
        raise ValueError(f"Unsupported activation function: {activation_name}")



================================================
File: sauron/mil_models/models_factory.py
================================================
from typing import Any, Dict, Type, Union

import torch.nn as nn

from .ABMIL import DAttention as AttMIL  # Using DAttention as att_mil
from .ABMIL import GatedAttention
from .DiffABMIL import DifferentiableAttentionMIL
from .MambaMIL import MambaMIL
from .MaxMIL import MaxMIL
from .MeanMIL import MeanMIL
from .S4MIL import S4Model
from .TransMIL import TransMIL
from .WIKGMIL import WiKG

# Mapping of model types to their corresponding classes
MODEL_MAP: Dict[str, Type[nn.Module]] = {
    "mean_mil": MeanMIL,
    "max_mil": MaxMIL,
    "att_mil": AttMIL,  # Standard Attention MIL (ABMIL's DAttention)
    "gated_att_mil": GatedAttention,  # Gated Attention MIL
    "diff_abmil": DifferentiableAttentionMIL,  # Differentiable Attention (Multi-Head)
    "trans_mil": TransMIL,
    "s4_mil": S4Model,  # Alias "s4model" can be handled by .lower()
    "mamba_mil": MambaMIL,
    "wikg_mil": WiKG,
}


def initialize_mil_model(args: Any) -> nn.Module:
    """Initialize the MIL model based on args namespace."""

    model_type_key = args.model_type.lower()

    # Common parameters for most models
    common_params = {
        "in_dim": args.in_dim,
        "n_classes": args.n_classes,
        "dropout_rate": getattr(
            args, "dropout_rate", getattr(args, "drop_out", 0.25)
        ),  # Standardize dropout arg name
        "activation": getattr(
            args, "activation_fn", "relu"
        ),  # Standardize activation arg name
        "is_survival": args.task_type == "survival",
    }

    print(
        f"Initializing model: {model_type_key} (Activation: {common_params['activation']}, "
        f"Survival: {common_params['is_survival']}, Dropout: {common_params['dropout_rate']})"
    )

    if model_type_key not in MODEL_MAP:
        raise NotImplementedError(
            f"Model type '{model_type_key}' not implemented or not in MODEL_MAP."
        )

    model_class = MODEL_MAP[model_type_key]

    # Model-specific parameters
    specific_params = {}
    if model_type_key == "mamba_mil":
        specific_params.update(
            {
                "embed_dim": getattr(args, "mambamil_embed_dim", 512),
                "num_mamba_layers": getattr(
                    args, "mambamil_layers", 2
                ),  # Renamed from mambamil_layer
                "mamba_type": getattr(args, "mambamil_type", "SRMamba"),
                "srmamba_rate": getattr(args, "mambamil_rate", 10),
            }
        )
    elif model_type_key == "s4_mil":
        specific_params.update(
            {
                "embed_dim": getattr(args, "s4mil_embed_dim", 512),
                "s4_d_state": getattr(args, "s4mil_d_state", 64),
            }
        )
    elif model_type_key == "trans_mil":
        specific_params.update(
            {
                "embed_dim": getattr(args, "transmil_embed_dim", 512),
                "num_transformer_layers": getattr(args, "transmil_layers", 2),
                "num_attn_heads": getattr(args, "transmil_heads", 8),
            }
        )
    elif model_type_key == "att_mil" or model_type_key == "gated_att_mil":
        # ABMIL variants might have specific embed_dim, attention_hidden_dim if configurable
        pass  # Uses default internal dimensions or can be made configurable via args
    elif model_type_key == "diff_abmil":
        specific_params.update(
            {
                "embed_dim": getattr(args, "diffabmil_embed_dim", 512),
                "num_heads": getattr(args, "diffabmil_num_heads", 8),
            }
        )
    elif model_type_key == "wikg_mil":
        specific_params.update(
            {
                "hidden_dim": getattr(args, "wikg_hidden_dim", 512),
                "top_k_neighbors": getattr(args, "wikg_topk", 6),
                "agg_type": getattr(args, "wikg_agg_type", "bi-interaction"),
                "pool_type": getattr(args, "wikg_pool_type", "attn"),
                # activation for WiKG is handled in common_params, but WiKG often uses LeakyReLU
                "activation": getattr(
                    args, "activation_fn", "leaky_relu"
                ),  # Default leaky for WiKG
            }
        )
    elif model_type_key in ["max_mil", "mean_mil"]:
        specific_params.update(
            {
                "hidden_dim": getattr(
                    args, f"{model_type_key.split('_')[0]}_mil_hidden_dim", 512
                ),
            }
        )

    # Combine common and specific parameters
    all_params = {**common_params, **specific_params}

    # Filter params for the specific model constructor
    # This requires inspecting the model's __init__ signature, or assuming all models handle extra **kwargs
    # For robustness, it's better to only pass expected args.
    # However, many __init__ are now standardized.
    # A simpler approach for now is to pass all_params and let models pick what they need
    # if their __init__ are designed for that or we ensure only relevant ones are passed.

    # For now, let's assume model constructors can handle the combined dict,
    # or we'll rely on the specific_params logic to be exhaustive for non-common ones.
    # A more robust way: inspect model_class.__init__ and filter.
    # For this refactor, I'm standardizing __init__ so common_params + specific_params should largely work.

    return model_class(**all_params)



================================================
File: sauron/mil_models/mamba_ssm/__init__.py
================================================
__version__ = "1.1.2"

from mil_models.mamba_ssm.models.mixer_seq_simple import MambaLMHeadModel
from mil_models.mamba_ssm.modules.bimamba import BiMamba
from mil_models.mamba_ssm.modules.mamba_simple import Mamba
from mil_models.mamba_ssm.modules.srmamba import SRMamba
from mil_models.mamba_ssm.ops.selective_scan_interface import (
    mamba_inner_fn,
    selective_scan_fn,
)



================================================
File: sauron/mil_models/mamba_ssm/models/__init__.py
================================================



================================================
File: sauron/mil_models/mamba_ssm/models/config_mamba.py
================================================
from dataclasses import dataclass, field


@dataclass
class MambaConfig:

    d_model: int = 2560
    n_layer: int = 64
    vocab_size: int = 50277
    ssm_cfg: dict = field(default_factory=dict)
    rms_norm: bool = True
    residual_in_fp32: bool = True
    fused_add_norm: bool = True
    pad_vocab_size_multiple: int = 8



================================================
File: sauron/mil_models/mamba_ssm/models/mixer_seq_simple.py
================================================
# Copyright (c) 2023, Albert Gu, Tri Dao.

import json
import math
import os
from collections import namedtuple
from functools import partial

import torch
import torch.nn as nn

from mamba_ssm.models.config_mamba import MambaConfig
from mamba_ssm.modules.mamba_simple import Block, Mamba
from mamba_ssm.utils.generation import GenerationMixin
from mamba_ssm.utils.hf import load_config_hf, load_state_dict_hf

try:
    from mamba_ssm.ops.triton.layernorm import RMSNorm, layer_norm_fn, rms_norm_fn
except ImportError:
    RMSNorm, layer_norm_fn, rms_norm_fn = None, None, None


def create_block(
    d_model,
    ssm_cfg=None,
    norm_epsilon=1e-5,
    rms_norm=False,
    residual_in_fp32=False,
    fused_add_norm=False,
    layer_idx=None,
    device=None,
    dtype=None,
):
    if ssm_cfg is None:
        ssm_cfg = {}
    factory_kwargs = {"device": device, "dtype": dtype}
    mixer_cls = partial(Mamba, layer_idx=layer_idx, **ssm_cfg, **factory_kwargs)
    norm_cls = partial(
        nn.LayerNorm if not rms_norm else RMSNorm, eps=norm_epsilon, **factory_kwargs
    )
    block = Block(
        d_model,
        mixer_cls,
        norm_cls=norm_cls,
        fused_add_norm=fused_add_norm,
        residual_in_fp32=residual_in_fp32,
    )
    block.layer_idx = layer_idx
    return block


# https://github.com/huggingface/transformers/blob/c28d04e9e252a1a099944e325685f14d242ecdcd/src/transformers/models/gpt2/modeling_gpt2.py#L454
def _init_weights(
    module,
    n_layer,
    initializer_range=0.02,  # Now only used for embedding layer.
    rescale_prenorm_residual=True,
    n_residuals_per_layer=1,  # Change to 2 if we have MLP
):
    if isinstance(module, nn.Linear):
        if module.bias is not None:
            if not getattr(module.bias, "_no_reinit", False):
                nn.init.zeros_(module.bias)
    elif isinstance(module, nn.Embedding):
        nn.init.normal_(module.weight, std=initializer_range)

    if rescale_prenorm_residual:
        # Reinitialize selected weights subject to the OpenAI GPT-2 Paper Scheme:
        #   > A modified initialization which accounts for the accumulation on the residual path with model depth. Scale
        #   > the weights of residual layers at initialization by a factor of 1/√N where N is the # of residual layers.
        #   >   -- GPT-2 :: https://openai.com/blog/better-language-models/
        #
        # Reference (Megatron-LM): https://github.com/NVIDIA/Megatron-LM/blob/main/megatron/model/gpt_model.py
        for name, p in module.named_parameters():
            if name in ["out_proj.weight", "fc2.weight"]:
                # Special Scaled Initialization --> There are 2 Layer Norms per Transformer Block
                # Following Pytorch init, except scale by 1/sqrt(2 * n_layer)
                # We need to reinit p since this code could be called multiple times
                # Having just p *= scale would repeatedly scale it down
                nn.init.kaiming_uniform_(p, a=math.sqrt(5))
                with torch.no_grad():
                    p /= math.sqrt(n_residuals_per_layer * n_layer)


class MixerModel(nn.Module):
    def __init__(
        self,
        d_model: int,
        n_layer: int,
        vocab_size: int,
        ssm_cfg=None,
        norm_epsilon: float = 1e-5,
        rms_norm: bool = False,
        initializer_cfg=None,
        fused_add_norm=False,
        residual_in_fp32=False,
        device=None,
        dtype=None,
    ) -> None:
        factory_kwargs = {"device": device, "dtype": dtype}
        super().__init__()
        self.residual_in_fp32 = residual_in_fp32

        self.embedding = nn.Embedding(vocab_size, d_model, **factory_kwargs)

        # We change the order of residual and layer norm:
        # Instead of LN -> Attn / MLP -> Add, we do:
        # Add -> LN -> Attn / MLP / Mixer, returning both the residual branch (output of Add) and
        # the main branch (output of MLP / Mixer). The model definition is unchanged.
        # This is for performance reason: we can fuse add + layer_norm.
        self.fused_add_norm = fused_add_norm
        if self.fused_add_norm:
            if layer_norm_fn is None or rms_norm_fn is None:
                raise ImportError("Failed to import Triton LayerNorm / RMSNorm kernels")

        self.layers = nn.ModuleList(
            [
                create_block(
                    d_model,
                    ssm_cfg=ssm_cfg,
                    norm_epsilon=norm_epsilon,
                    rms_norm=rms_norm,
                    residual_in_fp32=residual_in_fp32,
                    fused_add_norm=fused_add_norm,
                    layer_idx=i,
                    **factory_kwargs,
                )
                for i in range(n_layer)
            ]
        )

        self.norm_f = (nn.LayerNorm if not rms_norm else RMSNorm)(
            d_model, eps=norm_epsilon, **factory_kwargs
        )

        self.apply(
            partial(
                _init_weights,
                n_layer=n_layer,
                **(initializer_cfg if initializer_cfg is not None else {}),
            )
        )

    def allocate_inference_cache(self, batch_size, max_seqlen, dtype=None, **kwargs):
        return {
            i: layer.allocate_inference_cache(
                batch_size, max_seqlen, dtype=dtype, **kwargs
            )
            for i, layer in enumerate(self.layers)
        }

    def forward(self, input_ids, inference_params=None):
        hidden_states = self.embedding(input_ids)
        residual = None
        for layer in self.layers:
            hidden_states, residual = layer(
                hidden_states, residual, inference_params=inference_params
            )
        if not self.fused_add_norm:
            residual = (
                (hidden_states + residual) if residual is not None else hidden_states
            )
            hidden_states = self.norm_f(residual.to(dtype=self.norm_f.weight.dtype))
        else:
            # Set prenorm=False here since we don't need the residual
            fused_add_norm_fn = (
                rms_norm_fn if isinstance(self.norm_f, RMSNorm) else layer_norm_fn
            )
            hidden_states = fused_add_norm_fn(
                hidden_states,
                self.norm_f.weight,
                self.norm_f.bias,
                eps=self.norm_f.eps,
                residual=residual,
                prenorm=False,
                residual_in_fp32=self.residual_in_fp32,
            )
        return hidden_states


class MambaLMHeadModel(nn.Module, GenerationMixin):
    def __init__(
        self,
        config: MambaConfig,
        initializer_cfg=None,
        device=None,
        dtype=None,
    ) -> None:
        self.config = config
        d_model = config.d_model
        n_layer = config.n_layer
        vocab_size = config.vocab_size
        ssm_cfg = config.ssm_cfg
        rms_norm = config.rms_norm
        residual_in_fp32 = config.residual_in_fp32
        fused_add_norm = config.fused_add_norm
        pad_vocab_size_multiple = config.pad_vocab_size_multiple
        factory_kwargs = {"device": device, "dtype": dtype}

        super().__init__()
        if vocab_size % pad_vocab_size_multiple != 0:
            vocab_size += pad_vocab_size_multiple - (
                vocab_size % pad_vocab_size_multiple
            )
        self.backbone = MixerModel(
            d_model=d_model,
            n_layer=n_layer,
            vocab_size=vocab_size,
            ssm_cfg=ssm_cfg,
            rms_norm=rms_norm,
            initializer_cfg=initializer_cfg,
            fused_add_norm=fused_add_norm,
            residual_in_fp32=residual_in_fp32,
            **factory_kwargs,
        )
        self.lm_head = nn.Linear(d_model, vocab_size, bias=False, **factory_kwargs)

        # Initialize weights and apply final processing
        self.apply(
            partial(
                _init_weights,
                n_layer=n_layer,
                **(initializer_cfg if initializer_cfg is not None else {}),
            )
        )
        self.tie_weights()

    def tie_weights(self):
        self.lm_head.weight = self.backbone.embedding.weight

    def allocate_inference_cache(self, batch_size, max_seqlen, dtype=None, **kwargs):
        return self.backbone.allocate_inference_cache(
            batch_size, max_seqlen, dtype=dtype, **kwargs
        )

    def forward(
        self, input_ids, position_ids=None, inference_params=None, num_last_tokens=0
    ):
        """
        "position_ids" is just to be compatible with Transformer generation. We don't use it.
        num_last_tokens: if > 0, only return the logits for the last n tokens
        """
        hidden_states = self.backbone(input_ids, inference_params=inference_params)
        if num_last_tokens > 0:
            hidden_states = hidden_states[:, -num_last_tokens:]
        lm_logits = self.lm_head(hidden_states)
        CausalLMOutput = namedtuple("CausalLMOutput", ["logits"])
        return CausalLMOutput(logits=lm_logits)

    @classmethod
    def from_pretrained(cls, pretrained_model_name, device=None, dtype=None, **kwargs):
        config_data = load_config_hf(pretrained_model_name)
        config = MambaConfig(**config_data)
        model = cls(config, device=device, dtype=dtype, **kwargs)
        model.load_state_dict(
            load_state_dict_hf(pretrained_model_name, device=device, dtype=dtype)
        )
        return model

    def save_pretrained(self, save_directory):
        """
        Minimal implementation of save_pretrained for MambaLMHeadModel.
        Save the model and its configuration file to a directory.
        """
        # Ensure save_directory exists
        if not os.path.exists(save_directory):
            os.makedirs(save_directory)

        # Save the model's state_dict
        model_path = os.path.join(save_directory, "pytorch_model.bin")
        torch.save(self.state_dict(), model_path)

        # Save the configuration of the model
        config_path = os.path.join(save_directory, "config.json")
        with open(config_path, "w") as f:
            json.dump(self.config.__dict__, f)



================================================
File: sauron/mil_models/mamba_ssm/modules/__init__.py
================================================



================================================
File: sauron/mil_models/mamba_ssm/modules/bimamba.py
================================================
# Copyright (c) 2023, Tri Dao, Albert Gu.

import math
from typing import Optional

import torch
import torch.nn as nn
import torch.nn.functional as F
from einops import rearrange, repeat
from torch import Tensor

from mamba.mamba_ssm.ops.selective_scan_interface import (
    mamba_inner_fn_no_out_proj,
    selective_scan_fn,
)

try:
    from causal_conv1d import causal_conv1d_fn, causal_conv1d_update
except ImportError:
    causal_conv1d_fn, causal_conv1d_update = None

try:
    from mamba_ssm.ops.triton.selective_state_update import selective_state_update
except ImportError:
    selective_state_update = None

try:
    from mamba_ssm.ops.triton.layernorm import RMSNorm, layer_norm_fn, rms_norm_fn
except ImportError:
    RMSNorm, layer_norm_fn, rms_norm_fn = None, None, None


class TransposeTokenReEmbedding:
    @staticmethod
    def transpose_normal_padding(x, rate):
        x = rearrange(x, "b c l -> b l c")
        B, N, C = x.shape
        value = N // rate
        if N % rate != 0:
            padding_length = (value + 1) * rate - N
            padded_x = torch.nn.functional.pad(x, (0, 0, 0, padding_length))
        else:
            padded_x = x
        x_ = rearrange(padded_x, "b (k w) d -> b (w k) d", w=rate)
        x_ = rearrange(x_, "b l c -> b c l")
        return x_

    @staticmethod
    def transpose_remove_padding(x, rate, length):
        x = rearrange(x, "b c l -> b l c")
        x = rearrange(x, "b (w k) d -> b (k w) d", w=rate)
        x = x[:, :length, :]
        x = rearrange(x, "b l c -> b c l")
        return x


class BiMamba(nn.Module):
    def __init__(
        self,
        d_model,
        d_state=16,
        d_conv=4,
        expand=2,
        dt_rank="auto",
        dt_min=0.001,
        dt_max=0.1,
        dt_init="random",
        dt_scale=1.0,
        dt_init_floor=1e-4,
        conv_bias=True,
        bias=False,
        use_fast_path=True,  # Fused kernel options
        layer_idx=None,
        device=None,
        dtype=None,
    ):
        factory_kwargs = {"device": device, "dtype": dtype}
        super().__init__()
        self.d_model = d_model
        self.d_state = d_state
        self.d_conv = d_conv
        self.expand = expand
        self.d_inner = int(self.expand * self.d_model)
        self.dt_rank = math.ceil(self.d_model / 16) if dt_rank == "auto" else dt_rank
        self.use_fast_path = use_fast_path
        self.layer_idx = layer_idx

        self.in_proj = nn.Linear(
            self.d_model, self.d_inner * 2, bias=bias, **factory_kwargs
        )

        self.conv1d = nn.Conv1d(
            in_channels=self.d_inner,
            out_channels=self.d_inner,
            bias=conv_bias,
            kernel_size=d_conv,
            groups=self.d_inner,
            padding=d_conv - 1,
            **factory_kwargs,
        )
        self.conv1d_b = nn.Conv1d(
            in_channels=self.d_inner,
            out_channels=self.d_inner,
            bias=conv_bias,
            kernel_size=d_conv,
            groups=self.d_inner,
            padding=d_conv - 1,
            **factory_kwargs,
        )

        self.activation = "silu"
        self.act = nn.SiLU()

        self.x_proj = nn.Linear(
            self.d_inner, self.dt_rank + self.d_state * 2, bias=False, **factory_kwargs
        )
        self.x_proj_b = nn.Linear(
            self.d_inner, self.dt_rank + self.d_state * 2, bias=False, **factory_kwargs
        )

        self.dt_proj = nn.Linear(
            self.dt_rank, self.d_inner, bias=True, **factory_kwargs
        )
        self.dt_proj_b = nn.Linear(
            self.dt_rank, self.d_inner, bias=True, **factory_kwargs
        )

        # Initialize special dt projection to preserve variance at initialization
        dt_init_std = self.dt_rank**-0.5 * dt_scale
        if dt_init == "constant":
            nn.init.constant_(self.dt_proj.weight, dt_init_std)
            nn.init.constant_(self.dt_proj_b.weight, dt_init_std)  # ?
        elif dt_init == "random":
            nn.init.uniform_(self.dt_proj.weight, -dt_init_std, dt_init_std)
            nn.init.uniform_(self.dt_proj_b.weight, -dt_init_std, dt_init_std)  # ?
        else:
            raise NotImplementedError

        # Initialize dt bias so that F.softplus(dt_bias) is between dt_min and dt_max
        dt = torch.exp(
            torch.rand(self.d_inner, **factory_kwargs)
            * (math.log(dt_max) - math.log(dt_min))
            + math.log(dt_min)
        ).clamp(min=dt_init_floor)
        # Inverse of softplus: https://github.com/pytorch/pytorch/issues/72759
        inv_dt = dt + torch.log(-torch.expm1(-dt))
        with torch.no_grad():
            self.dt_proj.bias.copy_(inv_dt)
            self.dt_proj_b.bias.copy_(inv_dt)  # ?
        # Our initialization would set all Linear.bias to zero, need to mark this one as _no_reinit
        self.dt_proj.bias._no_reinit = True
        self.dt_proj_b.bias._no_reinit = True  # ?

        # S4D real initialization
        A = repeat(
            torch.arange(1, self.d_state + 1, dtype=torch.float32, device=device),
            "n -> d n",
            d=self.d_inner,
        ).contiguous()
        A_log = torch.log(A)  # Keep A_log in fp32
        self.A_log = nn.Parameter(A_log)
        self.A_log._no_weight_decay = True

        A_b = repeat(
            torch.arange(1, self.d_state + 1, dtype=torch.float32, device=device),
            "n -> d n",
            d=self.d_inner,
        ).contiguous()
        A_b_log = torch.log(A_b)  # Keep A_b_log in fp32
        self.A_b_log = nn.Parameter(A_b_log)
        self.A_b_log._no_weight_decay = True

        # D "skip" parameter
        self.D = nn.Parameter(torch.ones(self.d_inner, device=device))  # Keep in fp32
        self.D._no_weight_decay = True
        self.D_b = nn.Parameter(torch.ones(self.d_inner, device=device))  # Keep in fp32
        self.D_b._no_weight_decay = True

        self.out_proj = nn.Linear(
            self.d_inner, self.d_model, bias=bias, **factory_kwargs
        )

    def forward(self, hidden_states, inference_params=None, rate=10):
        """
        hidden_states: (B, L, D)
        Returns: same shape as hidden_states
        """
        batch, seqlen, dim = hidden_states.shape

        conv_state, ssm_state = None, None
        if inference_params is not None:
            conv_state, ssm_state = self._get_states_from_cache(inference_params, batch)
            if inference_params.seqlen_offset > 0:
                # The states are updated inplace
                out, _, _ = self.step(hidden_states, conv_state, ssm_state)
                return out

        # We do matmul and transpose BLH -> HBL at the same time
        xz = rearrange(
            self.in_proj.weight @ rearrange(hidden_states, "b l d -> d (b l)"),
            "d (b l) -> b d l",
            l=seqlen,
        )
        if self.in_proj.bias is not None:
            xz = xz + rearrange(self.in_proj.bias.to(dtype=xz.dtype), "d -> d 1")

        A = -torch.exp(self.A_log.float())  # (d_inner, d_state)
        A_b = -torch.exp(self.A_b_log.float())
        # In the backward pass we write dx and dz next to each other to avoid torch.cat
        if (
            self.use_fast_path and inference_params is None
        ):  # Doesn't support outputting the states
            out = mamba_inner_fn_no_out_proj(
                xz,
                self.conv1d.weight,
                self.conv1d.bias,
                self.x_proj.weight,
                self.dt_proj.weight,
                A,
                None,  # input-dependent B
                None,  # input-dependent C
                self.D.float(),
                delta_bias=self.dt_proj.bias.float(),
                delta_softplus=True,
            )
            xz_b = xz.flip([-1])
            out_b = mamba_inner_fn_no_out_proj(
                xz_b,
                self.conv1d_b.weight,
                self.conv1d_b.bias,
                self.x_proj_b.weight,
                self.dt_proj_b.weight,
                A_b,
                None,
                None,
                self.D_b.float(),
                delta_bias=self.dt_proj_b.bias.float(),
                delta_softplus=True,
            )
            out_b = out_b.flip([-1])
            out = F.linear(
                rearrange(out + out_b, "b d l -> b l d"),
                self.out_proj.weight,
                self.out_proj.bias,
            )
        else:
            # x, z
            x, z = xz.chunk(2, dim=1)
            x_b = x.flip([-1])
            # Compute short convolution
            if conv_state is not None:
                # If we just take x[:, :, -self.d_conv :], it will error if seqlen < self.d_conv
                # Instead F.pad will pad with zeros if seqlen < self.d_conv, and truncate otherwise.
                conv_state.copy_(
                    F.pad(x, (self.d_conv - x.shape[-1], 0))
                )  # Update state (B D W)
            if causal_conv1d_fn is None:
                x = self.act(self.conv1d(x)[..., :seqlen])
                x_b = self.act(self.conv1d_b(x_b)[..., :seqlen])
            else:
                assert self.activation in ["silu", "swish"]
                x = causal_conv1d_fn(
                    x=x,
                    weight=rearrange(self.conv1d.weight, "d 1 w -> d w"),
                    bias=self.conv1d.bias,
                    activation=self.activation,
                )
                x_b = causal_conv1d_fn(
                    x=x_b,
                    weight=rearrange(self.conv1d_b.weight, "d 1 w -> d w"),
                    bias=self.conv1d_b.bias,
                    activation=self.activation,
                )

            # We're careful here about the layout, to avoid extra transposes.
            # We want dt to have d as the slowest moving dimension
            # and L as the fastest moving dimension, since those are what the ssm_scan kernel expects.
            x_dbl = self.x_proj(rearrange(x, "b d l -> (b l) d"))  # (bl d)
            dt, B, C = torch.split(
                x_dbl, [self.dt_rank, self.d_state, self.d_state], dim=-1
            )
            dt = self.dt_proj.weight @ dt.t()
            dt = rearrange(dt, "d (b l) -> b d l", l=seqlen)
            B = rearrange(B, "(b l) dstate -> b dstate l", l=seqlen).contiguous()
            C = rearrange(C, "(b l) dstate -> b dstate l", l=seqlen).contiguous()

            x_dbl_b = self.x_proj_b(rearrange(x_b, "b d l -> (b l) d"))  # (bl d)
            dt_b, B_b, C_b = torch.split(
                x_dbl_b, [self.dt_rank, self.d_state, self.d_state], dim=-1
            )
            dt_b = self.dt_proj_b.weight @ dt_b.t()
            dt_b = rearrange(dt_b, "d (b l) -> b d l", l=seqlen)
            B_b = rearrange(B_b, "(b l) dstate -> b dstate l", l=seqlen).contiguous()
            C_b = rearrange(C_b, "(b l) dstate -> b dstate l", l=seqlen).contiguous()
            assert self.activation in ["silu", "swish"]
            y = selective_scan_fn(
                x,
                dt,
                A,
                B,
                C,
                self.D.float(),
                z=z,
                delta_bias=self.dt_proj.bias.float(),
                delta_softplus=True,
                return_last_state=ssm_state is not None,
            )
            y_b = selective_scan_fn(
                x_b,
                dt_b,
                A_b,
                B_b,
                C_b,
                self.D_b.float(),
                z=z,
                delta_bias=self.dt_proj_b.bias.float(),
                delta_softplus=True,
                return_last_state=ssm_state is not None,
            )
            if ssm_state is not None:
                y, last_state = y
                ssm_state.copy_(last_state)
            y = rearrange(y, "b d l -> b l d")
            y_b = rearrange(y_b, "b d l -> b l d")
            out = self.out_proj(y_b + y)
        return out

    def step(self, hidden_states, conv_state, ssm_state):
        dtype = hidden_states.dtype
        assert (
            hidden_states.shape[1] == 1
        ), "Only support decoding with 1 token at a time for now"
        xz = self.in_proj(hidden_states.squeeze(1))  # (B 2D)
        x, z = xz.chunk(2, dim=-1)  # (B D)

        # Conv step
        if causal_conv1d_update is None:
            conv_state.copy_(
                torch.roll(conv_state, shifts=-1, dims=-1)
            )  # Update state (B D W)
            conv_state[:, :, -1] = x
            x = torch.sum(
                conv_state * rearrange(self.conv1d.weight, "d 1 w -> d w"), dim=-1
            )  # (B D)
            if self.conv1d.bias is not None:
                x = x + self.conv1d.bias
            x = self.act(x).to(dtype=dtype)
        else:
            x = causal_conv1d_update(
                x,
                conv_state,
                rearrange(self.conv1d.weight, "d 1 w -> d w"),
                self.conv1d.bias,
                self.activation,
            )

        x_db = self.x_proj(x)  # (B dt_rank+2*d_state)
        dt, B, C = torch.split(x_db, [self.dt_rank, self.d_state, self.d_state], dim=-1)
        # Don't add dt_bias here
        dt = F.linear(dt, self.dt_proj.weight)  # (B d_inner)
        A = -torch.exp(self.A_log.float())  # (d_inner, d_state)

        # SSM step
        if selective_state_update is None:
            # Discretize A and B
            dt = F.softplus(dt + self.dt_proj.bias.to(dtype=dt.dtype))
            dA = torch.exp(torch.einsum("bd,dn->bdn", dt, A))
            dB = torch.einsum("bd,bn->bdn", dt, B)
            ssm_state.copy_(ssm_state * dA + rearrange(x, "b d -> b d 1") * dB)
            y = torch.einsum("bdn,bn->bd", ssm_state.to(dtype), C)
            y = y + self.D.to(dtype) * x
            y = y * self.act(z)  # (B D)
        else:
            y = selective_state_update(
                ssm_state,
                x,
                dt,
                A,
                B,
                C,
                self.D,
                z=z,
                dt_bias=self.dt_proj.bias,
                dt_softplus=True,
            )

        out = self.out_proj(y)
        return out.unsqueeze(1), conv_state, ssm_state

    def allocate_inference_cache(self, batch_size, max_seqlen, dtype=None, **kwargs):
        device = self.out_proj.weight.device
        conv_dtype = self.conv1d.weight.dtype if dtype is None else dtype
        conv_state = torch.zeros(
            batch_size,
            self.d_model * self.expand,
            self.d_conv,
            device=device,
            dtype=conv_dtype,
        )
        ssm_dtype = self.dt_proj.weight.dtype if dtype is None else dtype
        # ssm_dtype = torch.float32
        ssm_state = torch.zeros(
            batch_size,
            self.d_model * self.expand,
            self.d_state,
            device=device,
            dtype=ssm_dtype,
        )
        return conv_state, ssm_state

    def _get_states_from_cache(
        self, inference_params, batch_size, initialize_states=False
    ):
        assert self.layer_idx is not None
        if self.layer_idx not in inference_params.key_value_memory_dict:
            batch_shape = (batch_size,)
            conv_state = torch.zeros(
                batch_size,
                self.d_model * self.expand,
                self.d_conv,
                device=self.conv1d.weight.device,
                dtype=self.conv1d.weight.dtype,
            )
            ssm_state = torch.zeros(
                batch_size,
                self.d_model * self.expand,
                self.d_state,
                device=self.dt_proj.weight.device,
                dtype=self.dt_proj.weight.dtype,
                # dtype=torch.float32,
            )
            inference_params.key_value_memory_dict[self.layer_idx] = (
                conv_state,
                ssm_state,
            )
        else:
            conv_state, ssm_state = inference_params.key_value_memory_dict[
                self.layer_idx
            ]
            # TODO: What if batch size changes between generation, and we reuse the same states?
            if initialize_states:
                conv_state.zero_()
                ssm_state.zero_()
        return conv_state, ssm_state


class Block(nn.Module):
    def __init__(
        self,
        dim,
        mixer_cls,
        norm_cls=nn.LayerNorm,
        fused_add_norm=False,
        residual_in_fp32=False,
    ):
        """
        Simple block wrapping a mixer class with LayerNorm/RMSNorm and residual connection"

        This Block has a slightly different structure compared to a regular
        prenorm Transformer block.
        The standard block is: LN -> MHA/MLP -> Add.
        [Ref: https://arxiv.org/abs/2002.04745]
        Here we have: Add -> LN -> Mixer, returning both
        the hidden_states (output of the mixer) and the residual.
        This is purely for performance reasons, as we can fuse add and LayerNorm.
        The residual needs to be provided (except for the very first block).
        """
        super().__init__()
        self.residual_in_fp32 = residual_in_fp32
        self.fused_add_norm = fused_add_norm
        self.mixer = mixer_cls(dim)
        self.norm = norm_cls(dim)
        if self.fused_add_norm:
            assert RMSNorm is not None, "RMSNorm import fails"
            assert isinstance(
                self.norm, (nn.LayerNorm, RMSNorm)
            ), "Only LayerNorm and RMSNorm are supported for fused_add_norm"

    def forward(
        self,
        hidden_states: Tensor,
        residual: Optional[Tensor] = None,
        inference_params=None,
    ):
        r"""Pass the input through the encoder layer.

        Args:
            hidden_states: the sequence to the encoder layer (required).
            residual: hidden_states = Mixer(LN(residual))
        """
        if not self.fused_add_norm:
            residual = (
                (hidden_states + residual) if residual is not None else hidden_states
            )
            hidden_states = self.norm(residual.to(dtype=self.norm.weight.dtype))
            if self.residual_in_fp32:
                residual = residual.to(torch.float32)
        else:
            fused_add_norm_fn = (
                rms_norm_fn if isinstance(self.norm, RMSNorm) else layer_norm_fn
            )
            hidden_states, residual = fused_add_norm_fn(
                hidden_states,
                self.norm.weight,
                self.norm.bias,
                residual=residual,
                prenorm=True,
                residual_in_fp32=self.residual_in_fp32,
                eps=self.norm.eps,
            )
        hidden_states = self.mixer(hidden_states, inference_params=inference_params)
        return hidden_states, residual

    def allocate_inference_cache(self, batch_size, max_seqlen, dtype=None, **kwargs):
        return self.mixer.allocate_inference_cache(
            batch_size, max_seqlen, dtype=dtype, **kwargs
        )



================================================
File: sauron/mil_models/mamba_ssm/modules/mamba_simple.py
================================================
# Copyright (c) 2023, Tri Dao, Albert Gu.

import math
from typing import Optional

import torch
import torch.nn as nn
import torch.nn.functional as F
from einops import rearrange, repeat
from torch import Tensor

from mamba_ssm.ops.selective_scan_interface import mamba_inner_fn, selective_scan_fn

try:
    from causal_conv1d import causal_conv1d_fn, causal_conv1d_update
except ImportError:
    causal_conv1d_fn, causal_conv1d_update = None

try:
    from mamba_ssm.ops.triton.selective_state_update import selective_state_update
except ImportError:
    selective_state_update = None

try:
    from mamba_ssm.ops.triton.layernorm import RMSNorm, layer_norm_fn, rms_norm_fn
except ImportError:
    RMSNorm, layer_norm_fn, rms_norm_fn = None, None, None


class Mamba(nn.Module):
    def __init__(
        self,
        d_model,
        d_state=16,
        d_conv=4,
        expand=2,
        dt_rank="auto",
        dt_min=0.001,
        dt_max=0.1,
        dt_init="random",
        dt_scale=1.0,
        dt_init_floor=1e-4,
        conv_bias=True,
        bias=False,
        use_fast_path=True,  # Fused kernel options
        layer_idx=None,
        device=None,
        dtype=None,
    ):
        factory_kwargs = {"device": device, "dtype": dtype}
        super().__init__()
        self.d_model = d_model
        self.d_state = d_state
        self.d_conv = d_conv
        self.expand = expand
        self.d_inner = int(self.expand * self.d_model)
        self.dt_rank = math.ceil(self.d_model / 16) if dt_rank == "auto" else dt_rank
        self.use_fast_path = use_fast_path
        self.layer_idx = layer_idx

        self.in_proj = nn.Linear(
            self.d_model, self.d_inner * 2, bias=bias, **factory_kwargs
        )

        self.conv1d = nn.Conv1d(
            in_channels=self.d_inner,
            out_channels=self.d_inner,
            bias=conv_bias,
            kernel_size=d_conv,
            groups=self.d_inner,
            padding=d_conv - 1,
            **factory_kwargs,
        )

        self.activation = "silu"
        self.act = nn.SiLU()

        self.x_proj = nn.Linear(
            self.d_inner, self.dt_rank + self.d_state * 2, bias=False, **factory_kwargs
        )
        self.dt_proj = nn.Linear(
            self.dt_rank, self.d_inner, bias=True, **factory_kwargs
        )

        # Initialize special dt projection to preserve variance at initialization
        dt_init_std = self.dt_rank**-0.5 * dt_scale
        if dt_init == "constant":
            nn.init.constant_(self.dt_proj.weight, dt_init_std)
        elif dt_init == "random":
            nn.init.uniform_(self.dt_proj.weight, -dt_init_std, dt_init_std)
        else:
            raise NotImplementedError

        # Initialize dt bias so that F.softplus(dt_bias) is between dt_min and dt_max
        dt = torch.exp(
            torch.rand(self.d_inner, **factory_kwargs)
            * (math.log(dt_max) - math.log(dt_min))
            + math.log(dt_min)
        ).clamp(min=dt_init_floor)
        # Inverse of softplus: https://github.com/pytorch/pytorch/issues/72759
        inv_dt = dt + torch.log(-torch.expm1(-dt))
        with torch.no_grad():
            self.dt_proj.bias.copy_(inv_dt)
        # Our initialization would set all Linear.bias to zero, need to mark this one as _no_reinit
        self.dt_proj.bias._no_reinit = True

        # S4D real initialization
        A = repeat(
            torch.arange(1, self.d_state + 1, dtype=torch.float32, device=device),
            "n -> d n",
            d=self.d_inner,
        ).contiguous()
        A_log = torch.log(A)  # Keep A_log in fp32
        self.A_log = nn.Parameter(A_log)
        self.A_log._no_weight_decay = True

        # D "skip" parameter
        self.D = nn.Parameter(torch.ones(self.d_inner, device=device))  # Keep in fp32
        self.D._no_weight_decay = True

        self.out_proj = nn.Linear(
            self.d_inner, self.d_model, bias=bias, **factory_kwargs
        )

    def forward(self, hidden_states, inference_params=None):
        """
        hidden_states: (B, L, D)
        Returns: same shape as hidden_states
        """
        batch, seqlen, dim = hidden_states.shape

        conv_state, ssm_state = None, None
        if inference_params is not None:
            conv_state, ssm_state = self._get_states_from_cache(inference_params, batch)
            if inference_params.seqlen_offset > 0:
                # The states are updated inplace
                out, _, _ = self.step(hidden_states, conv_state, ssm_state)
                return out

        # We do matmul and transpose BLH -> HBL at the same time
        xz = rearrange(
            self.in_proj.weight @ rearrange(hidden_states, "b l d -> d (b l)"),
            "d (b l) -> b d l",
            l=seqlen,
        )
        if self.in_proj.bias is not None:
            xz = xz + rearrange(self.in_proj.bias.to(dtype=xz.dtype), "d -> d 1")

        A = -torch.exp(self.A_log.float())  # (d_inner, d_state)
        # In the backward pass we write dx and dz next to each other to avoid torch.cat
        if (
            self.use_fast_path and inference_params is None
        ):  # Doesn't support outputting the states
            out = mamba_inner_fn(
                xz,
                self.conv1d.weight,
                self.conv1d.bias,
                self.x_proj.weight,
                self.dt_proj.weight,
                self.out_proj.weight,
                self.out_proj.bias,
                A,
                None,  # input-dependent B
                None,  # input-dependent C
                self.D.float(),
                delta_bias=self.dt_proj.bias.float(),
                delta_softplus=True,
            )
        else:
            x, z = xz.chunk(2, dim=1)
            # Compute short convolution
            if conv_state is not None:
                # If we just take x[:, :, -self.d_conv :], it will error if seqlen < self.d_conv
                # Instead F.pad will pad with zeros if seqlen < self.d_conv, and truncate otherwise.
                conv_state.copy_(
                    F.pad(x, (self.d_conv - x.shape[-1], 0))
                )  # Update state (B D W)
            if causal_conv1d_fn is None:
                x = self.act(self.conv1d(x)[..., :seqlen])
            else:
                assert self.activation in ["silu", "swish"]
                x = causal_conv1d_fn(
                    x=x,
                    weight=rearrange(self.conv1d.weight, "d 1 w -> d w"),
                    bias=self.conv1d.bias,
                    activation=self.activation,
                )

            # We're careful here about the layout, to avoid extra transposes.
            # We want dt to have d as the slowest moving dimension
            # and L as the fastest moving dimension, since those are what the ssm_scan kernel expects.
            x_dbl = self.x_proj(rearrange(x, "b d l -> (b l) d"))  # (bl d)
            dt, B, C = torch.split(
                x_dbl, [self.dt_rank, self.d_state, self.d_state], dim=-1
            )
            dt = self.dt_proj.weight @ dt.t()
            dt = rearrange(dt, "d (b l) -> b d l", l=seqlen)
            B = rearrange(B, "(b l) dstate -> b dstate l", l=seqlen).contiguous()
            C = rearrange(C, "(b l) dstate -> b dstate l", l=seqlen).contiguous()
            assert self.activation in ["silu", "swish"]
            y = selective_scan_fn(
                x,
                dt,
                A,
                B,
                C,
                self.D.float(),
                z=z,
                delta_bias=self.dt_proj.bias.float(),
                delta_softplus=True,
                return_last_state=ssm_state is not None,
            )
            if ssm_state is not None:
                y, last_state = y
                ssm_state.copy_(last_state)
            y = rearrange(y, "b d l -> b l d")
            out = self.out_proj(y)
        return out

    def step(self, hidden_states, conv_state, ssm_state):
        dtype = hidden_states.dtype
        assert (
            hidden_states.shape[1] == 1
        ), "Only support decoding with 1 token at a time for now"
        xz = self.in_proj(hidden_states.squeeze(1))  # (B 2D)
        x, z = xz.chunk(2, dim=-1)  # (B D)

        # Conv step
        if causal_conv1d_update is None:
            conv_state.copy_(
                torch.roll(conv_state, shifts=-1, dims=-1)
            )  # Update state (B D W)
            conv_state[:, :, -1] = x
            x = torch.sum(
                conv_state * rearrange(self.conv1d.weight, "d 1 w -> d w"), dim=-1
            )  # (B D)
            if self.conv1d.bias is not None:
                x = x + self.conv1d.bias
            x = self.act(x).to(dtype=dtype)
        else:
            x = causal_conv1d_update(
                x,
                conv_state,
                rearrange(self.conv1d.weight, "d 1 w -> d w"),
                self.conv1d.bias,
                self.activation,
            )

        x_db = self.x_proj(x)  # (B dt_rank+2*d_state)
        dt, B, C = torch.split(x_db, [self.dt_rank, self.d_state, self.d_state], dim=-1)
        # Don't add dt_bias here
        dt = F.linear(dt, self.dt_proj.weight)  # (B d_inner)
        A = -torch.exp(self.A_log.float())  # (d_inner, d_state)

        # SSM step
        if selective_state_update is None:
            # Discretize A and B
            dt = F.softplus(dt + self.dt_proj.bias.to(dtype=dt.dtype))
            dA = torch.exp(torch.einsum("bd,dn->bdn", dt, A))
            dB = torch.einsum("bd,bn->bdn", dt, B)
            ssm_state.copy_(ssm_state * dA + rearrange(x, "b d -> b d 1") * dB)
            y = torch.einsum("bdn,bn->bd", ssm_state.to(dtype), C)
            y = y + self.D.to(dtype) * x
            y = y * self.act(z)  # (B D)
        else:
            y = selective_state_update(
                ssm_state,
                x,
                dt,
                A,
                B,
                C,
                self.D,
                z=z,
                dt_bias=self.dt_proj.bias,
                dt_softplus=True,
            )

        out = self.out_proj(y)
        return out.unsqueeze(1), conv_state, ssm_state

    def allocate_inference_cache(self, batch_size, max_seqlen, dtype=None, **kwargs):
        device = self.out_proj.weight.device
        conv_dtype = self.conv1d.weight.dtype if dtype is None else dtype
        conv_state = torch.zeros(
            batch_size,
            self.d_model * self.expand,
            self.d_conv,
            device=device,
            dtype=conv_dtype,
        )
        ssm_dtype = self.dt_proj.weight.dtype if dtype is None else dtype
        # ssm_dtype = torch.float32
        ssm_state = torch.zeros(
            batch_size,
            self.d_model * self.expand,
            self.d_state,
            device=device,
            dtype=ssm_dtype,
        )
        return conv_state, ssm_state

    def _get_states_from_cache(
        self, inference_params, batch_size, initialize_states=False
    ):
        assert self.layer_idx is not None
        if self.layer_idx not in inference_params.key_value_memory_dict:
            batch_shape = (batch_size,)
            conv_state = torch.zeros(
                batch_size,
                self.d_model * self.expand,
                self.d_conv,
                device=self.conv1d.weight.device,
                dtype=self.conv1d.weight.dtype,
            )
            ssm_state = torch.zeros(
                batch_size,
                self.d_model * self.expand,
                self.d_state,
                device=self.dt_proj.weight.device,
                dtype=self.dt_proj.weight.dtype,
                # dtype=torch.float32,
            )
            inference_params.key_value_memory_dict[self.layer_idx] = (
                conv_state,
                ssm_state,
            )
        else:
            conv_state, ssm_state = inference_params.key_value_memory_dict[
                self.layer_idx
            ]
            # TODO: What if batch size changes between generation, and we reuse the same states?
            if initialize_states:
                conv_state.zero_()
                ssm_state.zero_()
        return conv_state, ssm_state


class Block(nn.Module):
    def __init__(
        self,
        dim,
        mixer_cls,
        norm_cls=nn.LayerNorm,
        fused_add_norm=False,
        residual_in_fp32=False,
    ):
        """
        Simple block wrapping a mixer class with LayerNorm/RMSNorm and residual connection"

        This Block has a slightly different structure compared to a regular
        prenorm Transformer block.
        The standard block is: LN -> MHA/MLP -> Add.
        [Ref: https://arxiv.org/abs/2002.04745]
        Here we have: Add -> LN -> Mixer, returning both
        the hidden_states (output of the mixer) and the residual.
        This is purely for performance reasons, as we can fuse add and LayerNorm.
        The residual needs to be provided (except for the very first block).
        """
        super().__init__()
        self.residual_in_fp32 = residual_in_fp32
        self.fused_add_norm = fused_add_norm
        self.mixer = mixer_cls(dim)
        self.norm = norm_cls(dim)
        if self.fused_add_norm:
            assert RMSNorm is not None, "RMSNorm import fails"
            assert isinstance(
                self.norm, (nn.LayerNorm, RMSNorm)
            ), "Only LayerNorm and RMSNorm are supported for fused_add_norm"

    def forward(
        self,
        hidden_states: Tensor,
        residual: Optional[Tensor] = None,
        inference_params=None,
    ):
        r"""Pass the input through the encoder layer.

        Args:
            hidden_states: the sequence to the encoder layer (required).
            residual: hidden_states = Mixer(LN(residual))
        """
        if not self.fused_add_norm:
            residual = (
                (hidden_states + residual) if residual is not None else hidden_states
            )
            hidden_states = self.norm(residual.to(dtype=self.norm.weight.dtype))
            if self.residual_in_fp32:
                residual = residual.to(torch.float32)
        else:
            fused_add_norm_fn = (
                rms_norm_fn if isinstance(self.norm, RMSNorm) else layer_norm_fn
            )
            hidden_states, residual = fused_add_norm_fn(
                hidden_states,
                self.norm.weight,
                self.norm.bias,
                residual=residual,
                prenorm=True,
                residual_in_fp32=self.residual_in_fp32,
                eps=self.norm.eps,
            )
        hidden_states = self.mixer(hidden_states, inference_params=inference_params)
        return hidden_states, residual

    def allocate_inference_cache(self, batch_size, max_seqlen, dtype=None, **kwargs):
        return self.mixer.allocate_inference_cache(
            batch_size, max_seqlen, dtype=dtype, **kwargs
        )



================================================
File: sauron/mil_models/mamba_ssm/modules/srmamba.py
================================================
# Copyright (c) 2023, Tri Dao, Albert Gu.

import math
from typing import Optional

import torch
import torch.nn as nn
import torch.nn.functional as F
from einops import rearrange, repeat
from torch import Tensor

from mamba.mamba_ssm.ops.selective_scan_interface import (
    mamba_inner_fn_no_out_proj,
    selective_scan_fn,
)

try:
    from causal_conv1d import causal_conv1d_fn, causal_conv1d_update
except ImportError:
    causal_conv1d_fn, causal_conv1d_update = None

try:
    from mamba_ssm.ops.triton.selective_state_update import selective_state_update
except ImportError:
    selective_state_update = None

try:
    from mamba_ssm.ops.triton.layernorm import RMSNorm, layer_norm_fn, rms_norm_fn
except ImportError:
    RMSNorm, layer_norm_fn, rms_norm_fn = None, None, None


class TransposeTokenReEmbedding:
    @staticmethod
    def transpose_normal_padding(x, rate):
        x = rearrange(x, "b c l -> b l c")
        B, N, C = x.shape
        value = N // rate
        if N % rate != 0:
            padding_length = (value + 1) * rate - N
            padded_x = torch.nn.functional.pad(x, (0, 0, 0, padding_length))
        else:
            padded_x = x
        x_ = rearrange(padded_x, "b (k w) d -> b (w k) d", w=rate)
        x_ = rearrange(x_, "b l c -> b c l")
        return x_

    @staticmethod
    def transpose_remove_padding(x, rate, length):
        x = rearrange(x, "b c l -> b l c")
        x = rearrange(x, "b (w k) d -> b (k w) d", w=rate)
        x = x[:, :length, :]
        x = rearrange(x, "b l c -> b c l")
        return x


class SRMamba(nn.Module):
    def __init__(
        self,
        d_model,
        d_state=16,
        d_conv=4,
        expand=2,
        dt_rank="auto",
        dt_min=0.001,
        dt_max=0.1,
        dt_init="random",
        dt_scale=1.0,
        dt_init_floor=1e-4,
        conv_bias=True,
        bias=False,
        use_fast_path=True,  # Fused kernel options
        layer_idx=None,
        device=None,
        dtype=None,
    ):
        factory_kwargs = {"device": device, "dtype": dtype}
        super().__init__()
        self.d_model = d_model
        self.d_state = d_state
        self.d_conv = d_conv
        self.expand = expand
        self.d_inner = int(self.expand * self.d_model)
        self.dt_rank = math.ceil(self.d_model / 16) if dt_rank == "auto" else dt_rank
        self.use_fast_path = use_fast_path
        self.layer_idx = layer_idx

        self.in_proj = nn.Linear(
            self.d_model, self.d_inner * 2, bias=bias, **factory_kwargs
        )

        self.conv1d = nn.Conv1d(
            in_channels=self.d_inner,
            out_channels=self.d_inner,
            bias=conv_bias,
            kernel_size=d_conv,
            groups=self.d_inner,
            padding=d_conv - 1,
            **factory_kwargs,
        )
        self.conv1d_b = nn.Conv1d(
            in_channels=self.d_inner,
            out_channels=self.d_inner,
            bias=conv_bias,
            kernel_size=d_conv,
            groups=self.d_inner,
            padding=d_conv - 1,
            **factory_kwargs,
        )

        self.activation = "silu"
        self.act = nn.SiLU()

        self.x_proj = nn.Linear(
            self.d_inner, self.dt_rank + self.d_state * 2, bias=False, **factory_kwargs
        )
        self.x_proj_b = nn.Linear(
            self.d_inner, self.dt_rank + self.d_state * 2, bias=False, **factory_kwargs
        )

        self.dt_proj = nn.Linear(
            self.dt_rank, self.d_inner, bias=True, **factory_kwargs
        )
        self.dt_proj_b = nn.Linear(
            self.dt_rank, self.d_inner, bias=True, **factory_kwargs
        )

        # Initialize special dt projection to preserve variance at initialization
        dt_init_std = self.dt_rank**-0.5 * dt_scale
        if dt_init == "constant":
            nn.init.constant_(self.dt_proj.weight, dt_init_std)
            nn.init.constant_(self.dt_proj_b.weight, dt_init_std)  # ?
        elif dt_init == "random":
            nn.init.uniform_(self.dt_proj.weight, -dt_init_std, dt_init_std)
            nn.init.uniform_(self.dt_proj_b.weight, -dt_init_std, dt_init_std)  # ?
        else:
            raise NotImplementedError

        # Initialize dt bias so that F.softplus(dt_bias) is between dt_min and dt_max
        dt = torch.exp(
            torch.rand(self.d_inner, **factory_kwargs)
            * (math.log(dt_max) - math.log(dt_min))
            + math.log(dt_min)
        ).clamp(min=dt_init_floor)
        # Inverse of softplus: https://github.com/pytorch/pytorch/issues/72759
        inv_dt = dt + torch.log(-torch.expm1(-dt))
        with torch.no_grad():
            self.dt_proj.bias.copy_(inv_dt)
            self.dt_proj_b.bias.copy_(inv_dt)  # ?
        # Our initialization would set all Linear.bias to zero, need to mark this one as _no_reinit
        self.dt_proj.bias._no_reinit = True
        self.dt_proj_b.bias._no_reinit = True  # ?

        # S4D real initialization
        A = repeat(
            torch.arange(1, self.d_state + 1, dtype=torch.float32, device=device),
            "n -> d n",
            d=self.d_inner,
        ).contiguous()
        A_log = torch.log(A)  # Keep A_log in fp32
        self.A_log = nn.Parameter(A_log)
        self.A_log._no_weight_decay = True

        A_b = repeat(
            torch.arange(1, self.d_state + 1, dtype=torch.float32, device=device),
            "n -> d n",
            d=self.d_inner,
        ).contiguous()
        A_b_log = torch.log(A_b)  # Keep A_b_log in fp32
        self.A_b_log = nn.Parameter(A_b_log)
        self.A_b_log._no_weight_decay = True

        # D "skip" parameter
        self.D = nn.Parameter(torch.ones(self.d_inner, device=device))  # Keep in fp32
        self.D._no_weight_decay = True
        self.D_b = nn.Parameter(torch.ones(self.d_inner, device=device))  # Keep in fp32
        self.D_b._no_weight_decay = True

        self.out_proj = nn.Linear(
            self.d_inner, self.d_model, bias=bias, **factory_kwargs
        )

    def forward(self, hidden_states, inference_params=None, rate=10):
        """
        hidden_states: (B, L, D)
        Returns: same shape as hidden_states
        """
        batch, seqlen, dim = hidden_states.shape

        conv_state, ssm_state = None, None
        if inference_params is not None:
            conv_state, ssm_state = self._get_states_from_cache(inference_params, batch)
            if inference_params.seqlen_offset > 0:
                # The states are updated inplace
                out, _, _ = self.step(hidden_states, conv_state, ssm_state)
                return out

        # We do matmul and transpose BLH -> HBL at the same time
        xz = rearrange(
            self.in_proj.weight @ rearrange(hidden_states, "b l d -> d (b l)"),
            "d (b l) -> b d l",
            l=seqlen,
        )
        if self.in_proj.bias is not None:
            xz = xz + rearrange(self.in_proj.bias.to(dtype=xz.dtype), "d -> d 1")

        A = -torch.exp(self.A_log.float())  # (d_inner, d_state)
        A_b = -torch.exp(self.A_b_log.float())
        # In the backward pass we write dx and dz next to each other to avoid torch.cat
        if (
            self.use_fast_path and inference_params is None
        ):  # Doesn't support outputting the states
            out = mamba_inner_fn_no_out_proj(
                xz,
                self.conv1d.weight,
                self.conv1d.bias,
                self.x_proj.weight,
                self.dt_proj.weight,
                A,
                None,  # input-dependent B
                None,  # input-dependent C
                self.D.float(),
                delta_bias=self.dt_proj.bias.float(),
                delta_softplus=True,
            )
            # 直接末尾补零并转置: TMamba
            N, C, L = xz.shape
            xz_b = TransposeTokenReEmbedding.transpose_normal_padding(xz, rate=rate)
            out_b = mamba_inner_fn_no_out_proj(
                xz_b,
                self.conv1d_b.weight,
                self.conv1d_b.bias,
                self.x_proj_b.weight,
                self.dt_proj_b.weight,
                A_b,
                None,
                None,
                self.D_b.float(),
                delta_bias=self.dt_proj_b.bias.float(),
                delta_softplus=True,
            )
            # 将序列转置，并删除padding的位置
            out_b = TransposeTokenReEmbedding.transpose_remove_padding(
                out_b, rate=rate, length=L
            )
            out = F.linear(
                rearrange(out + out_b, "b d l -> b l d"),
                self.out_proj.weight,
                self.out_proj.bias,
            )
        else:
            # x, z拆分用于两个分支的聚合
            x, z = xz.chunk(2, dim=1)
            x_b = x.flip([-1])
            # Compute short convolution
            if conv_state is not None:
                # If we just take x[:, :, -self.d_conv :], it will error if seqlen < self.d_conv
                # Instead F.pad will pad with zeros if seqlen < self.d_conv, and truncate otherwise.
                conv_state.copy_(
                    F.pad(x, (self.d_conv - x.shape[-1], 0))
                )  # Update state (B D W)
            # 为无causal_conv1d_fn提供了手动的处理方案
            if causal_conv1d_fn is None:
                x = self.act(self.conv1d(x)[..., :seqlen])
                x = self.act(self.conv1d_b(x_b)[..., :seqlen])
            else:
                assert self.activation in ["silu", "swish"]
                x = causal_conv1d_fn(
                    x=x,
                    weight=rearrange(self.conv1d.weight, "d 1 w -> d w"),
                    bias=self.conv1d.bias,
                    activation=self.activation,
                )
                x_b = causal_conv1d_fn(
                    x=x_b,
                    weight=rearrange(self.conv1d_b.weight, "d 1 w -> d w"),
                    bias=self.conv1d_b.bias,
                    activation=self.activation,
                )

            # We're careful here about the layout, to avoid extra transposes.
            # We want dt to have d as the slowest moving dimension
            # and L as the fastest moving dimension, since those are what the ssm_scan kernel expects.
            # SSM内核操作，分别为前向和反向生成对应的B和C
            x_dbl = self.x_proj(rearrange(x, "b d l -> (b l) d"))  # (bl d)
            dt, B, C = torch.split(
                x_dbl, [self.dt_rank, self.d_state, self.d_state], dim=-1
            )
            dt = self.dt_proj.weight @ dt.t()
            dt = rearrange(dt, "d (b l) -> b d l", l=seqlen)
            B = rearrange(B, "(b l) dstate -> b dstate l", l=seqlen).contiguous()
            C = rearrange(C, "(b l) dstate -> b dstate l", l=seqlen).contiguous()

            x_dbl_b = self.x_proj_b(rearrange(x_b, "b d l -> (b l) d"))  # (bl d)
            dt_b, B_b, C_b = torch.split(
                x_dbl_b, [self.dt_rank, self.d_state, self.d_state], dim=-1
            )
            dt_b = self.dt_proj_b.weight @ dt_b.t()
            dt_b = rearrange(dt_b, "d (b l) -> b d l", l=seqlen)
            B_b = rearrange(B_b, "(b l) dstate -> b dstate l", l=seqlen).contiguous()
            C_b = rearrange(C_b, "(b l) dstate -> b dstate l", l=seqlen).contiguous()
            assert self.activation in ["silu", "swish"]
            y = selective_scan_fn(
                x,
                dt,
                A,
                B,
                C,
                self.D.float(),
                z=z,
                delta_bias=self.dt_proj.bias.float(),
                delta_softplus=True,
                return_last_state=ssm_state is not None,
            )
            y_b = selective_scan_fn(
                x_b,
                dt_b,
                A_b,
                B_b,
                C_b,
                self.D_b.float(),
                z=z,
                delta_bias=self.dt_proj_b.bias.float(),
                delta_softplus=True,
                return_last_state=ssm_state is not None,
            )
            if ssm_state is not None:
                y, last_state = y
                ssm_state.copy_(last_state)
            y = rearrange(y, "b d l -> b l d")
            y_b = rearrange(y_b, "b d l -> b l d")
            out = self.out_proj(y_b + y)
        return out

    def step(self, hidden_states, conv_state, ssm_state):
        dtype = hidden_states.dtype
        assert (
            hidden_states.shape[1] == 1
        ), "Only support decoding with 1 token at a time for now"
        xz = self.in_proj(hidden_states.squeeze(1))  # (B 2D)
        x, z = xz.chunk(2, dim=-1)  # (B D)

        # Conv step
        if causal_conv1d_update is None:
            conv_state.copy_(
                torch.roll(conv_state, shifts=-1, dims=-1)
            )  # Update state (B D W)
            conv_state[:, :, -1] = x
            x = torch.sum(
                conv_state * rearrange(self.conv1d.weight, "d 1 w -> d w"), dim=-1
            )  # (B D)
            if self.conv1d.bias is not None:
                x = x + self.conv1d.bias
            x = self.act(x).to(dtype=dtype)
        else:
            x = causal_conv1d_update(
                x,
                conv_state,
                rearrange(self.conv1d.weight, "d 1 w -> d w"),
                self.conv1d.bias,
                self.activation,
            )

        x_db = self.x_proj(x)  # (B dt_rank+2*d_state)
        dt, B, C = torch.split(x_db, [self.dt_rank, self.d_state, self.d_state], dim=-1)
        # Don't add dt_bias here
        dt = F.linear(dt, self.dt_proj.weight)  # (B d_inner)
        A = -torch.exp(self.A_log.float())  # (d_inner, d_state)

        # SSM step
        if selective_state_update is None:
            # Discretize A and B
            dt = F.softplus(dt + self.dt_proj.bias.to(dtype=dt.dtype))
            dA = torch.exp(torch.einsum("bd,dn->bdn", dt, A))
            dB = torch.einsum("bd,bn->bdn", dt, B)
            ssm_state.copy_(ssm_state * dA + rearrange(x, "b d -> b d 1") * dB)
            y = torch.einsum("bdn,bn->bd", ssm_state.to(dtype), C)
            y = y + self.D.to(dtype) * x
            y = y * self.act(z)  # (B D)
        else:
            y = selective_state_update(
                ssm_state,
                x,
                dt,
                A,
                B,
                C,
                self.D,
                z=z,
                dt_bias=self.dt_proj.bias,
                dt_softplus=True,
            )

        out = self.out_proj(y)
        return out.unsqueeze(1), conv_state, ssm_state

    def allocate_inference_cache(self, batch_size, max_seqlen, dtype=None, **kwargs):
        device = self.out_proj.weight.device
        conv_dtype = self.conv1d.weight.dtype if dtype is None else dtype
        conv_state = torch.zeros(
            batch_size,
            self.d_model * self.expand,
            self.d_conv,
            device=device,
            dtype=conv_dtype,
        )
        ssm_dtype = self.dt_proj.weight.dtype if dtype is None else dtype
        # ssm_dtype = torch.float32
        ssm_state = torch.zeros(
            batch_size,
            self.d_model * self.expand,
            self.d_state,
            device=device,
            dtype=ssm_dtype,
        )
        return conv_state, ssm_state

    def _get_states_from_cache(
        self, inference_params, batch_size, initialize_states=False
    ):
        assert self.layer_idx is not None
        if self.layer_idx not in inference_params.key_value_memory_dict:
            batch_shape = (batch_size,)
            conv_state = torch.zeros(
                batch_size,
                self.d_model * self.expand,
                self.d_conv,
                device=self.conv1d.weight.device,
                dtype=self.conv1d.weight.dtype,
            )
            ssm_state = torch.zeros(
                batch_size,
                self.d_model * self.expand,
                self.d_state,
                device=self.dt_proj.weight.device,
                dtype=self.dt_proj.weight.dtype,
                # dtype=torch.float32,
            )
            inference_params.key_value_memory_dict[self.layer_idx] = (
                conv_state,
                ssm_state,
            )
        else:
            conv_state, ssm_state = inference_params.key_value_memory_dict[
                self.layer_idx
            ]
            # TODO: What if batch size changes between generation, and we reuse the same states?
            if initialize_states:
                conv_state.zero_()
                ssm_state.zero_()
        return conv_state, ssm_state


class Block(nn.Module):
    def __init__(
        self,
        dim,
        mixer_cls,
        norm_cls=nn.LayerNorm,
        fused_add_norm=False,
        residual_in_fp32=False,
    ):
        """
        Simple block wrapping a mixer class with LayerNorm/RMSNorm and residual connection"

        This Block has a slightly different structure compared to a regular
        prenorm Transformer block.
        The standard block is: LN -> MHA/MLP -> Add.
        [Ref: https://arxiv.org/abs/2002.04745]
        Here we have: Add -> LN -> Mixer, returning both
        the hidden_states (output of the mixer) and the residual.
        This is purely for performance reasons, as we can fuse add and LayerNorm.
        The residual needs to be provided (except for the very first block).
        """
        super().__init__()
        self.residual_in_fp32 = residual_in_fp32
        self.fused_add_norm = fused_add_norm
        self.mixer = mixer_cls(dim)
        self.norm = norm_cls(dim)
        if self.fused_add_norm:
            assert RMSNorm is not None, "RMSNorm import fails"
            assert isinstance(
                self.norm, (nn.LayerNorm, RMSNorm)
            ), "Only LayerNorm and RMSNorm are supported for fused_add_norm"

    def forward(
        self,
        hidden_states: Tensor,
        residual: Optional[Tensor] = None,
        inference_params=None,
    ):
        r"""Pass the input through the encoder layer.

        Args:
            hidden_states: the sequence to the encoder layer (required).
            residual: hidden_states = Mixer(LN(residual))
        """
        if not self.fused_add_norm:
            residual = (
                (hidden_states + residual) if residual is not None else hidden_states
            )
            hidden_states = self.norm(residual.to(dtype=self.norm.weight.dtype))
            if self.residual_in_fp32:
                residual = residual.to(torch.float32)
        else:
            fused_add_norm_fn = (
                rms_norm_fn if isinstance(self.norm, RMSNorm) else layer_norm_fn
            )
            hidden_states, residual = fused_add_norm_fn(
                hidden_states,
                self.norm.weight,
                self.norm.bias,
                residual=residual,
                prenorm=True,
                residual_in_fp32=self.residual_in_fp32,
                eps=self.norm.eps,
            )
        hidden_states = self.mixer(hidden_states, inference_params=inference_params)
        return hidden_states, residual

    def allocate_inference_cache(self, batch_size, max_seqlen, dtype=None, **kwargs):
        return self.mixer.allocate_inference_cache(
            batch_size, max_seqlen, dtype=dtype, **kwargs
        )



================================================
File: sauron/mil_models/mamba_ssm/ops/__init__.py
================================================



================================================
File: sauron/mil_models/mamba_ssm/ops/selective_scan_interface.py
================================================
# Copyright (c) 2023, Tri Dao, Albert Gu.

import causal_conv1d_cuda
import selective_scan_cuda
import torch
import torch.nn.functional as F
from causal_conv1d import causal_conv1d_fn
from einops import rearrange, repeat
from torch.cuda.amp import custom_bwd, custom_fwd


class SelectiveScanFn(torch.autograd.Function):
    @staticmethod
    def forward(
        ctx,
        u,
        delta,
        A,
        B,
        C,
        D=None,
        z=None,
        delta_bias=None,
        delta_softplus=False,
        return_last_state=False,
    ):
        if u.stride(-1) != 1:
            u = u.contiguous()
        if delta.stride(-1) != 1:
            delta = delta.contiguous()
        if D is not None:
            D = D.contiguous()
        if B.stride(-1) != 1:
            B = B.contiguous()
        if C.stride(-1) != 1:
            C = C.contiguous()
        if z is not None and z.stride(-1) != 1:
            z = z.contiguous()
        if B.dim() == 3:
            B = rearrange(B, "b dstate l -> b 1 dstate l")
            ctx.squeeze_B = True
        if C.dim() == 3:
            C = rearrange(C, "b dstate l -> b 1 dstate l")
            ctx.squeeze_C = True
        out, x, *rest = selective_scan_cuda.fwd(
            u, delta, A, B, C, D, z, delta_bias, delta_softplus
        )
        ctx.delta_softplus = delta_softplus
        ctx.has_z = z is not None
        last_state = x[:, :, -1, 1::2]  # (batch, dim, dstate)
        if not ctx.has_z:
            ctx.save_for_backward(u, delta, A, B, C, D, delta_bias, x)
            return out if not return_last_state else (out, last_state)
        else:
            ctx.save_for_backward(u, delta, A, B, C, D, z, delta_bias, x, out)
            out_z = rest[0]
            return out_z if not return_last_state else (out_z, last_state)

    @staticmethod
    def backward(ctx, dout, *args):
        if not ctx.has_z:
            u, delta, A, B, C, D, delta_bias, x = ctx.saved_tensors
            z = None
            out = None
        else:
            u, delta, A, B, C, D, z, delta_bias, x, out = ctx.saved_tensors
        if dout.stride(-1) != 1:
            dout = dout.contiguous()
        # The kernel supports passing in a pre-allocated dz (e.g., in case we want to fuse the
        # backward of selective_scan_cuda with the backward of chunk).
        # Here we just pass in None and dz will be allocated in the C++ code.
        du, ddelta, dA, dB, dC, dD, ddelta_bias, *rest = selective_scan_cuda.bwd(
            u,
            delta,
            A,
            B,
            C,
            D,
            z,
            delta_bias,
            dout,
            x,
            out,
            None,
            ctx.delta_softplus,
            False,  # option to recompute out_z, not used here
        )
        dz = rest[0] if ctx.has_z else None
        dB = dB.squeeze(1) if getattr(ctx, "squeeze_B", False) else dB
        dC = dC.squeeze(1) if getattr(ctx, "squeeze_C", False) else dC
        return (
            du,
            ddelta,
            dA,
            dB,
            dC,
            dD if D is not None else None,
            dz,
            ddelta_bias if delta_bias is not None else None,
            None,
            None,
        )


def selective_scan_fn(
    u,
    delta,
    A,
    B,
    C,
    D=None,
    z=None,
    delta_bias=None,
    delta_softplus=False,
    return_last_state=False,
):
    """if return_last_state is True, returns (out, last_state)
    last_state has shape (batch, dim, dstate). Note that the gradient of the last state is
    not considered in the backward pass.
    """
    return SelectiveScanFn.apply(
        u, delta, A, B, C, D, z, delta_bias, delta_softplus, return_last_state
    )


def selective_scan_ref(
    u,
    delta,
    A,
    B,
    C,
    D=None,
    z=None,
    delta_bias=None,
    delta_softplus=False,
    return_last_state=False,
):
    """
    u: r(B D L)
    delta: r(B D L)
    A: c(D N) or r(D N)
    B: c(D N) or r(B N L) or r(B N 2L) or r(B G N L) or (B G N L)
    C: c(D N) or r(B N L) or r(B N 2L) or r(B G N L) or (B G N L)
    D: r(D)
    z: r(B D L)
    delta_bias: r(D), fp32

    out: r(B D L)
    last_state (optional): r(B D dstate) or c(B D dstate)
    """
    dtype_in = u.dtype
    u = u.float()
    delta = delta.float()
    if delta_bias is not None:
        delta = delta + delta_bias[..., None].float()
    if delta_softplus:
        delta = F.softplus(delta)
    batch, dim, dstate = u.shape[0], A.shape[0], A.shape[1]
    is_variable_B = B.dim() >= 3
    is_variable_C = C.dim() >= 3
    if A.is_complex():
        if is_variable_B:
            B = torch.view_as_complex(
                rearrange(B.float(), "... (L two) -> ... L two", two=2)
            )
        if is_variable_C:
            C = torch.view_as_complex(
                rearrange(C.float(), "... (L two) -> ... L two", two=2)
            )
    else:
        B = B.float()
        C = C.float()
    x = A.new_zeros((batch, dim, dstate))
    ys = []
    deltaA = torch.exp(torch.einsum("bdl,dn->bdln", delta, A))
    if not is_variable_B:
        deltaB_u = torch.einsum("bdl,dn,bdl->bdln", delta, B, u)
    else:
        if B.dim() == 3:
            deltaB_u = torch.einsum("bdl,bnl,bdl->bdln", delta, B, u)
        else:
            B = repeat(B, "B G N L -> B (G H) N L", H=dim // B.shape[1])
            deltaB_u = torch.einsum("bdl,bdnl,bdl->bdln", delta, B, u)
    if is_variable_C and C.dim() == 4:
        C = repeat(C, "B G N L -> B (G H) N L", H=dim // C.shape[1])
    last_state = None
    for i in range(u.shape[2]):
        x = deltaA[:, :, i] * x + deltaB_u[:, :, i]
        if not is_variable_C:
            y = torch.einsum("bdn,dn->bd", x, C)
        else:
            if C.dim() == 3:
                y = torch.einsum("bdn,bn->bd", x, C[:, :, i])
            else:
                y = torch.einsum("bdn,bdn->bd", x, C[:, :, :, i])
        if i == u.shape[2] - 1:
            last_state = x
        if y.is_complex():
            y = y.real * 2
        ys.append(y)
    y = torch.stack(ys, dim=2)  # (batch dim L)
    out = y if D is None else y + u * rearrange(D, "d -> d 1")
    if z is not None:
        out = out * F.silu(z)
    out = out.to(dtype=dtype_in)
    return out if not return_last_state else (out, last_state)


class MambaInnerFn(torch.autograd.Function):
    @staticmethod
    @custom_fwd
    def forward(
        ctx,
        xz,
        conv1d_weight,
        conv1d_bias,
        x_proj_weight,
        delta_proj_weight,
        out_proj_weight,
        out_proj_bias,
        A,
        B=None,
        C=None,
        D=None,
        delta_bias=None,
        B_proj_bias=None,
        C_proj_bias=None,
        delta_softplus=True,
        checkpoint_lvl=1,
    ):
        """
        xz: (batch, dim, seqlen)
        """
        assert checkpoint_lvl in [0, 1]
        L = xz.shape[-1]
        delta_rank = delta_proj_weight.shape[1]
        d_state = A.shape[-1] * (1 if not A.is_complex() else 2)
        if torch.is_autocast_enabled():
            x_proj_weight = x_proj_weight.to(dtype=torch.get_autocast_gpu_dtype())
            delta_proj_weight = delta_proj_weight.to(
                dtype=torch.get_autocast_gpu_dtype()
            )
            out_proj_weight = out_proj_weight.to(dtype=torch.get_autocast_gpu_dtype())
            out_proj_bias = (
                out_proj_bias.to(dtype=torch.get_autocast_gpu_dtype())
                if out_proj_bias is not None
                else None
            )
        if xz.stride(-1) != 1:
            xz = xz.contiguous()
        conv1d_weight = rearrange(conv1d_weight, "d 1 w -> d w")
        x, z = xz.chunk(2, dim=1)
        conv1d_bias = conv1d_bias.contiguous() if conv1d_bias is not None else None
        conv1d_out = causal_conv1d_cuda.causal_conv1d_fwd(
            x, conv1d_weight, conv1d_bias, None, True
        )  # ？
        # We're being very careful here about the layout, to avoid extra transposes.
        # We want delta to have d as the slowest moving dimension
        # and L as the fastest moving dimension, since those are what the ssm_scan kernel expects.
        x_dbl = F.linear(
            rearrange(conv1d_out, "b d l -> (b l) d"), x_proj_weight
        )  # (bl d)
        delta = rearrange(
            delta_proj_weight @ x_dbl[:, :delta_rank].t(), "d (b l) -> b d l", l=L
        )
        ctx.is_variable_B = B is None
        ctx.is_variable_C = C is None
        ctx.B_proj_bias_is_None = B_proj_bias is None
        ctx.C_proj_bias_is_None = C_proj_bias is None
        if B is None:  # variable B
            B = x_dbl[:, delta_rank : delta_rank + d_state]  # (bl dstate)
            if B_proj_bias is not None:
                B = B + B_proj_bias.to(dtype=B.dtype)
            if not A.is_complex():
                # B = rearrange(B, "(b l) dstate -> b dstate l", l=L).contiguous()
                B = rearrange(B, "(b l) dstate -> b 1 dstate l", l=L).contiguous()
            else:
                B = rearrange(
                    B, "(b l) (dstate two) -> b 1 dstate (l two)", l=L, two=2
                ).contiguous()
        else:
            if B.stride(-1) != 1:
                B = B.contiguous()
        if C is None:  # variable C
            C = x_dbl[:, -d_state:]  # (bl dstate)
            if C_proj_bias is not None:
                C = C + C_proj_bias.to(dtype=C.dtype)
            if not A.is_complex():
                # C = rearrange(C, "(b l) dstate -> b dstate l", l=L).contiguous()
                C = rearrange(C, "(b l) dstate -> b 1 dstate l", l=L).contiguous()
            else:
                C = rearrange(
                    C, "(b l) (dstate two) -> b 1 dstate (l two)", l=L, two=2
                ).contiguous()
        else:
            if C.stride(-1) != 1:
                C = C.contiguous()
        if D is not None:
            D = D.contiguous()
        out, scan_intermediates, out_z = selective_scan_cuda.fwd(
            conv1d_out, delta, A, B, C, D, z, delta_bias, delta_softplus
        )
        ctx.delta_softplus = delta_softplus
        ctx.out_proj_bias_is_None = out_proj_bias is None
        ctx.checkpoint_lvl = checkpoint_lvl
        if (
            checkpoint_lvl >= 1
        ):  # Will recompute conv1d_out and delta in the backward pass
            conv1d_out, delta = None, None
        ctx.save_for_backward(
            xz,
            conv1d_weight,
            conv1d_bias,
            x_dbl,
            x_proj_weight,
            delta_proj_weight,
            out_proj_weight,
            conv1d_out,
            delta,
            A,
            B,
            C,
            D,
            delta_bias,
            scan_intermediates,
            out,
        )
        return F.linear(
            rearrange(out_z, "b d l -> b l d"), out_proj_weight, out_proj_bias
        )

    @staticmethod
    @custom_bwd
    def backward(ctx, dout):
        # dout: (batch, seqlen, dim)
        (
            xz,
            conv1d_weight,
            conv1d_bias,
            x_dbl,
            x_proj_weight,
            delta_proj_weight,
            out_proj_weight,
            conv1d_out,
            delta,
            A,
            B,
            C,
            D,
            delta_bias,
            scan_intermediates,
            out,
        ) = ctx.saved_tensors
        L = xz.shape[-1]
        delta_rank = delta_proj_weight.shape[1]
        d_state = A.shape[-1] * (1 if not A.is_complex() else 2)
        x, z = xz.chunk(2, dim=1)
        if dout.stride(-1) != 1:
            dout = dout.contiguous()
        if ctx.checkpoint_lvl == 1:
            conv1d_out = causal_conv1d_cuda.causal_conv1d_fwd(
                x, conv1d_weight, conv1d_bias, None, True
            )
            delta = rearrange(
                delta_proj_weight @ x_dbl[:, :delta_rank].t(), "d (b l) -> b d l", l=L
            )
        # The kernel supports passing in a pre-allocated dz (e.g., in case we want to fuse the
        # backward of selective_scan_cuda with the backward of chunk).
        dxz = torch.empty_like(xz)  # (batch, dim, seqlen)
        dx, dz = dxz.chunk(2, dim=1)
        dout = rearrange(dout, "b l e -> e (b l)")
        dout_y = rearrange(out_proj_weight.t() @ dout, "d (b l) -> b d l", l=L)
        dconv1d_out, ddelta, dA, dB, dC, dD, ddelta_bias, dz, out_z = (
            selective_scan_cuda.bwd(
                conv1d_out,
                delta,
                A,
                B,
                C,
                D,
                z,
                delta_bias,
                dout_y,
                scan_intermediates,
                out,
                dz,
                ctx.delta_softplus,
                True,  # option to recompute out_z
            )
        )
        dout_proj_weight = torch.einsum(
            "eB,dB->ed", dout, rearrange(out_z, "b d l -> d (b l)")
        )
        dout_proj_bias = dout.sum(dim=(0, 1)) if not ctx.out_proj_bias_is_None else None
        dD = dD if D is not None else None
        dx_dbl = torch.empty_like(x_dbl)
        dB_proj_bias = None
        if ctx.is_variable_B:
            if not A.is_complex():
                dB = rearrange(dB, "b 1 dstate l -> (b l) dstate").contiguous()
            else:
                dB = rearrange(
                    dB, "b 1 dstate (l two) -> (b l) (dstate two)", two=2
                ).contiguous()
            dB_proj_bias = dB.sum(0) if not ctx.B_proj_bias_is_None else None
            dx_dbl[:, delta_rank : delta_rank + d_state] = dB  # (bl d)
            dB = None
        dC_proj_bias = None
        if ctx.is_variable_C:
            if not A.is_complex():
                dC = rearrange(dC, "b 1 dstate l -> (b l) dstate").contiguous()
            else:
                dC = rearrange(
                    dC, "b 1 dstate (l two) -> (b l) (dstate two)", two=2
                ).contiguous()
            dC_proj_bias = dC.sum(0) if not ctx.C_proj_bias_is_None else None
            dx_dbl[:, -d_state:] = dC  # (bl d)
            dC = None
        ddelta = rearrange(ddelta, "b d l -> d (b l)")
        ddelta_proj_weight = torch.einsum("dB,Br->dr", ddelta, x_dbl[:, :delta_rank])
        dx_dbl[:, :delta_rank] = torch.einsum("dB,dr->Br", ddelta, delta_proj_weight)
        dconv1d_out = rearrange(dconv1d_out, "b d l -> d (b l)")
        dx_proj_weight = torch.einsum(
            "Br,Bd->rd", dx_dbl, rearrange(conv1d_out, "b d l -> (b l) d")
        )
        dconv1d_out = torch.addmm(
            dconv1d_out, x_proj_weight.t(), dx_dbl.t(), out=dconv1d_out
        )
        dconv1d_out = rearrange(
            dconv1d_out, "d (b l) -> b d l", b=x.shape[0], l=x.shape[-1]
        )
        # The kernel supports passing in a pre-allocated dx (e.g., in case we want to fuse the
        # backward of conv1d with the backward of chunk).
        dx, dconv1d_weight, dconv1d_bias = causal_conv1d_cuda.causal_conv1d_bwd(
            x, conv1d_weight, conv1d_bias, dconv1d_out, None, dx, True
        )
        dconv1d_bias = dconv1d_bias if conv1d_bias is not None else None
        dconv1d_weight = rearrange(dconv1d_weight, "d w -> d 1 w")
        return (
            dxz,
            dconv1d_weight,
            dconv1d_bias,
            dx_proj_weight,
            ddelta_proj_weight,
            dout_proj_weight,
            dout_proj_bias,
            dA,
            dB,
            dC,
            dD,
            ddelta_bias if delta_bias is not None else None,
            dB_proj_bias,
            dC_proj_bias,
            None,
        )


class MambaInnerFnNoOutProj(torch.autograd.Function):
    @staticmethod
    @custom_fwd
    def forward(
        ctx,
        xz,
        conv1d_weight,
        conv1d_bias,
        x_proj_weight,
        delta_proj_weight,
        A,
        B=None,
        C=None,
        D=None,
        delta_bias=None,
        B_proj_bias=None,
        C_proj_bias=None,
        delta_softplus=True,
        checkpoint_lvl=1,
    ):
        """
        xz: (batch, dim, seqlen)
        """
        assert checkpoint_lvl in [0, 1]
        L = xz.shape[-1]
        delta_rank = delta_proj_weight.shape[1]
        d_state = A.shape[-1] * (1 if not A.is_complex() else 2)
        if torch.is_autocast_enabled():
            x_proj_weight = x_proj_weight.to(dtype=torch.get_autocast_gpu_dtype())
            delta_proj_weight = delta_proj_weight.to(
                dtype=torch.get_autocast_gpu_dtype()
            )
        if xz.stride(-1) != 1:
            xz = xz.contiguous()
        conv1d_weight = rearrange(conv1d_weight, "d 1 w -> d w")
        x, z = xz.chunk(2, dim=1)
        conv1d_bias = conv1d_bias.contiguous() if conv1d_bias is not None else None
        conv1d_out = causal_conv1d_cuda.causal_conv1d_fwd(
            x, conv1d_weight, conv1d_bias, None, True
        )
        # We're being very careful here about the layout, to avoid extra transposes.
        # We want delta to have d as the slowest moving dimension
        # and L as the fastest moving dimension, since those are what the ssm_scan kernel expects.
        x_dbl = F.linear(
            rearrange(conv1d_out, "b d l -> (b l) d"), x_proj_weight
        )  # (bl d)
        delta = rearrange(
            delta_proj_weight @ x_dbl[:, :delta_rank].t(), "d (b l) -> b d l", l=L
        )
        ctx.is_variable_B = B is None
        ctx.is_variable_C = C is None
        ctx.B_proj_bias_is_None = B_proj_bias is None
        ctx.C_proj_bias_is_None = C_proj_bias is None
        if B is None:  # variable B
            B = x_dbl[:, delta_rank : delta_rank + d_state]  # (bl dstate)
            if B_proj_bias is not None:
                B = B + B_proj_bias.to(dtype=B.dtype)
            if not A.is_complex():
                # B = rearrange(B, "(b l) dstate -> b dstate l", l=L).contiguous()
                B = rearrange(B, "(b l) dstate -> b 1 dstate l", l=L).contiguous()
            else:
                B = rearrange(
                    B, "(b l) (dstate two) -> b 1 dstate (l two)", l=L, two=2
                ).contiguous()
        else:
            if B.stride(-1) != 1:
                B = B.contiguous()
        if C is None:  # variable C
            C = x_dbl[:, -d_state:]  # (bl dstate)
            if C_proj_bias is not None:
                C = C + C_proj_bias.to(dtype=C.dtype)
            if not A.is_complex():
                # C = rearrange(C, "(b l) dstate -> b dstate l", l=L).contiguous()
                C = rearrange(C, "(b l) dstate -> b 1 dstate l", l=L).contiguous()
            else:
                C = rearrange(
                    C, "(b l) (dstate two) -> b 1 dstate (l two)", l=L, two=2
                ).contiguous()
        else:
            if C.stride(-1) != 1:
                C = C.contiguous()
        if D is not None:
            D = D.contiguous()
        out, scan_intermediates, out_z = selective_scan_cuda.fwd(
            conv1d_out, delta, A, B, C, D, z, delta_bias, delta_softplus
        )
        ctx.delta_softplus = delta_softplus
        ctx.checkpoint_lvl = checkpoint_lvl
        if (
            checkpoint_lvl >= 1
        ):  # Will recompute conv1d_out and delta in the backward pass
            conv1d_out, delta = None, None
        ctx.save_for_backward(
            xz,
            conv1d_weight,
            conv1d_bias,
            x_dbl,
            x_proj_weight,
            delta_proj_weight,
            conv1d_out,
            delta,
            A,
            B,
            C,
            D,
            delta_bias,
            scan_intermediates,
            out,
        )
        # return rearrange(out_z, "b d l -> b l d")
        return out_z

    @staticmethod
    @custom_bwd
    def backward(ctx, dout):
        # dout: (batch, seqlen, dim)
        (
            xz,
            conv1d_weight,
            conv1d_bias,
            x_dbl,
            x_proj_weight,
            delta_proj_weight,
            conv1d_out,
            delta,
            A,
            B,
            C,
            D,
            delta_bias,
            scan_intermediates,
            out,
        ) = ctx.saved_tensors
        L = xz.shape[-1]
        delta_rank = delta_proj_weight.shape[1]
        d_state = A.shape[-1] * (1 if not A.is_complex() else 2)
        x, z = xz.chunk(2, dim=1)
        if dout.stride(-1) != 1:
            dout = dout.contiguous()
        if ctx.checkpoint_lvl == 1:
            conv1d_out = causal_conv1d_cuda.causal_conv1d_fwd(
                x, conv1d_weight, conv1d_bias, None, True
            )
            delta = rearrange(
                delta_proj_weight @ x_dbl[:, :delta_rank].t(), "d (b l) -> b d l", l=L
            )
        # The kernel supports passing in a pre-allocated dz (e.g., in case we want to fuse the
        # backward of selective_scan_cuda with the backward of chunk).
        dxz = torch.empty_like(xz)  # (batch, dim, seqlen)
        dx, dz = dxz.chunk(2, dim=1)
        # dout_y = rearrange(dout, "b l d -> b d l") # because no arrange at end of forward, so dout shape is b d l
        dconv1d_out, ddelta, dA, dB, dC, dD, ddelta_bias, dz, out_z = (
            selective_scan_cuda.bwd(
                conv1d_out,
                delta,
                A,
                B,
                C,
                D,
                z,
                delta_bias,
                dout,
                scan_intermediates,
                out,
                dz,
                ctx.delta_softplus,
                True,  # option to recompute out_z
            )
        )
        dD = dD if D is not None else None
        dx_dbl = torch.empty_like(x_dbl)
        dB_proj_bias = None
        if ctx.is_variable_B:
            if not A.is_complex():
                dB = rearrange(dB, "b 1 dstate l -> (b l) dstate").contiguous()
            else:
                dB = rearrange(
                    dB, "b 1 dstate (l two) -> (b l) (dstate two)", two=2
                ).contiguous()
            dB_proj_bias = dB.sum(0) if not ctx.B_proj_bias_is_None else None
            dx_dbl[:, delta_rank : delta_rank + d_state] = dB  # (bl d)
            dB = None
        dC_proj_bias = None
        if ctx.is_variable_C:
            if not A.is_complex():
                dC = rearrange(dC, "b 1 dstate l -> (b l) dstate").contiguous()
            else:
                dC = rearrange(
                    dC, "b 1 dstate (l two) -> (b l) (dstate two)", two=2
                ).contiguous()
            dC_proj_bias = dC.sum(0) if not ctx.C_proj_bias_is_None else None
            dx_dbl[:, -d_state:] = dC  # (bl d)
            dC = None
        ddelta = rearrange(ddelta, "b d l -> d (b l)")
        ddelta_proj_weight = torch.einsum("dB,Br->dr", ddelta, x_dbl[:, :delta_rank])
        dx_dbl[:, :delta_rank] = torch.einsum("dB,dr->Br", ddelta, delta_proj_weight)
        dconv1d_out = rearrange(dconv1d_out, "b d l -> d (b l)")
        dx_proj_weight = torch.einsum(
            "Br,Bd->rd", dx_dbl, rearrange(conv1d_out, "b d l -> (b l) d")
        )
        dconv1d_out = torch.addmm(
            dconv1d_out, x_proj_weight.t(), dx_dbl.t(), out=dconv1d_out
        )
        dconv1d_out = rearrange(
            dconv1d_out, "d (b l) -> b d l", b=x.shape[0], l=x.shape[-1]
        )
        # The kernel supports passing in a pre-allocated dx (e.g., in case we want to fuse the
        # backward of conv1d with the backward of chunk).
        dx, dconv1d_weight, dconv1d_bias = causal_conv1d_cuda.causal_conv1d_bwd(
            x, conv1d_weight, conv1d_bias, dconv1d_out, None, dx, True
        )  # ?
        dconv1d_bias = dconv1d_bias if conv1d_bias is not None else None
        dconv1d_weight = rearrange(dconv1d_weight, "d w -> d 1 w")
        return (
            dxz,
            dconv1d_weight,
            dconv1d_bias,
            dx_proj_weight,
            ddelta_proj_weight,
            dA,
            dB,
            dC,
            dD,
            ddelta_bias if delta_bias is not None else None,
            dB_proj_bias,
            dC_proj_bias,
            None,
        )


def mamba_inner_fn(
    xz,
    conv1d_weight,
    conv1d_bias,
    x_proj_weight,
    delta_proj_weight,
    out_proj_weight,
    out_proj_bias,
    A,
    B=None,
    C=None,
    D=None,
    delta_bias=None,
    B_proj_bias=None,
    C_proj_bias=None,
    delta_softplus=True,
):
    return MambaInnerFn.apply(
        xz,
        conv1d_weight,
        conv1d_bias,
        x_proj_weight,
        delta_proj_weight,
        out_proj_weight,
        out_proj_bias,
        A,
        B,
        C,
        D,
        delta_bias,
        B_proj_bias,
        C_proj_bias,
        delta_softplus,
    )


def mamba_inner_fn_no_out_proj(
    xz,
    conv1d_weight,
    conv1d_bias,
    x_proj_weight,
    delta_proj_weight,
    A,
    B=None,
    C=None,
    D=None,
    delta_bias=None,
    B_proj_bias=None,
    C_proj_bias=None,
    delta_softplus=True,
):
    return MambaInnerFnNoOutProj.apply(
        xz,
        conv1d_weight,
        conv1d_bias,
        x_proj_weight,
        delta_proj_weight,
        A,
        B,
        C,
        D,
        delta_bias,
        B_proj_bias,
        C_proj_bias,
        delta_softplus,
    )


def mamba_inner_ref(
    xz,
    conv1d_weight,
    conv1d_bias,
    x_proj_weight,
    delta_proj_weight,
    out_proj_weight,
    out_proj_bias,
    A,
    B=None,
    C=None,
    D=None,
    delta_bias=None,
    B_proj_bias=None,
    C_proj_bias=None,
    delta_softplus=True,
):
    L = xz.shape[-1]
    delta_rank = delta_proj_weight.shape[1]
    d_state = A.shape[-1] * (1 if not A.is_complex() else 2)
    x, z = xz.chunk(2, dim=1)
    x = causal_conv1d_fn(
        x, rearrange(conv1d_weight, "d 1 w -> d w"), conv1d_bias, "silu"
    )
    # We're being very careful here about the layout, to avoid extra transposes.
    # We want delta to have d as the slowest moving dimension
    # and L as the fastest moving dimension, since those are what the ssm_scan kernel expects.
    x_dbl = F.linear(rearrange(x, "b d l -> (b l) d"), x_proj_weight)  # (bl d)
    delta = delta_proj_weight @ x_dbl[:, :delta_rank].t()
    delta = rearrange(delta, "d (b l) -> b d l", l=L)
    if B is None:  # variable B
        B = x_dbl[:, delta_rank : delta_rank + d_state]  # (bl d)
        if B_proj_bias is not None:
            B = B + B_proj_bias.to(dtype=B.dtype)
        if not A.is_complex():
            B = rearrange(B, "(b l) dstate -> b dstate l", l=L).contiguous()
        else:
            B = rearrange(
                B, "(b l) (dstate two) -> b dstate (l two)", l=L, two=2
            ).contiguous()
    if C is None:  # variable B
        C = x_dbl[:, -d_state:]  # (bl d)
        if C_proj_bias is not None:
            C = C + C_proj_bias.to(dtype=C.dtype)
        if not A.is_complex():
            C = rearrange(C, "(b l) dstate -> b dstate l", l=L).contiguous()
        else:
            C = rearrange(
                C, "(b l) (dstate two) -> b dstate (l two)", l=L, two=2
            ).contiguous()
    y = selective_scan_fn(
        x, delta, A, B, C, D, z=z, delta_bias=delta_bias, delta_softplus=True
    )
    return F.linear(rearrange(y, "b d l -> b l d"), out_proj_weight, out_proj_bias)



================================================
File: sauron/mil_models/mamba_ssm/ops/triton/__init__.py
================================================



================================================
File: sauron/mil_models/mamba_ssm/ops/triton/layernorm.py
================================================
# Copyright (c) 2023, Tri Dao.
# Implement residual + layer_norm / rms_norm.

# Based on the Triton LayerNorm tutorial: https://triton-lang.org/main/getting-started/tutorials/05-layer-norm.html
# For the backward pass, we keep weight_grad and bias_grad in registers and accumulate.
# This is faster for dimensions up to 8k, but after that it's much slower due to register spilling.
# The models we train have hidden dim up to 8k anyway (e.g. Llama 70B), so this is fine.

import math

import torch
import torch.nn.functional as F
import triton
import triton.language as tl
from torch.cuda.amp import custom_bwd, custom_fwd


def layer_norm_ref(
    x, weight, bias, residual=None, eps=1e-6, prenorm=False, upcast=False
):
    dtype = x.dtype
    if upcast:
        weight = weight.float()
        bias = bias.float() if bias is not None else None
    if upcast:
        x = x.float()
        residual = residual.float() if residual is not None else residual
    if residual is not None:
        x = (x + residual).to(x.dtype)
    out = F.layer_norm(
        x.to(weight.dtype), x.shape[-1:], weight=weight, bias=bias, eps=eps
    ).to(dtype)
    return out if not prenorm else (out, x)


def rms_norm_ref(x, weight, bias, residual=None, eps=1e-6, prenorm=False, upcast=False):
    dtype = x.dtype
    if upcast:
        weight = weight.float()
        bias = bias.float() if bias is not None else None
    if upcast:
        x = x.float()
        residual = residual.float() if residual is not None else residual
    if residual is not None:
        x = (x + residual).to(x.dtype)
    rstd = 1 / torch.sqrt((x.square()).mean(dim=-1, keepdim=True) + eps)
    out = (x * rstd * weight) + bias if bias is not None else (x * rstd * weight)
    out = out.to(dtype)
    return out if not prenorm else (out, x)


@triton.autotune(
    configs=[
        triton.Config({}, num_warps=1),
        triton.Config({}, num_warps=2),
        triton.Config({}, num_warps=4),
        triton.Config({}, num_warps=8),
        triton.Config({}, num_warps=16),
        triton.Config({}, num_warps=32),
    ],
    key=["N", "HAS_RESIDUAL", "STORE_RESIDUAL_OUT", "IS_RMS_NORM", "HAS_BIAS"],
)
# @triton.heuristics({"HAS_BIAS": lambda args: args["B"] is not None})
# @triton.heuristics({"HAS_RESIDUAL": lambda args: args["RESIDUAL"] is not None})
@triton.jit
def _layer_norm_fwd_1pass_kernel(
    X,  # pointer to the input
    Y,  # pointer to the output
    W,  # pointer to the weights
    B,  # pointer to the biases
    RESIDUAL,  # pointer to the residual
    RESIDUAL_OUT,  # pointer to the residual
    Mean,  # pointer to the mean
    Rstd,  # pointer to the 1/std
    stride_x_row,  # how much to increase the pointer when moving by 1 row
    stride_y_row,
    stride_res_row,
    stride_res_out_row,
    N,  # number of columns in X
    eps,  # epsilon to avoid division by zero
    IS_RMS_NORM: tl.constexpr,
    BLOCK_N: tl.constexpr,
    HAS_RESIDUAL: tl.constexpr,
    STORE_RESIDUAL_OUT: tl.constexpr,
    HAS_BIAS: tl.constexpr,
):
    # Map the program id to the row of X and Y it should compute.
    row = tl.program_id(0)
    X += row * stride_x_row
    Y += row * stride_y_row
    if HAS_RESIDUAL:
        RESIDUAL += row * stride_res_row
    if STORE_RESIDUAL_OUT:
        RESIDUAL_OUT += row * stride_res_out_row
    # Compute mean and variance
    cols = tl.arange(0, BLOCK_N)
    x = tl.load(X + cols, mask=cols < N, other=0.0).to(tl.float32)
    if HAS_RESIDUAL:
        residual = tl.load(RESIDUAL + cols, mask=cols < N, other=0.0).to(tl.float32)
        x += residual
    if STORE_RESIDUAL_OUT:
        tl.store(RESIDUAL_OUT + cols, x, mask=cols < N)
    if not IS_RMS_NORM:
        mean = tl.sum(x, axis=0) / N
        tl.store(Mean + row, mean)
        xbar = tl.where(cols < N, x - mean, 0.0)
        var = tl.sum(xbar * xbar, axis=0) / N
    else:
        xbar = tl.where(cols < N, x, 0.0)
        var = tl.sum(xbar * xbar, axis=0) / N
    rstd = 1 / tl.sqrt(var + eps)
    tl.store(Rstd + row, rstd)
    # Normalize and apply linear transformation
    mask = cols < N
    w = tl.load(W + cols, mask=mask).to(tl.float32)
    if HAS_BIAS:
        b = tl.load(B + cols, mask=mask).to(tl.float32)
    x_hat = (x - mean) * rstd if not IS_RMS_NORM else x * rstd
    y = x_hat * w + b if HAS_BIAS else x_hat * w
    # Write output
    tl.store(Y + cols, y, mask=mask)


def _layer_norm_fwd(
    x,
    weight,
    bias,
    eps,
    residual=None,
    out_dtype=None,
    residual_dtype=None,
    is_rms_norm=False,
):
    if residual is not None:
        residual_dtype = residual.dtype
    M, N = x.shape
    assert x.stride(-1) == 1
    if residual is not None:
        assert residual.stride(-1) == 1
        assert residual.shape == (M, N)
    assert weight.shape == (N,)
    assert weight.stride(-1) == 1
    if bias is not None:
        assert bias.stride(-1) == 1
        assert bias.shape == (N,)
    # allocate output
    y = torch.empty_like(x, dtype=x.dtype if out_dtype is None else out_dtype)
    assert y.stride(-1) == 1
    if residual is not None or (
        residual_dtype is not None and residual_dtype != x.dtype
    ):
        residual_out = torch.empty(M, N, device=x.device, dtype=residual_dtype)
        assert residual_out.stride(-1) == 1
    else:
        residual_out = None
    mean = (
        torch.empty((M,), dtype=torch.float32, device=x.device)
        if not is_rms_norm
        else None
    )
    rstd = torch.empty((M,), dtype=torch.float32, device=x.device)
    # Less than 64KB per feature: enqueue fused kernel
    MAX_FUSED_SIZE = 65536 // x.element_size()
    BLOCK_N = min(MAX_FUSED_SIZE, triton.next_power_of_2(N))
    if N > BLOCK_N:
        raise RuntimeError("This layer norm doesn't support feature dim >= 64KB.")
    # heuristics for number of warps
    with torch.cuda.device(x.device.index):
        _layer_norm_fwd_1pass_kernel[(M,)](
            x,
            y,
            weight,
            bias,
            residual,
            residual_out,
            mean,
            rstd,
            x.stride(0),
            y.stride(0),
            residual.stride(0) if residual is not None else 0,
            residual_out.stride(0) if residual_out is not None else 0,
            N,
            eps,
            is_rms_norm,
            BLOCK_N,
            residual is not None,
            residual_out is not None,
            bias is not None,
        )
    # residual_out is None if residual is None and residual_dtype == input_dtype
    return y, mean, rstd, residual_out if residual_out is not None else x


@triton.autotune(
    configs=[
        triton.Config({}, num_warps=1),
        triton.Config({}, num_warps=2),
        triton.Config({}, num_warps=4),
        triton.Config({}, num_warps=8),
        triton.Config({}, num_warps=16),
        triton.Config({}, num_warps=32),
    ],
    key=["N", "HAS_DRESIDUAL", "STORE_DRESIDUAL", "IS_RMS_NORM", "HAS_BIAS"],
)
# @triton.heuristics({"HAS_BIAS": lambda args: args["B"] is not None})
# @triton.heuristics({"HAS_DRESIDUAL": lambda args: args["DRESIDUAL"] is not None})
# @triton.heuristics({"STORE_DRESIDUAL": lambda args: args["DRESIDUAL_IN"] is not None})
@triton.heuristics({"RECOMPUTE_OUTPUT": lambda args: args["Y"] is not None})
@triton.jit
def _layer_norm_bwd_kernel(
    X,  # pointer to the input
    W,  # pointer to the weights
    B,  # pointer to the biases
    Y,  # pointer to the output to be recomputed
    DY,  # pointer to the output gradient
    DX,  # pointer to the input gradient
    DW,  # pointer to the partial sum of weights gradient
    DB,  # pointer to the partial sum of biases gradient
    DRESIDUAL,
    DRESIDUAL_IN,
    Mean,  # pointer to the mean
    Rstd,  # pointer to the 1/std
    stride_x_row,  # how much to increase the pointer when moving by 1 row
    stride_y_row,
    stride_dy_row,
    stride_dx_row,
    stride_dres_row,
    stride_dres_in_row,
    M,  # number of rows in X
    N,  # number of columns in X
    eps,  # epsilon to avoid division by zero
    rows_per_program,
    IS_RMS_NORM: tl.constexpr,
    BLOCK_N: tl.constexpr,
    HAS_DRESIDUAL: tl.constexpr,
    STORE_DRESIDUAL: tl.constexpr,
    HAS_BIAS: tl.constexpr,
    RECOMPUTE_OUTPUT: tl.constexpr,
):
    # Map the program id to the elements of X, DX, and DY it should compute.
    row_block_id = tl.program_id(0)
    row_start = row_block_id * rows_per_program
    cols = tl.arange(0, BLOCK_N)
    mask = cols < N
    X += row_start * stride_x_row
    if HAS_DRESIDUAL:
        DRESIDUAL += row_start * stride_dres_row
    if STORE_DRESIDUAL:
        DRESIDUAL_IN += row_start * stride_dres_in_row
    DY += row_start * stride_dy_row
    DX += row_start * stride_dx_row
    if RECOMPUTE_OUTPUT:
        Y += row_start * stride_y_row
    w = tl.load(W + cols, mask=mask).to(tl.float32)
    if RECOMPUTE_OUTPUT and HAS_BIAS:
        b = tl.load(B + cols, mask=mask, other=0.0).to(tl.float32)
    dw = tl.zeros((BLOCK_N,), dtype=tl.float32)
    if HAS_BIAS:
        db = tl.zeros((BLOCK_N,), dtype=tl.float32)
    row_end = min((row_block_id + 1) * rows_per_program, M)
    for row in range(row_start, row_end):
        # Load data to SRAM
        x = tl.load(X + cols, mask=mask, other=0).to(tl.float32)
        dy = tl.load(DY + cols, mask=mask, other=0).to(tl.float32)
        if not IS_RMS_NORM:
            mean = tl.load(Mean + row)
        rstd = tl.load(Rstd + row)
        # Compute dx
        xhat = (x - mean) * rstd if not IS_RMS_NORM else x * rstd
        xhat = tl.where(mask, xhat, 0.0)
        if RECOMPUTE_OUTPUT:
            y = xhat * w + b if HAS_BIAS else xhat * w
            tl.store(Y + cols, y, mask=mask)
        wdy = w * dy
        dw += dy * xhat
        if HAS_BIAS:
            db += dy
        if not IS_RMS_NORM:
            c1 = tl.sum(xhat * wdy, axis=0) / N
            c2 = tl.sum(wdy, axis=0) / N
            dx = (wdy - (xhat * c1 + c2)) * rstd
        else:
            c1 = tl.sum(xhat * wdy, axis=0) / N
            dx = (wdy - xhat * c1) * rstd
        if HAS_DRESIDUAL:
            dres = tl.load(DRESIDUAL + cols, mask=mask, other=0).to(tl.float32)
            dx += dres
        # Write dx
        if STORE_DRESIDUAL:
            tl.store(DRESIDUAL_IN + cols, dx, mask=mask)
        tl.store(DX + cols, dx, mask=mask)

        X += stride_x_row
        if HAS_DRESIDUAL:
            DRESIDUAL += stride_dres_row
        if STORE_DRESIDUAL:
            DRESIDUAL_IN += stride_dres_in_row
        if RECOMPUTE_OUTPUT:
            Y += stride_y_row
        DY += stride_dy_row
        DX += stride_dx_row
    tl.store(DW + row_block_id * N + cols, dw, mask=mask)
    if HAS_BIAS:
        tl.store(DB + row_block_id * N + cols, db, mask=mask)


def _layer_norm_bwd(
    dy,
    x,
    weight,
    bias,
    eps,
    mean,
    rstd,
    dresidual=None,
    has_residual=False,
    is_rms_norm=False,
    x_dtype=None,
    recompute_output=False,
):
    M, N = x.shape
    assert x.stride(-1) == 1
    assert dy.stride(-1) == 1
    assert dy.shape == (M, N)
    if dresidual is not None:
        assert dresidual.stride(-1) == 1
        assert dresidual.shape == (M, N)
    assert weight.shape == (N,)
    assert weight.stride(-1) == 1
    if bias is not None:
        assert bias.stride(-1) == 1
        assert bias.shape == (N,)
    # allocate output
    dx = (
        torch.empty_like(x)
        if x_dtype is None
        else torch.empty(M, N, dtype=x_dtype, device=x.device)
    )
    dresidual_in = torch.empty_like(x) if has_residual and dx.dtype != x.dtype else None
    y = (
        torch.empty(M, N, dtype=dy.dtype, device=dy.device)
        if recompute_output
        else None
    )

    # Less than 64KB per feature: enqueue fused kernel
    MAX_FUSED_SIZE = 65536 // x.element_size()
    BLOCK_N = min(MAX_FUSED_SIZE, triton.next_power_of_2(N))
    if N > BLOCK_N:
        raise RuntimeError("This layer norm doesn't support feature dim >= 64KB.")
    sm_count = torch.cuda.get_device_properties(x.device).multi_processor_count
    _dw = torch.empty((sm_count, N), dtype=torch.float32, device=weight.device)
    _db = (
        torch.empty((sm_count, N), dtype=torch.float32, device=bias.device)
        if bias is not None
        else None
    )
    rows_per_program = math.ceil(M / sm_count)
    grid = (sm_count,)
    with torch.cuda.device(x.device.index):
        _layer_norm_bwd_kernel[grid](
            x,
            weight,
            bias,
            y,
            dy,
            dx,
            _dw,
            _db,
            dresidual,
            dresidual_in,
            mean,
            rstd,
            x.stride(0),
            0 if not recompute_output else y.stride(0),
            dy.stride(0),
            dx.stride(0),
            dresidual.stride(0) if dresidual is not None else 0,
            dresidual_in.stride(0) if dresidual_in is not None else 0,
            M,
            N,
            eps,
            rows_per_program,
            is_rms_norm,
            BLOCK_N,
            dresidual is not None,
            dresidual_in is not None,
            bias is not None,
        )
    dw = _dw.sum(0).to(weight.dtype)
    db = _db.sum(0).to(bias.dtype) if bias is not None else None
    # Don't need to compute dresidual_in separately in this case
    if has_residual and dx.dtype == x.dtype:
        dresidual_in = dx
    return (
        (dx, dw, db, dresidual_in)
        if not recompute_output
        else (dx, dw, db, dresidual_in, y)
    )


class LayerNormFn(torch.autograd.Function):
    @staticmethod
    def forward(
        ctx,
        x,
        weight,
        bias,
        residual=None,
        eps=1e-6,
        prenorm=False,
        residual_in_fp32=False,
        is_rms_norm=False,
    ):
        x_shape_og = x.shape
        # reshape input data into 2D tensor
        x = x.reshape(-1, x.shape[-1])
        if x.stride(-1) != 1:
            x = x.contiguous()
        if residual is not None:
            assert residual.shape == x_shape_og
            residual = residual.reshape(-1, residual.shape[-1])
            if residual.stride(-1) != 1:
                residual = residual.contiguous()
        weight = weight.contiguous()
        if bias is not None:
            bias = bias.contiguous()
        residual_dtype = (
            residual.dtype
            if residual is not None
            else (torch.float32 if residual_in_fp32 else None)
        )
        y, mean, rstd, residual_out = _layer_norm_fwd(
            x,
            weight,
            bias,
            eps,
            residual,
            residual_dtype=residual_dtype,
            is_rms_norm=is_rms_norm,
        )
        ctx.save_for_backward(residual_out, weight, bias, mean, rstd)
        ctx.x_shape_og = x_shape_og
        ctx.eps = eps
        ctx.is_rms_norm = is_rms_norm
        ctx.has_residual = residual is not None
        ctx.prenorm = prenorm
        ctx.x_dtype = x.dtype
        y = y.reshape(x_shape_og)
        return y if not prenorm else (y, residual_out.reshape(x_shape_og))

    @staticmethod
    def backward(ctx, dy, *args):
        x, weight, bias, mean, rstd = ctx.saved_tensors
        dy = dy.reshape(-1, dy.shape[-1])
        if dy.stride(-1) != 1:
            dy = dy.contiguous()
        assert dy.shape == x.shape
        if ctx.prenorm:
            dresidual = args[0]
            dresidual = dresidual.reshape(-1, dresidual.shape[-1])
            if dresidual.stride(-1) != 1:
                dresidual = dresidual.contiguous()
            assert dresidual.shape == x.shape
        else:
            dresidual = None
        dx, dw, db, dresidual_in = _layer_norm_bwd(
            dy,
            x,
            weight,
            bias,
            ctx.eps,
            mean,
            rstd,
            dresidual,
            ctx.has_residual,
            ctx.is_rms_norm,
            x_dtype=ctx.x_dtype,
        )
        return (
            dx.reshape(ctx.x_shape_og),
            dw,
            db,
            dresidual_in.reshape(ctx.x_shape_og) if ctx.has_residual else None,
            None,
            None,
            None,
            None,
        )


def layer_norm_fn(
    x,
    weight,
    bias,
    residual=None,
    eps=1e-6,
    prenorm=False,
    residual_in_fp32=False,
    is_rms_norm=False,
):
    return LayerNormFn.apply(
        x, weight, bias, residual, eps, prenorm, residual_in_fp32, is_rms_norm
    )


def rms_norm_fn(
    x, weight, bias, residual=None, prenorm=False, residual_in_fp32=False, eps=1e-6
):
    return LayerNormFn.apply(
        x, weight, bias, residual, eps, prenorm, residual_in_fp32, True
    )


class RMSNorm(torch.nn.Module):
    def __init__(self, hidden_size, eps=1e-5, device=None, dtype=None):
        factory_kwargs = {"device": device, "dtype": dtype}
        super().__init__()
        self.eps = eps
        self.weight = torch.nn.Parameter(torch.empty(hidden_size, **factory_kwargs))
        self.register_parameter("bias", None)
        self.reset_parameters()

    def reset_parameters(self):
        torch.nn.init.ones_(self.weight)

    def forward(self, x, residual=None, prenorm=False, residual_in_fp32=False):
        return rms_norm_fn(
            x,
            self.weight,
            self.bias,
            residual=residual,
            eps=self.eps,
            prenorm=prenorm,
            residual_in_fp32=residual_in_fp32,
        )


class LayerNormLinearFn(torch.autograd.Function):
    @staticmethod
    @custom_fwd
    def forward(
        ctx,
        x,
        norm_weight,
        norm_bias,
        linear_weight,
        linear_bias,
        residual=None,
        eps=1e-6,
        prenorm=False,
        residual_in_fp32=False,
        is_rms_norm=False,
    ):
        x_shape_og = x.shape
        # reshape input data into 2D tensor
        x = x.reshape(-1, x.shape[-1])
        if x.stride(-1) != 1:
            x = x.contiguous()
        if residual is not None:
            assert residual.shape == x_shape_og
            residual = residual.reshape(-1, residual.shape[-1])
            if residual.stride(-1) != 1:
                residual = residual.contiguous()
        norm_weight = norm_weight.contiguous()
        if norm_bias is not None:
            norm_bias = norm_bias.contiguous()
        residual_dtype = (
            residual.dtype
            if residual is not None
            else (torch.float32 if residual_in_fp32 else None)
        )
        y, mean, rstd, residual_out = _layer_norm_fwd(
            x,
            norm_weight,
            norm_bias,
            eps,
            residual,
            out_dtype=(
                None
                if not torch.is_autocast_enabled()
                else torch.get_autocast_gpu_dtype()
            ),
            residual_dtype=residual_dtype,
            is_rms_norm=is_rms_norm,
        )
        y = y.reshape(x_shape_og)
        dtype = (
            torch.get_autocast_gpu_dtype() if torch.is_autocast_enabled() else y.dtype
        )
        linear_weight = linear_weight.to(dtype)
        linear_bias = linear_bias.to(dtype) if linear_bias is not None else None
        out = F.linear(y.to(linear_weight.dtype), linear_weight, linear_bias)
        # We don't store y, will be recomputed in the backward pass to save memory
        ctx.save_for_backward(
            residual_out, norm_weight, norm_bias, linear_weight, mean, rstd
        )
        ctx.x_shape_og = x_shape_og
        ctx.eps = eps
        ctx.is_rms_norm = is_rms_norm
        ctx.has_residual = residual is not None
        ctx.prenorm = prenorm
        ctx.x_dtype = x.dtype
        ctx.linear_bias_is_none = linear_bias is None
        return out if not prenorm else (out, residual_out.reshape(x_shape_og))

    @staticmethod
    @custom_bwd
    def backward(ctx, dout, *args):
        x, norm_weight, norm_bias, linear_weight, mean, rstd = ctx.saved_tensors
        dout = dout.reshape(-1, dout.shape[-1])
        dy = F.linear(dout, linear_weight.t())
        dlinear_bias = None if ctx.linear_bias_is_none else dout.sum(0)
        if dy.stride(-1) != 1:
            dy = dy.contiguous()
        assert dy.shape == x.shape
        if ctx.prenorm:
            dresidual = args[0]
            dresidual = dresidual.reshape(-1, dresidual.shape[-1])
            if dresidual.stride(-1) != 1:
                dresidual = dresidual.contiguous()
            assert dresidual.shape == x.shape
        else:
            dresidual = None
        dx, dnorm_weight, dnorm_bias, dresidual_in, y = _layer_norm_bwd(
            dy,
            x,
            norm_weight,
            norm_bias,
            ctx.eps,
            mean,
            rstd,
            dresidual,
            ctx.has_residual,
            ctx.is_rms_norm,
            x_dtype=ctx.x_dtype,
            recompute_output=True,
        )
        dlinear_weight = torch.einsum("bo,bi->oi", dout, y)
        return (
            dx.reshape(ctx.x_shape_og),
            dnorm_weight,
            dnorm_bias,
            dlinear_weight,
            dlinear_bias,
            dresidual_in.reshape(ctx.x_shape_og) if ctx.has_residual else None,
            None,
            None,
            None,
            None,
        )


def layer_norm_linear_fn(
    x,
    norm_weight,
    norm_bias,
    linear_weight,
    linear_bias,
    residual=None,
    eps=1e-6,
    prenorm=False,
    residual_in_fp32=False,
    is_rms_norm=False,
):
    return LayerNormLinearFn.apply(
        x,
        norm_weight,
        norm_bias,
        linear_weight,
        linear_bias,
        residual,
        eps,
        prenorm,
        residual_in_fp32,
        is_rms_norm,
    )



================================================
File: sauron/mil_models/mamba_ssm/ops/triton/selective_state_update.py
================================================
# Copyright (c) 2023, Tri Dao.

"""We want triton==2.1.0 for this"""

import torch
import torch.nn.functional as F
import triton
import triton.language as tl
from einops import rearrange


@triton.heuristics({"HAS_DT_BIAS": lambda args: args["dt_bias_ptr"] is not None})
@triton.heuristics({"HAS_D": lambda args: args["D_ptr"] is not None})
@triton.heuristics({"HAS_Z": lambda args: args["z_ptr"] is not None})
@triton.heuristics(
    {"BLOCK_SIZE_DSTATE": lambda args: triton.next_power_of_2(args["dstate"])}
)
@triton.jit
def _selective_scan_update_kernel(
    # Pointers to matrices
    state_ptr,
    x_ptr,
    dt_ptr,
    dt_bias_ptr,
    A_ptr,
    B_ptr,
    C_ptr,
    D_ptr,
    z_ptr,
    out_ptr,
    # Matrix dimensions
    batch,
    dim,
    dstate,
    # Strides
    stride_state_batch,
    stride_state_dim,
    stride_state_dstate,
    stride_x_batch,
    stride_x_dim,
    stride_dt_batch,
    stride_dt_dim,
    stride_dt_bias_dim,
    stride_A_dim,
    stride_A_dstate,
    stride_B_batch,
    stride_B_dstate,
    stride_C_batch,
    stride_C_dstate,
    stride_D_dim,
    stride_z_batch,
    stride_z_dim,
    stride_out_batch,
    stride_out_dim,
    # Meta-parameters
    DT_SOFTPLUS: tl.constexpr,
    BLOCK_SIZE_M: tl.constexpr,
    HAS_DT_BIAS: tl.constexpr,
    HAS_D: tl.constexpr,
    HAS_Z: tl.constexpr,
    BLOCK_SIZE_DSTATE: tl.constexpr,
):
    pid_m = tl.program_id(axis=0)
    pid_b = tl.program_id(axis=1)
    state_ptr += pid_b * stride_state_batch
    x_ptr += pid_b * stride_x_batch
    dt_ptr += pid_b * stride_dt_batch
    B_ptr += pid_b * stride_B_batch
    C_ptr += pid_b * stride_C_batch
    if HAS_Z:
        z_ptr += pid_b * stride_z_batch
    out_ptr += pid_b * stride_out_batch

    offs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)
    offs_n = tl.arange(0, BLOCK_SIZE_DSTATE)
    state_ptrs = state_ptr + (
        offs_m[:, None] * stride_state_dim + offs_n[None, :] * stride_state_dstate
    )
    x_ptrs = x_ptr + offs_m * stride_x_dim
    dt_ptrs = dt_ptr + offs_m * stride_dt_dim
    if HAS_DT_BIAS:
        dt_bias_ptrs = dt_bias_ptr + offs_m * stride_dt_bias_dim
    A_ptrs = A_ptr + (
        offs_m[:, None] * stride_A_dim + offs_n[None, :] * stride_A_dstate
    )
    B_ptrs = B_ptr + offs_n * stride_B_dstate
    C_ptrs = C_ptr + offs_n * stride_C_dstate
    if HAS_D:
        D_ptrs = D_ptr + offs_m * stride_D_dim
    if HAS_Z:
        z_ptrs = z_ptr + offs_m * stride_z_dim
    out_ptrs = out_ptr + offs_m * stride_out_dim

    state = tl.load(
        state_ptrs, mask=(offs_m[:, None] < dim) & (offs_n[None, :] < dstate), other=0.0
    )
    x = tl.load(x_ptrs, mask=offs_m < dim, other=0.0).to(tl.float32)
    dt = tl.load(dt_ptrs, mask=offs_m < dim, other=0.0).to(tl.float32)
    if HAS_DT_BIAS:
        dt += tl.load(dt_bias_ptrs, mask=offs_m < dim, other=0.0).to(tl.float32)
    if DT_SOFTPLUS:
        dt = tl.log(1.0 + tl.exp(dt))
    A = tl.load(
        A_ptrs, mask=(offs_m[:, None] < dim) & (offs_n[None, :] < dstate), other=0.0
    ).to(tl.float32)
    dA = tl.exp(A * dt[:, None])
    B = tl.load(B_ptrs, mask=offs_n < dstate, other=0.0).to(tl.float32)
    C = tl.load(C_ptrs, mask=offs_n < dstate, other=0.0).to(tl.float32)
    if HAS_D:
        D = tl.load(D_ptrs, mask=offs_m < dim, other=0.0).to(tl.float32)
    if HAS_Z:
        z = tl.load(z_ptrs, mask=offs_m < dim, other=0.0).to(tl.float32)

    dB = B[None, :] * dt[:, None]
    state = state * dA + dB * x[:, None]
    tl.store(
        state_ptrs, state, mask=(offs_m[:, None] < dim) & (offs_n[None, :] < dstate)
    )
    out = tl.sum(state * C[None, :], axis=1)
    if HAS_D:
        out += x * D
    if HAS_Z:
        out *= z * tl.sigmoid(z)
    tl.store(out_ptrs, out, mask=offs_m < dim)


def selective_state_update(
    state, x, dt, A, B, C, D=None, z=None, dt_bias=None, dt_softplus=False
):
    """
    Argument:
        state: (batch, dim, dstate)
        x: (batch, dim)
        dt: (batch, dim)
        A: (dim, dstate)
        B: (batch, dstate)
        C: (batch, dstate)
        D: (dim,)
        z: (batch, dim)
        dt_bias: (dim,)
    Return:
        out: (batch, dim)
    """
    batch, dim, dstate = state.shape
    assert x.shape == (batch, dim)
    assert dt.shape == x.shape
    assert A.shape == (dim, dstate)
    assert B.shape == (batch, dstate)
    assert C.shape == B.shape
    if D is not None:
        assert D.shape == (dim,)
    if z is not None:
        assert z.shape == x.shape
    if dt_bias is not None:
        assert dt_bias.shape == (dim,)
    out = torch.empty_like(x)

    def grid(META):
        return triton.cdiv(dim, META["BLOCK_SIZE_M"]), batch

    z_strides = (z.stride(0), z.stride(1)) if z is not None else (0, 0)
    # We don't want autotune since it will overwrite the state
    # We instead tune by hand.
    BLOCK_SIZE_M, num_warps = (
        (32, 4)
        if dstate <= 16
        else (
            (16, 4)
            if dstate <= 32
            else ((8, 4) if dstate <= 64 else ((4, 4) if dstate <= 128 else ((4, 8))))
        )
    )
    with torch.cuda.device(x.device.index):
        _selective_scan_update_kernel[grid](
            state,
            x,
            dt,
            dt_bias,
            A,
            B,
            C,
            D,
            z,
            out,
            batch,
            dim,
            dstate,
            state.stride(0),
            state.stride(1),
            state.stride(2),
            x.stride(0),
            x.stride(1),
            dt.stride(0),
            dt.stride(1),
            dt_bias.stride(0) if dt_bias is not None else 0,
            A.stride(0),
            A.stride(1),
            B.stride(0),
            B.stride(1),
            C.stride(0),
            C.stride(1),
            D.stride(0) if D is not None else 0,
            z_strides[0],
            z_strides[1],
            out.stride(0),
            out.stride(1),
            dt_softplus,
            BLOCK_SIZE_M,
            num_warps=num_warps,
        )
    return out


def selective_state_update_ref(
    state, x, dt, A, B, C, D=None, z=None, dt_bias=None, dt_softplus=False
):
    """
    Argument:
        state: (batch, dim, dstate)
        x: (batch, dim)
        dt: (batch, dim)
        A: (dim, dstate)
        B: (batch, dstate)
        C: (batch, dstate)
        D: (dim,)
        z: (batch, dim)
        dt_bias: (dim,)
    Return:
        out: (batch, dim)
    """
    batch, dim, dstate = state.shape
    assert x.shape == (batch, dim)
    assert dt.shape == x.shape
    assert A.shape == (dim, dstate)
    assert B.shape == (batch, dstate)
    assert C.shape == B.shape
    if D is not None:
        assert D.shape == (dim,)
    if z is not None:
        assert z.shape == x.shape
    if dt_bias is not None:
        assert dt_bias.shape == (dim,)
        dt = dt + dt_bias
    dt = F.softplus(dt) if dt_softplus else dt
    dA = torch.exp(rearrange(dt, "b d -> b d 1") * A)  # (batch, dim, dstate)
    dB = rearrange(dt, "b d -> b d 1") * rearrange(
        B, "b n -> b 1 n"
    )  # (batch, dim, dstate)
    state.copy_(state * dA + dB * rearrange(x, "b d -> b d 1"))  # (batch, dim, dstate
    out = torch.einsum("bdn,bn->bd", state.to(C.dtype), C)
    if D is not None:
        out += (x * D).to(out.dtype)
    return (out if z is None else out * F.silu(z)).to(x.dtype)



================================================
File: sauron/mil_models/mamba_ssm/utils/__init__.py
================================================



================================================
File: sauron/mil_models/mamba_ssm/utils/generation.py
================================================
# Copyright (c) 2023, Albert Gu, Tri Dao.
import gc
import time
from collections import namedtuple
from dataclasses import dataclass, field
from functools import partial
from typing import Callable, Optional, Sequence, Union

import torch
import torch.nn.functional as F
from einops import rearrange, repeat
from torch import Tensor
from torch.profiler import ProfilerActivity, profile, record_function
from transformers.generation import (
    GreedySearchDecoderOnlyOutput,
    SampleDecoderOnlyOutput,
    TextStreamer,
)


@dataclass
class InferenceParams:
    """Inference parameters that are passed to the main model in order
    to efficienly calculate and store the context during inference."""

    max_seqlen: int
    max_batch_size: int
    seqlen_offset: int = 0
    batch_size_offset: int = 0
    key_value_memory_dict: dict = field(default_factory=dict)
    lengths_per_sample: Optional[Tensor] = None

    def reset(self, max_seqlen, max_batch_size):
        self.max_seqlen = max_seqlen
        self.max_batch_size = max_batch_size
        self.seqlen_offset = 0
        if self.lengths_per_sample is not None:
            self.lengths_per_sample.zero_()


# https://github.com/NVIDIA/Megatron-LM/blob/0bb597b42c53355a567aba2a1357cc34b9d99ddd/megatron/text_generation/sampling.py
# https://github.com/huggingface/transformers/blob/a44985b41cfa2de48a5e1de7f1f93b7483da25d1/src/transformers/generation/logits_process.py#L231
def modify_logits_for_top_k_filtering(logits, top_k):
    """Set the logits for none top-k values to -inf. Done in-place."""
    indices_to_remove = logits < torch.topk(logits, top_k)[0][..., -1, None]
    logits.masked_fill_(indices_to_remove, float("-Inf"))


# https://github.com/NVIDIA/Megatron-LM/blob/0bb597b42c53355a567aba2a1357cc34b9d99ddd/megatron/text_generation/sampling.py
# https://github.com/huggingface/transformers/blob/a44985b41cfa2de48a5e1de7f1f93b7483da25d1/src/transformers/generation/logits_process.py#L170
def modify_logits_for_top_p_filtering(logits, top_p):
    """Set the logits for none top-p values to -inf. Done in-place."""
    if top_p <= 0.0 or top_p >= 1.0:
        return
    # First sort and calculate cumulative sum of probabilities.
    sorted_logits, sorted_indices = torch.sort(logits, descending=False)
    cumulative_probs = sorted_logits.softmax(dim=-1).cumsum(dim=-1)
    # Remove tokens with cumulative top_p above the threshold (token with 0 are kept)
    sorted_indices_to_remove = cumulative_probs <= (1 - top_p)
    # scatter sorted tensors to original indexing
    indices_to_remove = sorted_indices_to_remove.scatter(
        1, sorted_indices, sorted_indices_to_remove
    )
    logits.masked_fill_(indices_to_remove, float("-inf"))


def modify_logit_for_repetition_penalty(
    logits, prev_output_tokens, repetition_penalty=1.0
):
    """Apply repetition penalty. See https://arxiv.org/abs/1909.05858
    logits: (batch_size, vocab_size)
    prev_output_tokens: (batch_size, seq_len)
    """
    if repetition_penalty == 1.0:
        return logits
    score = torch.gather(logits, 1, prev_output_tokens)
    # if score < 0 then repetition penalty has to be multiplied to reduce the previous token probability
    score = torch.where(
        score < 0, score * repetition_penalty, score / repetition_penalty
    )
    logits.scatter_(1, prev_output_tokens, score)
    return logits


def sample(logits, top_k=1, top_p=0.0, temperature=1.0):
    """Sample from top-k logits.
    Arguments:
        logits: Tensor of shape (batch_size, vocab_size)
    """
    if top_k == 1:  # Short-circuit for greedy decoding
        return logits.argmax(dim=-1)
    else:
        if top_p > 0.0:
            assert top_p <= 1.0, "top-p should be in (0, 1]."
        if top_k > 0:
            top_k = min(top_k, logits.size(-1))  # Safety check
            logits_top, indices = torch.topk(logits, top_k, dim=-1)
            if temperature != 1.0:
                logits_top /= temperature
            modify_logits_for_top_p_filtering(logits_top, top_p)
            return indices[
                torch.arange(indices.shape[0], device=indices.device),
                torch.multinomial(
                    torch.softmax(logits_top, dim=-1), num_samples=1
                ).squeeze(dim=-1),
            ]
        else:
            # Clone so that when we modify for top_p we don't change the original logits
            logits_top = logits / temperature if temperature != 1.0 else logits.clone()
            modify_logits_for_top_p_filtering(logits_top, top_p)
            return torch.multinomial(
                torch.softmax(logits_top, dim=-1), num_samples=1
            ).squeeze(dim=-1)


@torch.inference_mode()
def decode(
    input_ids,
    model,
    max_length,
    top_k=1,
    top_p=0.0,
    temperature=1.0,
    repetition_penalty=1.0,
    eos_token_id=None,
    teacher_outputs=None,
    vocab_size=None,
    cg=False,
    enable_timing=False,
    streamer: Optional[TextStreamer] = None,
):
    """Decoding, either greedy or with top-k or top-p sampling.
    If top-k = 0, don't limit the number of candidates (pure sampling).
    Top-k and top-p can be used together. If top_k > 0 and top_p > 0, then top-k is applied first,
    then top-p.
    We assume that all sequences in the same batch have the same length.

    Arguments:
        input_ids: (batch, seq_len)
        max_length: int
        teacher_outputs (optional): (batch, seq_len). If provided, instead of sampling from the
            logits, the next token is taken from the teacher_outputs. Useful for testing.
    Returns: GreedySearchDecoderOnlyOutput or SampleDecoderOnlyOutput, with the following fields:
        sequences: (batch, max_length)
        scores: tuples of (batch, vocab_size)
    """
    if streamer is not None:
        streamer.put(input_ids.cpu())

    batch_size, seqlen_og = input_ids.shape
    teacher_output_len = teacher_outputs.shape[1] if teacher_outputs is not None else 0
    if cg:
        if not hasattr(model, "_decoding_cache"):
            model._decoding_cache = None
        model._decoding_cache = update_graph_cache(
            model,
            model._decoding_cache,
            batch_size,
            seqlen_og,
            max_length,
        )
        inference_params = model._decoding_cache.inference_params
        inference_params.reset(max_length, batch_size)
    else:
        inference_params = InferenceParams(
            max_seqlen=max_length, max_batch_size=batch_size
        )

    def get_logits(input_ids, inference_params):
        decoding = inference_params.seqlen_offset > 0
        if decoding:
            position_ids = torch.full(
                (batch_size, 1),
                inference_params.seqlen_offset,
                dtype=torch.long,
                device=input_ids.device,
            )
        else:
            position_ids = None
        if not cg or not decoding:
            logits = model(
                input_ids,
                position_ids=position_ids,
                inference_params=inference_params,
                num_last_tokens=1,
            ).logits.squeeze(dim=1)
        else:
            logits = model._decoding_cache.run(
                input_ids, position_ids, inference_params.seqlen_offset
            ).squeeze(dim=1)
        return logits[..., :vocab_size] if vocab_size is not None else logits

    def sample_tokens(logits, inference_params):
        if (
            teacher_outputs is None
            or teacher_output_len <= inference_params.seqlen_offset
        ):
            token = sample(logits, top_k=top_k, top_p=top_p, temperature=temperature)
        else:
            token = teacher_outputs[:, inference_params.seqlen_offset]
        # return rearrange(token, "b -> b 1")
        return token.unsqueeze(1)

    def should_stop(current_token, inference_params):
        if inference_params.seqlen_offset == 0:
            return False
        if eos_token_id is not None and (current_token == eos_token_id).all():
            return True
        if inference_params.seqlen_offset >= max_length - 1:
            return True
        return False

    start = torch.cuda.Event(enable_timing=enable_timing)
    end = torch.cuda.Event(enable_timing=enable_timing)

    if enable_timing:
        start.record()
    scores, sequences = [], [input_ids]
    sequences_cat = input_ids
    while not should_stop(sequences[-1], inference_params):
        scores.append(get_logits(sequences[-1], inference_params))
        inference_params.seqlen_offset += sequences[-1].shape[1]
        if repetition_penalty == 1.0:
            sampled_tokens = sample_tokens(scores[-1], inference_params)
        else:
            logits = modify_logit_for_repetition_penalty(
                scores[-1].clone(), sequences_cat, repetition_penalty
            )
            sampled_tokens = sample_tokens(logits, inference_params)
            sequences_cat = torch.cat([sequences_cat, sampled_tokens], dim=1)
        sequences.append(sampled_tokens)
        if streamer is not None:
            streamer.put(sampled_tokens.cpu())
    if streamer is not None:
        streamer.end()
    if enable_timing:
        end.record()
        torch.cuda.synchronize()
        print(f"Prompt processing + decoding time: {(start.elapsed_time(end)):.0f}ms")
    output_cls = (
        GreedySearchDecoderOnlyOutput if top_k == 1 else SampleDecoderOnlyOutput
    )
    return output_cls(sequences=torch.cat(sequences, dim=1), scores=tuple(scores))


class GenerationMixin:
    def allocate_inference_cache(self, batch_size, max_seqlen, dtype=None, **kwargs):
        raise NotImplementedError

    def generate(
        self,
        input_ids,
        max_length,
        top_k=1,
        top_p=0.0,
        temperature=1.0,
        return_dict_in_generate=False,
        output_scores=False,
        **kwargs,
    ):
        output = decode(
            input_ids,
            self,
            max_length,
            top_k=top_k,
            top_p=top_p,
            temperature=temperature,
            **kwargs,
        )
        if not output_scores:
            output.scores = None
        return output if return_dict_in_generate else output.sequences


@dataclass
class DecodingCGCache:
    max_batch_size: int = 0
    max_seqlen: int = 0
    device = None
    dtype = None
    callables: dict = field(default_factory=dict)
    mempool = None
    inference_params: Optional[InferenceParams] = None
    run: Optional[Callable] = None


@torch.inference_mode()
def update_graph_cache(
    model,
    cache,
    batch_size,
    seqlen_og,
    max_seqlen,
    decoding_seqlens=(1,),
    dtype=None,
    n_warmups=2,
):
    if cache is None:
        cache = DecodingCGCache()
    param_example = next(iter(model.parameters()))
    device = param_example.device
    if dtype is None:
        dtype = param_example.dtype
    if (
        (device, dtype) != (cache.device, cache.dtype)
        or batch_size > cache.max_batch_size
        or max_seqlen > cache.max_seqlen
    ):  # Invalidate the cache
        cache.callables = {}
        cache.mempool = None
        cache.inference_params = None
        gc.collect()
        cache.device, cache.dtype = device, dtype
        cache.max_batch_size, cache.max_seqlen = batch_size, max_seqlen
        assert hasattr(
            model, "allocate_inference_cache"
        ), "CUDA graph decoding requires that the model has a method allocate_inference_cache"
        inf_cache = model.allocate_inference_cache(batch_size, max_seqlen, dtype)
        lengths_per_sample = torch.full(
            (batch_size,), seqlen_og, dtype=torch.int32, device=device
        )
        cache.inference_params = InferenceParams(
            max_seqlen=max_seqlen,
            max_batch_size=batch_size,
            seqlen_offset=seqlen_og,
            key_value_memory_dict=inf_cache,
            lengths_per_sample=lengths_per_sample,
        )
        cache.mempool = torch.cuda.graphs.graph_pool_handle()
    for decoding_seqlen in decoding_seqlens:
        if (batch_size, decoding_seqlen) not in cache.callables:
            cache.callables[batch_size, decoding_seqlen] = capture_graph(
                model,
                cache.inference_params,
                batch_size,
                max_seqlen,
                decoding_seqlen=decoding_seqlen,
                mempool=cache.mempool,
                n_warmups=n_warmups,
            )

    def dispatch(input_ids, position_ids, seqlen):
        batch_size, decoding_seqlen = input_ids.shape[:2]
        return cache.callables[batch_size, decoding_seqlen](
            input_ids, position_ids, seqlen
        )

    cache.run = dispatch
    cache.inference_params.seqlen_offset = 0  # Reset so it's not confusing
    return cache


def capture_graph(
    model,
    inference_params,
    batch_size,
    max_seqlen,
    decoding_seqlen=1,
    mempool=None,
    n_warmups=2,
):
    device = next(iter(model.parameters())).device
    input_ids = torch.full(
        (batch_size, decoding_seqlen), 0, dtype=torch.long, device=device
    )
    position_ids = torch.full(
        (batch_size, decoding_seqlen), 0, dtype=torch.long, device=device
    )
    seqlen_offset_og = inference_params.seqlen_offset
    inference_params.seqlen_offset = max_seqlen - decoding_seqlen
    inference_params.lengths_per_sample[:] = inference_params.seqlen_offset

    # Warmup before capture
    s = torch.cuda.Stream()
    s.wait_stream(torch.cuda.current_stream())
    with torch.cuda.stream(s):
        for _ in range(n_warmups):
            logits = model(
                input_ids,
                position_ids=position_ids,
                inference_params=inference_params,
                num_last_tokens=decoding_seqlen,
            ).logits
        s.synchronize()
        # This might be needed for correctness if we run with NCCL_GRAPH_MIXING_SUPPORT=0,
        # which requires that graph launch and non-captured launch to not overlap (I think,
        # that's how I interpret the documentation). I'm not sure if this is required.
        if torch.distributed.is_initialized():
            torch.distributed.barrier()
    torch.cuda.current_stream().wait_stream(s)
    # Captures the graph
    # To allow capture, automatically sets a side stream as the current stream in the context
    graph = torch.cuda.CUDAGraph()
    with torch.cuda.graph(graph, pool=mempool):
        logits = model(
            input_ids,
            position_ids=position_ids,
            inference_params=inference_params,
            num_last_tokens=decoding_seqlen,
        ).logits

    def run(new_input_ids, new_position_ids, seqlen):
        inference_params.lengths_per_sample[:] = seqlen
        input_ids.copy_(new_input_ids)
        position_ids.copy_(new_position_ids)
        graph.replay()
        return logits.clone()

    inference_params.seqlen_offset = seqlen_offset_og
    return run



================================================
File: sauron/parse/argparse.py
================================================
import argparse


def get_args():
    parser = argparse.ArgumentParser(
        description="Configurations for Whole Slide Image (WSI) Training",
        formatter_class=argparse.ArgumentDefaultsHelpFormatter,
    )

    argument_groups_config = [
        (
            "Data & I/O Configuration",
            [
                (
                    ("--data_root_dir",),
                    {
                        "type": str,
                        "default": None,
                        "help": "Specify the root directory where the dataset is located. This is essential for loading the data correctly.",
                    },
                ),
                (
                    ("--results_dir",),
                    {
                        "default": "./results",
                        "help": "Path to the directory where training results and model checkpoints will be saved. Default is './results'.",
                    },
                ),
                (
                    ("--split_dir",),
                    {
                        "type": str,
                        "default": None,
                        "help": "Path to the directory containing custom data splits. If not provided, splits will be generated based on the task and label fraction.",
                    },
                ),
                (
                    ("--patch_size",),
                    {
                        "type": str,
                        "default": "",
                        "help": "Define the size of image patches in the format [height]x[width]. This is important for processing images.",
                    },
                ),
                (
                    ("--resolution",),
                    {
                        "type": str,
                        "default": "20x",
                        "help": "Set the magnification level for processing images. Examples include '10x' or '10x_40x' for combined levels.",
                    },
                ),
                (
                    ("--early_fusion",),
                    {
                        "action": "store_true",
                        "default": False,
                        "help": "Enable or disable early fusion for models that utilize multiple magnification levels. This can enhance model performance.",
                    },
                ),
                (
                    ("--preloading",),
                    {
                        "choices": ["yes", "no"],
                        "default": "no",
                        "help": "Specify whether to preload data into memory for faster access during training. Options are 'yes' or 'no'.",
                    },
                ),
            ],
        ),
        (
            "Training Hyperparameters",
            [
                (
                    ("--max_epochs",),
                    {
                        "type": int,
                        "default": 200,
                        "help": "Set the maximum number of epochs for training the model. Default is 200.",
                    },
                ),
                (
                    ("--lr",),
                    {
                        "type": float,
                        "default": 1e-4,
                        "help": "Initial learning rate for the optimizer. Adjust this for better convergence.",
                    },
                ),
                (
                    ("--reg",),
                    {
                        "type": float,
                        "default": 1e-5,
                        "help": "Weight decay factor for L2 regularization. Helps prevent overfitting.",
                    },
                ),
                (
                    ("--opt",),
                    {
                        "choices": ["adam", "sgd", "adamw"],
                        "default": "adam",
                        "help": "Choose the optimizer to use for training. Options include 'adam', 'sgd', or 'adamw'.",
                    },
                ),
                (
                    ("--drop_out",),
                    {
                        "type": float,
                        "default": 0.25,
                        "help": "Set the dropout probability to prevent overfitting during training.",
                    },
                ),
                (
                    ("--early_stopping",),
                    {
                        "action": "store_true",
                        "help": "Enable early stopping to halt training when validation performance stops improving.",
                    },
                ),
                (
                    ("--weighted_sample",),
                    {
                        "action": "store_true",
                        "help": "Enable weighted sampling to address class imbalance in the training dataset.",
                    },
                ),
                (
                    ("--batch_size",),
                    {
                        "type": int,
                        "default": 1,
                        "help": "Set the batch size for training.",
                    },
                ),
            ],
        ),
        (
            "Model Configuration",
            [
                (
                    ("--model_type",),
                    {
                        "type": str,
                        "default": "att_mil",
                        "help": "Specify the type of model architecture to use for training. Default is 'att_mil'.",
                    },
                ),
                (
                    ("--backbone",),
                    {
                        "type": str,
                        "default": "resnet50",
                        "help": "Select the backbone network for feature extraction. Default is 'resnet50'.",
                    },
                ),
                (
                    ("--in_dim",),
                    {
                        "type": int,
                        "default": 1024,
                        "help": "Set the input dimension for the model. This should match the output of the backbone network.",
                    },
                ),
            ],
        ),
        (
            "MambaMIL Specific Configuration",  # Conditionally relevant if model_type is MambaMIL
            [
                (
                    ("--mambamil_rate",),
                    {
                        "type": int,
                        "default": 10,
                        "help": "Rate parameter for MambaMIL, influencing the model's behavior.",
                    },
                ),
                (
                    ("--mambamil_layer",),
                    {
                        "type": int,
                        "default": 2,
                        "help": "Number of layers in the MambaMIL architecture.",
                    },
                ),
                (
                    ("--mambamil_type",),
                    {
                        "choices": ["Mamba", "BiMamba", "SRMamba"],
                        "default": "SRMamba",
                        "help": "Select the type of Mamba architecture to use. Options include 'Mamba', 'BiMamba', or 'SRMamba'.",
                    },
                ),
            ],
        ),
        (
            "Experiment & Reproducibility",
            [
                (
                    ("--task",),
                    {
                        "type": str,
                        "required": True,
                        "help": "Specify the task name or identifier for the experiment.",
                    },
                ),
                (
                    ("--task_type",),
                    {
                        "type": str,
                        "required": True,
                        "help": "Specify the task type ('classification' or 'survival').",
                    },
                ),
                (
                    ("--exp_code",),
                    {
                        "type": str,
                        "required": True,
                        "help": "Provide a unique experiment code for tracking purposes.",
                    },
                ),
                (
                    ("--seed",),
                    {
                        "type": int,
                        "default": 1,
                        "help": "Set the random seed for reproducibility of results. Default is 1.",
                    },
                ),
                (
                    ("--label_frac",),
                    {
                        "type": float,
                        "default": 1.0,
                        "help": "Specify the fraction of training labels to use. Default is 1.0 (use all labels).",
                    },
                ),
                (
                    ("--log_data",),
                    {
                        "action": "store_true",
                        "help": "Enable logging of training data using TensorBoard for visualization and analysis.",
                    },
                ),
                (
                    ("--testing",),
                    {
                        "action": "store_true",
                        "help": "Enable testing/debugging mode for the experiment.",
                    },
                ),
            ],
        ),
        (
            "Cross-Validation Configuration",
            [
                (
                    ("--k",),
                    {
                        "type": int,
                        "default": 10,
                        "help": "Specify the total number of folds for cross-validation. Default is 10.",
                    },
                ),
                (
                    ("--k_start",),
                    {
                        "type": int,
                        "default": -1,
                        "help": "Set the starting fold for cross-validation. Use -1 for the last fold.",
                    },
                ),
                (
                    ("--k_end",),
                    {
                        "type": int,
                        "default": -1,
                        "help": "Set the ending fold for cross-validation. Use -1 for the first fold.",
                    },
                ),
            ],
        ),
        (
            "Survival Configuration",
            [
                (
                    ("--bag_loss",),
                    {
                        "type": str,
                        "choices": ["svm", "ce", "ce_surv", "nll_surv", "cox_surv"],
                        "default": "nll_surv",
                        "help": "Slide-level classification loss function (default: nll_surv).",
                    },
                ),
                (
                    ("--alpha_surv",),
                    {
                        "type": float,
                        "default": 0.0,
                        "help": "How much to weigh uncensored patients.",
                    },
                ),
                (
                    ("--lambda_reg",),
                    {
                        "type": float,
                        "default": 1e-4,
                        "help": "L1-Regularization Strength (Default 1e-4).",
                    },
                ),
                (
                    ("--inst_loss",),
                    {
                        "type": str,
                        "choices": ["svm", "ce", None],
                        "default": None,
                        "help": "Instance-level clustering loss function (default: None).",
                    },
                ),
                (
                    ("--subtyping",),
                    {
                        "action": "store_true",
                        "default": False,
                        "help": "Enable subtyping problem.",
                    },
                ),
                (
                    ("--bag_weight",),
                    {
                        "type": float,
                        "default": 0.7,
                        "help": "Weight coefficient for bag-level loss (default: 0.7).",
                    },
                ),
                (
                    ("--B",),
                    {
                        "type": int,
                        "default": 8,
                        "help": "Number of positive/negative patches to sample for clam.",
                    },
                ),
                (
                    ("--gc",),
                    {
                        "type": int,
                        "default": 32,
                        "help": "Gradient Accumulation Step.",
                    },
                ),
            ],
        ),
    ]

    # Correcting the `early_fusion` argument based on best practices
    # Find and update early_fusion: argparse `type=bool` is problematic.
    # It's better to use `action='store_true'` or `action='store_false'`.
    # If `default=False`, use `action='store_true'`.
    for group_name, arg_list in argument_groups_config:
        for i, (name_or_flags, kwargs) in enumerate(arg_list):
            if name_or_flags == ("--early_fusion",):
                if "type" in kwargs and kwargs["type"] is bool:
                    del kwargs["type"]  # remove type=bool
                    if "default" in kwargs and kwargs["default"] is False:
                        kwargs["action"] = "store_true"
                    elif "default" in kwargs and kwargs["default"] is True:
                        kwargs["action"] = "store_false"
                    else:  # Default to store_true if not specified or ambiguous
                        kwargs["action"] = "store_true"
                    arg_list[i] = (name_or_flags, kwargs)  # Update the list
                break  # Found it

    for group_name, arg_definitions in argument_groups_config:
        group = parser.add_argument_group(group_name)
        for name_or_flags, kwargs in arg_definitions:
            group.add_argument(*name_or_flags, **kwargs)

    args = parser.parse_args()
    return args



================================================
File: sauron/preprocess/compute_embeddings.py
================================================
import os

import numpy as np
import torch
from torch.utils.data import DataLoader
from tqdm import tqdm

from sauron.data.dataloader import TileDataset
from sauron.utils.hdf5_utils import save_hdf5
from sauron.utils.load_encoders import get_encoder_class


def collate_features(batch):
    images = torch.stack([item[0] for item in batch], dim=0)
    coordinates = np.vstack([item[1] for item in batch])
    return images, coordinates


class TileEmbedder:
    def __init__(
        self,
        model_name="conch_ViT-B-16",
        model_repo="hf_hub:MahmoodLab/conch",
        target_patch_size=256,
        target_magnification=20,
        device="cuda",
        precision=None,
        save_directory=None,
        batch_size=64,
        num_workers=8,
    ):
        self.model_name = model_name
        self.model_repo = model_repo
        self.device = device
        self.save_directory = save_directory
        self.target_patch_size = target_patch_size
        self.target_magnification = target_magnification
        self.batch_size = batch_size
        self.num_workers = num_workers

        self.model, self.transform, self.model_precision = self._load_model()
        self.precision = precision if precision is not None else self.model_precision

        self.model.to(self.device)
        self.model.eval()

    def _load_model(self):
        model, transform, model_precision = get_encoder_class(self.model_name)
        return model, transform, model_precision

    def embed_tiles(self, whole_slide_image, tile_contours, slide_name) -> str:
        patch_save_path = os.path.join(
            self.save_directory, "patches", f"{slide_name}_patches.png"
        )
        embedding_save_path = os.path.join(
            self.save_directory, "patch_embeddings", f"{slide_name}.h5"
        )

        dataset = TileDataset(
            wsi=whole_slide_image,
            contours=tile_contours,
            target_patch_size=self.target_patch_size,
            target_magnification=self.target_magnification,
            eval_transform=self.transform,
            save_path=patch_save_path,
        )

        dataloader = DataLoader(
            dataset,
            batch_size=self.batch_size,
            shuffle=False,
            num_workers=self.num_workers,
            collate_fn=collate_features,
            pin_memory=True,
        )

        for batch_idx, (images, coordinates) in tqdm(
            enumerate(dataloader), total=len(dataloader), desc="Embedding tiles"
        ):
            images = images.to(self.device, non_blocking=True)
            with torch.inference_mode(), torch.amp.autocast(
                dtype=self.precision, device_type=self.device
            ):
                embeddings = self.model.encode_image(
                    images, proj_contrast=False, normalize=False
                )
            mode = "w" if batch_idx == 0 else "a"
            data_dict = {
                "features": embeddings.cpu().numpy(),
                "coordinates": coordinates,
            }
            save_hdf5(embedding_save_path, data_dict=data_dict, mode=mode)

        return embedding_save_path



================================================
File: sauron/preprocess/slide_embedder.py
================================================
import logging
import os
from pathlib import Path
from typing import List, Tuple

import openslide
from termcolor import colored
from tqdm import tqdm

# Assuming these are the correct locations and types for the sauron library
from sauron.preprocess.compute_embeddings import TileEmbedder
from sauron.utils.filehandler import PatientFolder
from sauron.utils.segmentation import TissueSegmenter
from sauron.utils.WSIObjects import OpenSlideWSI, get_pixel_size

# It's good practice to import GeoDataFrame if you are using it for type hints
# from geopandas import GeoDataFrame

# Configure logger
logging.basicConfig(
    level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s"
)
logger = logging.getLogger(__name__)


class SlideEmbedder:
    """Orchestrates the processing of whole-slide images (WSIs) to generate tile embeddings."""

    def __init__(
        self,
        slide_dir: Path | str,
        output_dir: Path | str,
        magnification: int,
        patch_size: int,
        batch_size: int,
        encoder: str,
    ):
        """
        Initializes the SlideEmbedder.

        Args:
            slide_dir (Path | str): Directory containing the WSI files.
            output_dir (Path | str): Root directory where processing outputs will be saved.
            magnification (int): The target magnification level (e.g., 20 for 20x).
            patch_size (int): The size of the square patches to extract, in pixels.
            batch_size (int): The batch size for processing tiles.
        """
        # IMPROVEMENT: Use pathlib.Path for modern, object-oriented path handling.
        self.slide_dir = Path(slide_dir)
        self.output_dir = Path(output_dir)
        self.magnification = magnification
        self.patch_size = patch_size
        self.batch_size = batch_size

    def process_slides(self) -> None:
        """
        Main method to find, process, and generate embeddings for all slides in the slide_dir.
        """
        try:
            slide_filenames = self._get_slide_filenames()
            if not slide_filenames:
                logger.warning(f"No slides found in {self.slide_dir}. Aborting.")
                return

            output_path = self._create_output_directories(len(slide_filenames))
            segmenter, tile_embedder = self._initialize_processors(
                output_path, self.encoder
            )

            for slide_filename in tqdm(slide_filenames, desc="Processing slides"):
                self._process_single_slide(slide_filename, segmenter, tile_embedder)

            logger.info(colored("Processing complete.", "green"))
        except Exception as e:
            logger.error(
                colored(f"A critical error occurred: {e}", "red"), exc_info=True
            )

    def _get_slide_filenames(self) -> List[str]:
        """Retrieves a list of slide filenames from the source directory."""
        patient_folder = PatientFolder(str(self.slide_dir))
        return patient_folder.get_data()

    def _create_output_directories(self, num_slides: int) -> Path:
        """
        Creates the necessary output directories for the processing run.

        Args:
            num_slides (int): The number of slides being processed, used for naming.

        Returns:
            Path: The path to the main output directory for this run.
        """
        # IMPROVEMENT: Cleaner and more maintainable directory naming.
        run_name = (
            f"slides_{num_slides}_mag_{self.magnification}x_patch_{self.patch_size}"
        )
        output_path = self.output_dir / run_name

        # Create all subdirectories in one go
        for subdir in ["segmentation", "patches", "patch_embeddings"]:
            (output_path / subdir).mkdir(parents=True, exist_ok=True)

        logger.info(f"Output will be saved to: {output_path}")
        return output_path

    def _initialize_processors(
        self, output_path: Path, encoder: str
    ) -> Tuple[TissueSegmenter, TileEmbedder]:
        """
        Initializes the tissue segmenter and tile embedder components.

        Args:
            output_path (Path): The base path for saving processor outputs.

        Returns:
            A tuple containing the initialized TissueSegmenter and TileEmbedder.
        """
        segmenter = TissueSegmenter(
            save_path=str(output_path / "segmentation"),
            batch_size=self.batch_size,
        )
        tile_embedder = TileEmbedder(
            model_name=encoder,
            target_patch_size=self.patch_size,
            target_mag=self.magnification,
            save_path=str(output_path),
        )
        return segmenter, tile_embedder

    def _process_single_slide(
        self,
        slide_filename: str,
        segmenter: TissueSegmenter,
        tile_embedder: TileEmbedder,
    ) -> None:
        """
        Processes a single WSI file: reads, segments, and embeds tiles.

        Args:
            slide_filename (str): The filename of the slide to process.
            segmenter (TissueSegmenter): The tissue segmentation processor.
            tile_embedder (TileEmbedder): The tile embedding processor.
        """
        try:
            logger.info(f"Processing slide: {slide_filename}")
            wsi = self._read_slide(slide_filename)
            pixel_size = get_pixel_size(wsi.img)
            # IMPROVEMENT: Use pathlib for cleaner file name manipulation.
            filename_no_extension = Path(slide_filename).stem

            gdf_contours = segmenter.segment_tissue(
                wsi=wsi,
                pixel_size=pixel_size,
                save_bn=filename_no_extension,
            )

            tile_embedder.embed_tiles(
                wsi=wsi,
                gdf_contours=gdf_contours,
                fn=filename_no_extension,
            )
        except openslide.OpenSlideError as e:
            logger.error(colored(f"Could not open slide {slide_filename}: {e}", "red"))
        except Exception as e:
            # IMPROVEMENT: exc_info=True provides a full traceback for better debugging.
            logger.error(
                colored(f"Error processing slide {slide_filename}: {e}", "red"),
                exc_info=True,
            )

    def _read_slide(self, slide_filename: str) -> OpenSlideWSI:
        """
        Opens a slide file using OpenSlide.

        Args:
            slide_filename (str): The filename of the slide.

        Returns:
            OpenSlideWSI: The wrapper object for the opened slide.
        """
        slide_path = self.slide_dir / slide_filename
        return OpenSlideWSI(openslide.OpenSlide(str(slide_path)))



================================================
File: sauron/training/__init__.py
================================================



================================================
File: sauron/training/lightning_module.py
================================================
# lightning_module.py

# sauron/training/lightning_module.py (New File)

import os
from typing import Any, Dict, List, Optional, Tuple, Union

import numpy as np
import pytorch_lightning as pl
import torch
import torch.nn as nn
from torch.optim.lr_scheduler import ReduceLROnPlateau

from sauron.losses.surv_loss import (  # Assuming these are accessible
    CoxSurvLoss,
    CrossEntropySurvLoss,
    NLLSurvLoss,
)
from sauron.utils.generic_utils import calculate_error  # Assuming accessible
from sauron.utils.metrics import (  # Assuming accessible
    _calculate_classification_auc,
    _calculate_survival_c_index,
)
from sauron.utils.optimizers import get_optim  # Assuming accessible


class SauronLightningModule(pl.LightningModule):
    def __init__(self, model: nn.Module, args: Any):
        super().__init__()
        self.model = model
        # Use save_hyperparameters to make args accessible via self.hparams
        # and log them. args should be a Namespace or a dict.
        self.save_hyperparameters(
            args, logger=True
        )  # logger=True to log them with PL loggers

        self.loss_fn = self._get_loss_fn()

        self.train_survival_loss_alpha = (
            self.hparams.alpha_surv
            if self.hparams.task_type == "survival"
            and hasattr(self.hparams, "alpha_surv")
            else 0.0
        )
        # For evaluation, Cox and NLL/CE typically use alpha=0 for loss calculation
        # unless specifically designed otherwise. The original code used 0.0.
        self.eval_survival_loss_alpha = 0.0

        self.lambda_reg = getattr(self.hparams, "lambda_reg", 0.0)
        # L1 regularization example (can be adapted for L2)
        self.reg_type = getattr(self.hparams, "reg_type", None)

        # To store patient-level results from the test set
        self.test_patient_results = []  # List to store per-batch results
        self.test_patient_results_aggregated: Optional[Dict[str, Any]] = None

    def _get_loss_fn(self) -> nn.Module:
        """Initializes the loss function based on hparams."""
        task_type = self.hparams.task_type
        if task_type == "classification":
            return nn.CrossEntropyLoss()
        elif task_type == "survival":
            alpha_surv = getattr(self.hparams, "alpha_surv", 0.0)  # Used for training
            bag_loss = self.hparams.bag_loss
            if bag_loss == "ce_surv":
                return CrossEntropySurvLoss(
                    alpha=alpha_surv
                )  # alpha here is for training
            elif bag_loss == "nll_surv":
                return NLLSurvLoss(alpha=alpha_surv)  # alpha here is for training
            elif bag_loss == "cox_surv":
                return (
                    CoxSurvLoss()
                )  # Cox usually doesn't use alpha in its primary formulation
            else:
                raise NotImplementedError(f"Survival loss {bag_loss} not implemented.")
        else:
            raise ValueError(f"Invalid task_type: {task_type}")

    def forward(self, x: torch.Tensor) -> Tuple:
        """Forward pass through the model."""
        return self.model(x)

    def _unpack_batch_data(
        self, batch_data: Tuple
    ) -> Tuple[
        torch.Tensor, torch.Tensor, Optional[Any], Optional[torch.Tensor], Optional[Any]
    ]:
        """Unpacks batch data. Returns (model_input, label, event_time_cpu, censorship_gpu, slide_id (if available))."""
        slide_id = None
        if self.hparams.task_type == "classification":
            if len(batch_data) == 2:
                data, label = batch_data
            elif len(batch_data) == 3:  # Assuming (data, label, slide_id)
                data, label, slide_id = batch_data
            else:
                raise ValueError(
                    f"Unexpected batch data length for classification: {len(batch_data)}"
                )
            # Device placement is handled by Lightning for batch tensors
            return data, label, None, None, slide_id

        elif self.hparams.task_type == "survival":
            if len(batch_data) == 5:  # data_WSI, _omic, label, event_time, c
                data_WSI, _omic, label, event_time, c = batch_data
            elif (
                len(batch_data) == 6
            ):  # data_WSI, _omic, label, event_time, c, slide_id
                data_WSI, _omic, label, event_time, c, slide_id = batch_data
            else:
                raise ValueError(
                    f"Unexpected batch data length for survival: {len(batch_data)}"
                )

            event_time_cpu = (
                event_time.cpu().numpy()
                if isinstance(event_time, torch.Tensor)
                else event_time
            )
            return data_WSI, label, event_time_cpu, c, slide_id
        else:
            raise ValueError(f"Unknown task_type: {self.hparams.task_type}")

    def _calculate_main_loss(
        self,
        model_outputs: Tuple,
        label: torch.Tensor,
        c_tensor: Optional[torch.Tensor],
        current_survival_alpha: float,
    ) -> torch.Tensor:
        """Calculates the main loss component."""
        if self.hparams.task_type == "classification":
            logits, _, _, _, _ = model_outputs  # Assuming model output structure
            return self.loss_fn(logits, label)
        elif self.hparams.task_type == "survival":
            hazards, S, _, _, _ = model_outputs  # Assuming model output structure
            # Pass alpha to survival loss function. The loss function itself
            # should handle how alpha is used (e.g., NLLSurvLoss).
            # CoxSurvLoss might ignore alpha.
            if isinstance(self.loss_fn, (NLLSurvLoss, CrossEntropySurvLoss)):
                return self.loss_fn(
                    hazards=hazards,
                    S=S,
                    Y=label,
                    c=c_tensor,
                    alpha=current_survival_alpha,
                )
            elif isinstance(self.loss_fn, CoxSurvLoss):
                return self.loss_fn(
                    hazards=hazards, S=S, Y=label, c=c_tensor
                )  # Cox might not take alpha
            else:  # Fallback for other survival losses that might not have alpha explicitly
                return self.loss_fn(hazards=hazards, S=S, Y=label, c=c_tensor)

        raise ValueError(
            f"Unknown task_type for loss calculation: {self.hparams.task_type}"
        )

    def _get_regularization_loss(self) -> torch.Tensor:
        """Calculates L1 or L2 regularization loss if enabled."""
        reg_loss = torch.tensor(0.0, device=self.device)
        if self.lambda_reg > 0 and self.reg_type:
            if self.reg_type.lower() == "l1":
                for param in self.model.parameters():
                    if param.requires_grad:
                        reg_loss += torch.sum(torch.abs(param))
            elif self.reg_type.lower() == "l2":
                for param in self.model.parameters():
                    if param.requires_grad:
                        reg_loss += torch.sum(param.pow(2))
            else:
                # warnings.warn(f"Unsupported regularization type: {self.reg_type}")
                pass  # Or raise error
        return reg_loss * self.lambda_reg

    def _common_step(self, batch: Tuple, batch_idx: int, stage: str) -> Dict[str, Any]:
        """Common logic for training, validation, and test steps."""
        model_input, label_gpu, event_time_cpu, c_gpu, slide_id_b = (
            self._unpack_batch_data(batch)
        )

        model_outputs = self(model_input)  # Calls forward

        current_alpha = (
            self.train_survival_loss_alpha
            if stage == "train"
            else self.eval_survival_loss_alpha
        )
        main_loss = self._calculate_main_loss(
            model_outputs, label_gpu, c_gpu, current_alpha
        )

        output_dict = {"main_loss": main_loss, "slide_ids": slide_id_b}

        if self.hparams.task_type == "classification":
            logits, y_prob, y_hat, _, _ = model_outputs
            output_dict.update(
                {
                    "preds": y_hat.detach(),  # For error calculation
                    "probs": y_prob.detach(),  # For AUC
                    "labels": label_gpu.detach(),
                    "logits_batch": logits.detach(),  # For patient results
                }
            )
        elif self.hparams.task_type == "survival":
            hazards, S, _, _, _ = model_outputs
            risk = -torch.sum(
                S, dim=1
            ).detach()  # Higher S sum means lower risk (longer survival)
            output_dict.update(
                {
                    "risks": risk,
                    "labels": label_gpu.detach(),  # Discrete time bin
                    "censorships": c_gpu.detach(),
                    "event_times": event_time_cpu,  # Already numpy/list on CPU
                    "hazards_batch": hazards.detach(),  # For patient results
                    "S_batch": S.detach(),  # For patient results
                }
            )
        return output_dict

    def training_step(self, batch: Tuple, batch_idx: int) -> torch.Tensor:
        step_output = self._common_step(batch, batch_idx, stage="train")
        main_loss = step_output["main_loss"]

        reg_loss = self._get_regularization_loss()
        total_loss = main_loss + reg_loss

        self.log(
            "train/loss_main",
            main_loss,
            on_step=True,
            on_epoch=True,
            prog_bar=False,
            logger=True,
        )
        if self.lambda_reg > 0 and self.reg_type:
            self.log(
                "train/loss_reg",
                reg_loss,
                on_step=True,
                on_epoch=True,
                prog_bar=False,
                logger=True,
            )
        self.log(
            "train/loss_total",
            total_loss,
            on_step=True,
            on_epoch=True,
            prog_bar=True,
            logger=True,
        )

        # Return loss for PL, and other items needed for epoch_end
        step_output["loss"] = total_loss  # PL needs 'loss' key for backprop
        return step_output

    def _aggregate_epoch_outputs(
        self, outputs: List[Dict[str, Any]], stage: str
    ) -> Dict[str, Any]:
        """Aggregates outputs from all steps in an epoch."""
        agg_data = {
            "avg_loss": torch.stack([x["main_loss"] for x in outputs]).mean().item()
        }  # Avg main loss

        if self.hparams.task_type == "classification":
            all_preds = torch.cat([x["preds"] for x in outputs]).cpu().numpy()
            all_probs = torch.cat([x["probs"] for x in outputs]).cpu().numpy()
            all_labels = torch.cat([x["labels"] for x in outputs]).cpu().numpy()
            agg_data.update(
                {
                    "all_preds": all_preds,
                    "all_probs": all_probs,
                    "all_labels": all_labels,
                }
            )
            if stage == "test" and getattr(
                self.hparams, "collect_patient_results_on_test", False
            ):
                all_logits = (
                    torch.cat([x["logits_batch"] for x in outputs]).cpu().numpy()
                )
                all_slide_ids_flat = [
                    item
                    for sublist in [
                        x["slide_ids"] for x in outputs if x["slide_ids"] is not None
                    ]
                    for item in (sublist if isinstance(sublist, list) else [sublist])
                ]
                agg_data.update(
                    {
                        "all_logits_test": all_logits,
                        "all_slide_ids_test": all_slide_ids_flat,
                    }
                )

        elif self.hparams.task_type == "survival":
            all_risks = torch.cat([x["risks"] for x in outputs]).cpu().numpy()
            all_labels = (
                torch.cat([x["labels"] for x in outputs]).cpu().numpy()
            )  # discrete time bins
            all_censorships = (
                torch.cat([x["censorships"] for x in outputs]).cpu().numpy()
            )
            # event_times are already numpy/list of lists
            all_event_times = (
                np.concatenate([x["event_times"] for x in outputs])
                if outputs and outputs[0]["event_times"] is not None
                else np.array([])
            )

            agg_data.update(
                {
                    "all_risks": all_risks,
                    "all_labels": all_labels,
                    "all_censorships": all_censorships,
                    "all_event_times": all_event_times,
                }
            )
            if stage == "test" and getattr(
                self.hparams, "collect_patient_results_on_test", False
            ):
                all_hazards = (
                    torch.cat([x["hazards_batch"] for x in outputs]).cpu().numpy()
                )
                all_S = torch.cat([x["S_batch"] for x in outputs]).cpu().numpy()
                all_slide_ids_flat = [
                    item
                    for sublist in [
                        x["slide_ids"] for x in outputs if x["slide_ids"] is not None
                    ]
                    for item in (sublist if isinstance(sublist, list) else [sublist])
                ]
                agg_data.update(
                    {
                        "all_hazards_test": all_hazards,
                        "all_S_test": all_S,
                        "all_slide_ids_test": all_slide_ids_flat,
                    }
                )

        return agg_data

    def _log_epoch_metrics(self, agg_data: Dict[str, Any], stage: str):
        """Logs metrics at the end of an epoch."""
        self.log(
            f"{stage}/loss",
            agg_data["avg_loss"],
            on_epoch=True,
            prog_bar=True,
            logger=True,
        )

        if self.hparams.task_type == "classification":
            epoch_error = calculate_error(
                torch.from_numpy(agg_data["all_preds"]),
                torch.from_numpy(agg_data["all_labels"]),
            )
            self.log(
                f"{stage}/error", epoch_error, on_epoch=True, prog_bar=True, logger=True
            )

            if agg_data["all_labels"].size > 0 and agg_data["all_probs"].size > 0:
                epoch_auc = _calculate_classification_auc(
                    agg_data["all_labels"],
                    agg_data["all_probs"],
                    self.hparams.n_classes,
                )
                self.log(
                    f"{stage}/auc", epoch_auc, on_epoch=True, prog_bar=True, logger=True
                )
                self.log(
                    f"{stage}_metric", epoch_auc, on_epoch=True
                )  # For EarlyStopping/ModelCheckpoint if monitoring AUC

                # Per-class accuracy (mimicking AccuracyLogger)
                for i in range(self.hparams.n_classes):
                    class_preds = agg_data["all_preds"][agg_data["all_labels"] == i]
                    class_labels = agg_data["all_labels"][
                        agg_data["all_labels"] == i
                    ]  # Should all be i
                    if len(class_labels) > 0:
                        correct_count = np.sum(
                            class_preds == i
                        )  # Assuming preds are class indices
                        acc = correct_count / len(class_labels)
                        self.log(
                            f"{stage}/class_{i}_acc", acc, on_epoch=True, logger=True
                        )
                        # print(f"    {stage.capitalize()} Class {i}: acc {acc:.4f} ({correct_count}/{len(class_labels)})")

        elif self.hparams.task_type == "survival":
            if len(agg_data["all_event_times"]) > 0:
                epoch_c_index = _calculate_survival_c_index(
                    agg_data["all_event_times"],
                    agg_data["all_censorships"],
                    agg_data["all_risks"],
                )
                self.log(
                    f"{stage}/c_index",
                    epoch_c_index,
                    on_epoch=True,
                    prog_bar=True,
                    logger=True,
                )
                self.log(
                    f"{stage}_metric", epoch_c_index, on_epoch=True
                )  # For EarlyStopping/ModelCheckpoint
            else:
                self.log(
                    f"{stage}/c_index",
                    np.nan,
                    on_epoch=True,
                    prog_bar=True,
                    logger=True,
                )
                self.log(f"{stage}_metric", np.nan, on_epoch=True)

    def training_epoch_end(self, outputs: List[Dict[str, Any]]):
        agg_data = self._aggregate_epoch_outputs(outputs, stage="train")
        self._log_epoch_metrics(agg_data, stage="train")
        # Log learning rate
        lr = self.optimizers().param_groups[0]["lr"]
        self.log("lr", lr, on_epoch=True, logger=True)

    def validation_step(self, batch: Tuple, batch_idx: int) -> Dict[str, Any]:
        step_output = self._common_step(batch, batch_idx, stage="val")
        self.log(
            "val/loss_step",
            step_output["main_loss"],
            on_step=True,
            on_epoch=False,
            logger=True,
        )
        return step_output

    def validation_epoch_end(self, outputs: List[Dict[str, Any]]):
        agg_data = self._aggregate_epoch_outputs(outputs, stage="val")
        self._log_epoch_metrics(agg_data, stage="val")
        # PL uses 'val_loss' by default for some callbacks, or you can specify 'val_metric'
        self.log(
            "val_loss", agg_data["avg_loss"], on_epoch=True, prog_bar=True
        )  # Ensure val_loss is logged for schedulers/callbacks

    def test_step(self, batch: Tuple, batch_idx: int) -> Dict[str, Any]:
        step_output = self._common_step(batch, batch_idx, stage="test")
        if (
            getattr(self.hparams, "collect_patient_results_on_test", False)
            and step_output["slide_ids"] is not None
        ):
            # This collects raw outputs per batch. Aggregation into dict happens in test_epoch_end
            self.test_patient_results.append(step_output)
        return step_output

    def test_epoch_end(self, outputs: List[Dict[str, Any]]):
        agg_data = self._aggregate_epoch_outputs(outputs, stage="test")
        self._log_epoch_metrics(agg_data, stage="test")

        if (
            getattr(self.hparams, "collect_patient_results_on_test", False)
            and self.test_patient_results
        ):
            # Aggregate collected patient results into the desired dictionary format
            # This is a simplified aggregation; adjust as needed for the exact structure.
            self.test_patient_results_aggregated = {}
            num_samples = len(agg_data.get("all_slide_ids_test", []))

            for i in range(num_samples):
                slide_id = str(agg_data["all_slide_ids_test"][i])
                if self.hparams.task_type == "classification":
                    self.test_patient_results_aggregated[slide_id] = {
                        "slide_id": slide_id,
                        "prob": agg_data["all_probs"][i].tolist(),
                        "pred": int(agg_data["all_preds"][i]),
                        "label": int(agg_data["all_labels"][i]),
                        "logits": agg_data["all_logits_test"][i].tolist(),
                    }
                elif self.hparams.task_type == "survival":
                    self.test_patient_results_aggregated[slide_id] = {
                        "slide_id": slide_id,
                        "risk": float(agg_data["all_risks"][i]),
                        "disc_label": int(
                            agg_data["all_labels"][i]
                        ),  # discrete time bin label
                        "survival_time": float(
                            agg_data["all_event_times"][i]
                        ),  # actual event time
                        "censorship": int(agg_data["all_censorships"][i]),
                        "hazards": agg_data["all_hazards_test"][i].tolist(),
                        "S": agg_data["all_S_test"][i].tolist(),
                    }
            # Clear the per-batch list after aggregation
            self.test_patient_results = []
            print(
                f"Aggregated {len(self.test_patient_results_aggregated)} patient results in test_epoch_end."
            )

    def configure_optimizers(self) -> Union[torch.optim.Optimizer, Tuple[List, List]]:
        # optimizer = torch.optim.Adam(self.parameters(), lr=self.hparams.lr, weight_decay=self.hparams.weight_decay)
        optimizer = get_optim(
            self.model, self.hparams
        )  # Assuming get_optim takes model and hparams (args)

        if (
            hasattr(self.hparams, "lr_scheduler_name")
            and self.hparams.lr_scheduler_name
        ):
            if self.hparams.lr_scheduler_name.lower() == "plateau":
                scheduler = ReduceLROnPlateau(
                    optimizer,
                    mode=getattr(
                        self.hparams, "lr_scheduler_mode", "min"
                    ),  # 'min' for val_loss, 'max' for val_metric
                    factor=getattr(self.hparams, "lr_scheduler_factor", 0.5),
                    patience=getattr(self.hparams, "lr_scheduler_patience", 10),
                    verbose=True,
                )
                return {
                    "optimizer": optimizer,
                    "lr_scheduler": {
                        "scheduler": scheduler,
                        "monitor": "val_loss"
                        if getattr(self.hparams, "lr_scheduler_mode", "min") == "min"
                        else "val_metric",  # or "val_metric"
                        "interval": "epoch",
                        "frequency": 1,
                    },
                }
            # Add other schedulers like CosineAnnealingLR here
            # elif self.hparams.lr_scheduler_name.lower() == 'cosine':
            #     scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=self.hparams.max_epochs) # Ensure max_epochs is in hparams
            #     return [optimizer], [scheduler]

        return optimizer



================================================
File: sauron/training/pipeline.py
================================================
import os
from typing import Any, Dict, List, Optional, Tuple

import pytorch_lightning as pl
import torch
from pytorch_lightning.callbacks import (
    Callback,
    EarlyStopping,
    ModelCheckpoint,
    TQDMProgressBar,
)
from pytorch_lightning.loggers import Logger, TensorBoardLogger
from torch.utils.data import DataLoader

from sauron.data.data_utils import get_dataloader
from sauron.mil_models.models_factory import initialize_mil_model
from sauron.training.lightning_module import SauronLightningModule

# --- Constants for default values (if not in args) ---
DEFAULT_NUM_WORKERS = 4
DEFAULT_VAL_TEST_BATCH_SIZE = 1
DEFAULT_ES_PATIENCE = 20
DEFAULT_GRAD_ACCUM_STEPS = 1
DEFAULT_LOG_EVERY_N_STEPS = 50
DEFAULT_TQDM_REFRESH_DIVISOR = 100


def _setup_environment_and_logger(
    base_results_dir: str, current_fold_num: int, log_data_flag: bool
) -> Tuple[str, Optional[Logger]]:
    """Sets up the results directory for the current fold and initializes the TensorBoardLogger."""
    fold_results_dir = os.path.join(base_results_dir, str(current_fold_num))
    os.makedirs(fold_results_dir, exist_ok=True)

    logger = None
    if log_data_flag:
        logger = TensorBoardLogger(
            save_dir=base_results_dir,
            name="",  # No subdirectory for model name, version handles fold
            version=str(current_fold_num),
            default_hp_metric=False,  # Recommended by PyTorch Lightning
        )
    print(f"Results directory for fold {current_fold_num}: {fold_results_dir}")
    return fold_results_dir, logger


def _initialize_dataloaders(
    train_dataset: Any,
    val_dataset: Any,
    test_dataset: Optional[Any],
    args: Any,
) -> Tuple[DataLoader, DataLoader, Optional[DataLoader]]:
    """Initializes train, validation, and optional test DataLoaders."""
    val_test_batch_size = getattr(
        args, "val_test_batch_size", DEFAULT_VAL_TEST_BATCH_SIZE
    )
    num_workers = getattr(args, "num_workers", DEFAULT_NUM_WORKERS)

    train_loader = get_dataloader(
        train_dataset,
        training=True,
        weighted=args.weighted_sample,
        batch_size=args.batch_size,
        num_workers=num_workers,
    )
    val_loader = get_dataloader(
        val_dataset,
        training=False,
        batch_size=val_test_batch_size,
        num_workers=num_workers,
    )
    test_loader = (
        get_dataloader(
            test_dataset,
            training=False,
            batch_size=val_test_batch_size,
            num_workers=num_workers,
        )
        if test_dataset
        else None
    )

    print(f"Train dataset size: {len(train_dataset)}")
    print(f"Validation dataset size: {len(val_dataset)}")
    if test_dataset:
        print(f"Test dataset size: {len(test_dataset)}")
    return train_loader, val_loader, test_loader


def _initialize_lightning_module(args: Any) -> SauronLightningModule:
    """Initializes the PyTorch model and wraps it with SauronLightningModule."""
    pytorch_model = initialize_mil_model(args)
    lightning_module = SauronLightningModule(model=pytorch_model, args=args)
    return lightning_module


def _configure_callbacks(
    args: Any, fold_results_dir: str, train_loader_len: int
) -> Tuple[List[Callback], ModelCheckpoint]:
    """Configures and returns PyTorch Lightning callbacks."""
    callbacks_list: List[Callback] = []

    # Determine monitor metric and mode for EarlyStopping and ModelCheckpoint
    monitor_metric_name = "val_loss"
    monitor_mode = "min"
    filename_template = "best_model-{epoch}-{val_loss:.4f}"

    if args.task_type == "classification":
        if (
            getattr(args, "monitor_metric", "loss").lower() == "metric"
        ):  # 'metric' typically AUC for classification
            monitor_metric_name = "val/auc"
            monitor_mode = "max"
            filename_template = "best_model-{epoch}-{val/auc:.4f}"
    elif args.task_type == "survival":  # For survival, 'metric' is C-Index
        if getattr(args, "monitor_metric", "loss").lower() == "metric":
            monitor_metric_name = "val/c_index"
            monitor_mode = "max"
            filename_template = "best_model-{epoch}-{val/c_index:.4f}"
        # If monitor_metric is 'loss' for survival, default val_loss and min mode are already set

    if args.early_stopping:
        early_stopping_callback = EarlyStopping(
            monitor=monitor_metric_name,
            patience=getattr(args, "es_patience", DEFAULT_ES_PATIENCE),
            verbose=True,
            mode=monitor_mode,
        )
        callbacks_list.append(early_stopping_callback)
        print(
            f"Early stopping enabled: monitor='{monitor_metric_name}', mode='{monitor_mode}'"
        )

    checkpoint_callback = ModelCheckpoint(
        dirpath=os.path.join(fold_results_dir, "checkpoints"),
        filename=filename_template,
        monitor=monitor_metric_name,
        mode=monitor_mode,
        save_top_k=1,
        save_last=True,
        verbose=True,
    )
    callbacks_list.append(checkpoint_callback)
    print(
        f"Model checkpointing enabled: monitor='{monitor_metric_name}', mode='{monitor_mode}'"
    )

    # TQDM Progress Bar
    refresh_rate = max(
        1,
        train_loader_len // DEFAULT_TQDM_REFRESH_DIVISOR if train_loader_len > 0 else 1,
    )
    callbacks_list.append(TQDMProgressBar(refresh_rate=refresh_rate))

    return callbacks_list, checkpoint_callback


def _configure_trainer(
    args: Any,
    logger: Optional[Logger],
    callbacks: List[Callback],
    train_loader_len: int,
) -> pl.Trainer:
    """Configures and returns the PyTorch Lightning Trainer."""
    grad_accum_steps = getattr(args, "gc", DEFAULT_GRAD_ACCUM_STEPS)

    trainer_params = {
        "logger": logger,
        "callbacks": callbacks,
        "max_epochs": args.max_epochs,
        "accelerator": "gpu" if torch.cuda.is_available() else "cpu",
        "devices": "auto"
        if torch.cuda.is_available()
        else 1,  # Simpler device selection
        "accumulate_grad_batches": grad_accum_steps,
        "deterministic": getattr(args, "deterministic", False),
        "log_every_n_steps": min(
            DEFAULT_LOG_EVERY_N_STEPS,
            train_loader_len // grad_accum_steps
            if train_loader_len > 0 and grad_accum_steps > 0
            else DEFAULT_LOG_EVERY_N_STEPS,
        ),
        # "precision": "16-mixed" if getattr(args, "amp", False) else 32, # Updated precision flag
    }
    if getattr(args, "amp", False):  # Automatic Mixed Precision
        trainer_params["precision"] = "16-mixed"

    if hasattr(args, "gradient_clip_val") and args.gradient_clip_val is not None:
        trainer_params["gradient_clip_val"] = args.gradient_clip_val
        trainer_params["gradient_clip_algorithm"] = getattr(
            args, "gradient_clip_algorithm", "norm"
        )

    return pl.Trainer(**trainer_params)


def _run_final_evaluation(
    trainer: pl.Trainer,
    lightning_module: SauronLightningModule,
    test_loader: Optional[DataLoader],
    checkpoint_callback: ModelCheckpoint,
    fold_results_dir: str,
) -> Tuple[Optional[Dict[str, Any]], Optional[Dict[str, Any]]]:
    """Runs final evaluation on the test set using the best model checkpoint."""
    test_metrics = None
    patient_results_dict = None

    best_model_path = checkpoint_callback.best_model_path

    # Fallback to last checkpoint if best_model_path is empty or doesn't exist
    if not best_model_path or not os.path.exists(best_model_path):
        if best_model_path:  # It was set but file is missing
            print(f"Warning: Best model path '{best_model_path}' does not exist.")

        last_ckpt_path = os.path.join(fold_results_dir, "checkpoints", "last.ckpt")
        if os.path.exists(last_ckpt_path):
            print(f"Attempting to use last checkpoint: '{last_ckpt_path}'")
            best_model_path = last_ckpt_path
        else:
            print(
                "Warning: No best or last checkpoint found. Skipping test evaluation."
            )
            return test_metrics, patient_results_dict

    if test_loader:
        print(f"\n--- Final Test Evaluation (using model from: {best_model_path}) ---")
        trainer.test(
            model=lightning_module, dataloaders=test_loader, ckpt_path=best_model_path
        )

        current_test_metrics = {}
        for key, value in trainer.callback_metrics.items():
            if key.startswith("test/"):
                metric_name = key.replace("test/", "")
                current_test_metrics[metric_name] = (
                    value.item() if isinstance(value, torch.Tensor) else value
                )
        test_metrics = current_test_metrics

        if (
            hasattr(lightning_module, "test_patient_results_aggregated")
            and lightning_module.test_patient_results_aggregated
        ):
            patient_results_dict = lightning_module.test_patient_results_aggregated
            # Optionally save patient results to a file here if desired
            # import json
            # patient_results_path = os.path.join(fold_results_dir, "test_patient_results.json")
            # with open(patient_results_path, 'w') as f:
            #     json.dump(patient_results_dict, f, indent=4)
            # print(f"Saved patient-level test results to {patient_results_path}")
    else:
        print("No test loader provided. Skipping final test evaluation.")

    return test_metrics, patient_results_dict


def _compile_results(
    current_fold_num: int,
    args: Any,
    checkpoint_callback: ModelCheckpoint,
    trainer: pl.Trainer,
    test_metrics: Optional[Dict[str, Any]],
    patient_results_dict: Optional[Dict[str, Any]],
) -> Dict[str, Any]:
    """Compiles all training and evaluation results into a dictionary."""
    results = {"fold": current_fold_num}

    if test_metrics:
        for metric_name, value in test_metrics.items():
            results[f"test_{metric_name}"] = value  # e.g. test_auc, test_loss

    if patient_results_dict:
        results["test_patient_results_dict"] = patient_results_dict

    # Add best validation metric from the checkpoint callback
    if checkpoint_callback.best_model_score is not None:
        # Sanitize monitor name for use as a key
        monitor_key = checkpoint_callback.monitor.replace("/", "_").replace("-", "_")
        results[f"best_val_{monitor_key}"] = (
            checkpoint_callback.best_model_score.item()
            if isinstance(checkpoint_callback.best_model_score, torch.Tensor)
            else checkpoint_callback.best_model_score
        )

    # For k-fold summary, the primary monitored validation metric is often used
    if args.k_fold:
        if checkpoint_callback.best_model_score is not None:
            results["kfold_val_metric"] = (
                checkpoint_callback.best_model_score.item()
                if isinstance(checkpoint_callback.best_model_score, torch.Tensor)
                else checkpoint_callback.best_model_score
            )
        else:
            # Fallback if best_model_score is somehow None (e.g., training interrupted before first validation)
            results["kfold_val_metric"] = trainer.callback_metrics.get(
                checkpoint_callback.monitor, float("nan")
            )

    return results


def train_fold(
    train_dataset: Any,
    val_dataset: Any,
    test_dataset: Optional[Any],
    current_fold_num: int,
    args: Any,
) -> Dict[str, Any]:
    """
    Trains a model for a single fold using PyTorch Lightning.

    Args:
        train_dataset: Training dataset.
        val_dataset: Validation dataset.
        test_dataset: Optional test dataset.
        current_fold_num: The current fold number.
        args: Namespace object containing command-line arguments and configurations.

    Returns:
        A dictionary containing training and evaluation results for the fold.
    """
    print(
        f"\n{'='*20} Training Fold (PyTorch Lightning): {current_fold_num} | Task: {args.task_type} {'='*20}"
    )

    # 1. Setup Environment and Logger
    fold_results_dir, logger = _setup_environment_and_logger(
        args.results_dir, current_fold_num, args.log_data
    )

    # 2. Initialize DataLoaders
    train_loader, val_loader, test_loader = _initialize_dataloaders(
        train_dataset, val_dataset, test_dataset, args
    )
    train_loader_len = len(train_loader) if train_loader else 0

    # 3. Initialize LightningModule
    lightning_module = _initialize_lightning_module(args)

    # 4. Configure Callbacks
    callbacks, checkpoint_cb = _configure_callbacks(
        args, fold_results_dir, train_loader_len
    )

    # 5. Configure and Initialize PyTorch Lightning Trainer
    trainer = _configure_trainer(args, logger, callbacks, train_loader_len)

    # 6. Start Training
    print("Starting training...")
    trainer.fit(
        model=lightning_module,
        train_dataloaders=train_loader,
        val_dataloaders=val_loader,
    )
    print("Training finished.")

    # 7. Final Evaluation on Test Set
    test_metrics, patient_results = _run_final_evaluation(
        trainer, lightning_module, test_loader, checkpoint_cb, fold_results_dir
    )

    # 8. Compile and Return Results
    final_results = _compile_results(
        current_fold_num, args, checkpoint_cb, trainer, test_metrics, patient_results
    )

    print(f"Finished training fold {current_fold_num}. Results: {final_results}")
    return final_results



================================================
File: sauron/utils/WSIObjects.py
================================================
from __future__ import annotations

from abc import ABC, abstractmethod
from typing import List, Optional, Tuple

import cucim
import cv2
import geopandas as gpd
import numpy as np
import openslide
from PIL import Image

from sauron.utils.warnings import CuImageWarning


def is_cuimage_instance(image: object) -> bool:
    """Checks if a given object is an instance of cucim.CuImage.

    This function safely checks for the CuImage type without raising an
    ImportError if cuCIM is not installed. A warning is issued if the
    library is not found.

    Args:
        image: The object to check.

    Returns:
        True if the object is a CuImage instance, False otherwise.
    """
    try:
        from cucim import CuImage
    except ImportError:
        CuImage = None
        CuImageWarning.warn()
    return CuImage is not None and isinstance(image, CuImage)


class WholeSlideImage(ABC):
    """Abstract base class for a unified whole-slide image (WSI) interface.

    This class defines a standard set of methods for interacting with
    whole-slide images, regardless of the underlying backend (e.g., OpenSlide,
    NumPy array, cuCIM). It ensures that different WSI formats can be handled
    interchangeably.

    Attributes:
        image_source: The underlying image object (e.g., openslide.OpenSlide).
        width (int): The width of the level 0 image in pixels.
        height (int): The height of the level 0 image in pixels.
    """

    def __init__(self, image_source: object):
        """Initializes the WholeSlideImage.

        Args:
            image_source: The source of the image data. Supported types are
                determined by subclasses and the wsi_factory.

        Raises:
            ValueError: If the image_source type is not supported.
        """
        self.image_source = image_source

        if not (
            isinstance(image_source, openslide.OpenSlide)
            or isinstance(image_source, np.ndarray)
            or is_cuimage_instance(image_source)
        ):
            raise ValueError(f"Invalid image type: {type(image_source)}")

        self.width, self.height = self.get_dimensions()

    @abstractmethod
    def to_numpy(self) -> np.ndarray:
        """Converts the entire WSI to a NumPy array.

        Note: This can consume a large amount of memory for high-resolution images.
        It's often used for smaller images or for getting a full-resolution
        view when memory is not a concern.

        Returns:
            A NumPy array representing the full image.
        """
        pass

    @abstractmethod
    def get_dimensions(self) -> Tuple[int, int]:
        """Gets the dimensions of the level 0 image.

        Returns:
            A tuple containing the (width, height) of the slide in pixels.
        """
        pass

    @abstractmethod
    def read_region(
        self, location: Tuple[int, int], level: int, size: Tuple[int, int]
    ) -> np.ndarray:
        """Reads a specified region from the slide.

        Args:
            location: A tuple (x, y) with the top-left coordinates of the region
                at level 0.
            level: The resolution level to read from.
            size: A tuple (width, height) of the region to read at the specified
                level.

        Returns:
            A NumPy array representing the image region.
        """
        pass

    @abstractmethod
    def get_thumbnail(self, width: int, height: int) -> np.ndarray:
        """Generates a thumbnail of the WSI.

        Args:
            width: The desired width of the thumbnail.
            height: The desired height of the thumbnail.

        Returns:
            A NumPy array representing the resized thumbnail.
        """
        pass

    def __repr__(self) -> str:
        """Provides a developer-friendly string representation of the WSI."""
        return f"<width={self.width}, height={self.height}, backend={self.__class__.__name__}>"

    @abstractmethod
    def create_patcher(
        self,
        patch_size: int,
        src_mpp: float,
        dst_mpp: Optional[float] = None,
        overlap: int = 0,
        mask: Optional[gpd.GeoDataFrame] = None,
        coords_only: bool = False,
        custom_coords: Optional[np.ndarray] = None,
    ) -> "WSIPatcher":
        """Creates a patcher instance for iterating over image patches.

        Args:
            patch_size: The desired size of the output patch in pixels.
            src_mpp: The microns-per-pixel (MPP) of the source WSI.
            dst_mpp: The desired MPP of the output patches. If None, no rescaling
                is performed.
            overlap: The number of overlapping pixels between adjacent patches.
            mask: A GeoDataFrame containing polygons to filter patches. Only
                patches that intersect with the mask are yielded.
            coords_only: If True, the patcher yields only (x, y) coordinates
                instead of image data.
            custom_coords: A NumPy array of (x, y) coordinates to extract patches
                from, bypassing the default grid generation.

        Returns:
            An instance of a WSIPatcher subclass.
        """
        pass


def wsi_factory(image_source: object) -> WholeSlideImage:
    """Factory function to create a WholeSlideImage instance from various sources.

    This function automatically selects the appropriate WSI backend based on the
    type of the input `image_source`. It supports file paths (strings),
    OpenSlide objects, NumPy arrays, and cuCIM objects.

    Args:
        image_source: The image source. Can be a file path (str),
            `openslide.OpenSlide`, `np.ndarray`, `cucim.CuImage`, or another
            `WholeSlideImage` instance.

    Returns:
        An appropriate subclass of WholeSlideImage.

    Raises:
        ValueError: If the `image_source` type is not supported.
    """
    try:
        from cucim import CuImage
    except ImportError:
        CuImage = None
        CuImageWarning.warn()

    if isinstance(image_source, WholeSlideImage):
        return image_source

    image_type_map = {
        openslide.OpenSlide: OpenSlideWSI,
        np.ndarray: NumpyWSI,
        str: lambda src: (
            CuImageWSI(CuImage(src))
            if CuImage
            else OpenSlideWSI(openslide.OpenSlide(src))
        ),
    }

    for image_type, constructor in image_type_map.items():
        if isinstance(image_source, image_type):
            return constructor(image_source)

    if is_cuimage_instance(image_source):
        return CuImageWSI(image_source)

    raise ValueError(f"Unsupported image type: {type(image_source)}")


class NumpyWSI(WholeSlideImage):
    """A WholeSlideImage implementation for NumPy arrays."""

    def __init__(self, image: np.ndarray):
        """Initializes the NumpyWSI.

        Args:
            image: The NumPy array representing the image.
        """
        super().__init__(image)

    def to_numpy(self) -> np.ndarray:
        """Returns the underlying NumPy array."""
        return self.image_source

    def get_dimensions(self) -> Tuple[int, int]:
        """Returns the dimensions (width, height) of the NumPy array."""
        return self.image_source.shape[1], self.image_source.shape[0]

    def read_region(
        self, location: Tuple[int, int], level: int, size: Tuple[int, int]
    ) -> np.ndarray:
        """Reads a region from the NumPy array.

        Args:
            location: Top-left (x, y) coordinates of the region.
            level: Ignored for NumPy arrays (always level 0).
            size: The (width, height) of the region to extract.

        Returns:
            A NumPy array view of the specified region.
        """
        x_start, y_start = location
        x_size, y_size = size
        return self.image_source[y_start : y_start + y_size, x_start : x_start + x_size]

    def get_thumbnail(self, width: int, height: int) -> np.ndarray:
        """Creates a resized thumbnail from the NumPy array."""
        return cv2.resize(self.image_source, (width, height))

    def create_patcher(
        self,
        patch_size: int,
        src_mpp: float,
        dst_mpp: Optional[float] = None,
        overlap: int = 0,
        mask: Optional[gpd.GeoDataFrame] = None,
        coords_only: bool = False,
        custom_coords: Optional[np.ndarray] = None,
    ) -> "NumpyWSIPatcher":
        """Creates a patcher for the NumPy array."""
        return NumpyWSIPatcher(
            self,
            patch_size,
            src_mpp,
            dst_mpp,
            overlap,
            mask,
            coords_only,
            custom_coords,
        )


class OpenSlideWSI(WholeSlideImage):
    """A WholeSlideImage implementation for OpenSlide-compatible files."""

    def __init__(self, image: openslide.OpenSlide):
        """Initializes the OpenSlideWSI.

        Args:
            image: An initialized openslide.OpenSlide object.
        """
        super().__init__(image)

    def to_numpy(self) -> np.ndarray:
        """Returns the entire image as a NumPy array by creating a full-size thumbnail."""
        return self.get_thumbnail(self.width, self.height)

    def get_dimensions(self) -> Tuple[int, int]:
        """Returns the level 0 dimensions from the OpenSlide object."""
        return self.image_source.dimensions

    def read_region(
        self, location: Tuple[int, int], level: int, size: Tuple[int, int]
    ) -> np.ndarray:
        """Reads a region using the OpenSlide backend."""
        return np.array(self.image_source.read_region(location, level, size))[:, :, :3]

    def get_thumbnail(self, width: int, height: int) -> np.ndarray:
        """Gets a thumbnail using the OpenSlide backend."""
        return np.array(self.image_source.get_thumbnail((width, height)))

    def get_best_level_for_downsample(self, downsample: float) -> int:
        """Determines the best WSI level for a given downsample factor."""
        return self.image_source.get_best_level_for_downsample(downsample)

    def level_dimensions(self) -> List[Tuple[int, int]]:
        """Gets the dimensions of each level in the WSI."""
        return self.image_source.level_dimensions

    def level_downsamples(self) -> List[float]:
        """Gets the downsample factor for each level in the WSI."""
        return self.image_source.level_downsamples

    def create_patcher(
        self,
        patch_size: int,
        src_mpp: float,
        dst_mpp: Optional[float] = None,
        overlap: int = 0,
        mask: Optional[gpd.GeoDataFrame] = None,
        coords_only: bool = False,
        custom_coords: Optional[np.ndarray] = None,
    ) -> "OpenSlideWSIPatcher":
        """Creates a patcher for the OpenSlide WSI."""
        return OpenSlideWSIPatcher(
            self,
            patch_size,
            src_mpp,
            dst_mpp,
            overlap,
            mask,
            coords_only,
            custom_coords,
        )


class CuImageWSI(WholeSlideImage):
    """A WholeSlideImage implementation for cuCIM-compatible files."""

    def __init__(self, image: "cucim.CuImage"):
        """Initializes the CuImageWSI.

        Args:
            image: An initialized cucim.CuImage object.
        """
        super().__init__(image)

    def to_numpy(self) -> np.ndarray:
        """Returns the entire image as a NumPy array by creating a full-size thumbnail."""
        return self.get_thumbnail(self.width, self.height)

    def get_dimensions(self) -> Tuple[int, int]:
        """Returns the level 0 dimensions from the cuCIM object."""
        return self.image_source.resolutions["level_dimensions"][0]

    def read_region(
        self, location: Tuple[int, int], level: int, size: Tuple[int, int]
    ) -> np.ndarray:
        """Reads a region using the cuCIM backend."""
        return np.array(
            self.image_source.read_region(location=location, level=level, size=size)
        )[:, :, :3]

    def get_thumbnail(self, width: int, height: int) -> np.ndarray:
        """Gets a thumbnail using the cuCIM backend.

        This method selects the most appropriate resolution level to read from
        and then resizes to the target dimensions.
        """
        downsample = self.width / width
        level = self.get_best_level_for_downsample(downsample)
        curr_width, curr_height = self.image_source.resolutions["level_dimensions"][
            level
        ]
        thumbnail = np.array(
            self.image_source.read_region(
                location=(0, 0), level=level, size=(curr_width, curr_height)
            )
        )[:, :, :3]
        return cv2.resize(thumbnail, (width, height))

    def get_best_level_for_downsample(self, downsample: float) -> int:
        """Determines the best WSI level for a given downsample factor."""
        downsamples = self.image_source.resolutions["level_downsamples"]
        for i, level_downsample in enumerate(downsamples):
            if downsample < level_downsample:
                return max(i - 1, 0)
        return len(downsamples) - 1

    def level_dimensions(self) -> List[Tuple[int, int]]:
        """Gets the dimensions of each level in the WSI."""
        return self.image_source.resolutions["level_dimensions"]

    def level_downsamples(self) -> List[float]:
        """Gets the downsample factor for each level in the WSI."""
        return self.image_source.resolutions["level_downsamples"]

    def create_patcher(
        self,
        patch_size: int,
        src_mpp: float,
        dst_mpp: Optional[float] = None,
        overlap: int = 0,
        mask: Optional[gpd.GeoDataFrame] = None,
        coords_only: bool = False,
        custom_coords: Optional[np.ndarray] = None,
    ) -> "CuImageWSIPatcher":
        """Creates a patcher for the cuCIM WSI."""
        return CuImageWSIPatcher(
            self,
            patch_size,
            src_mpp,
            dst_mpp,
            overlap,
            mask,
            coords_only,
            custom_coords,
        )


class WSIPatcher(ABC):
    """Iterator class to handle patch extraction, scaling, and mask intersection.

    This class provides an iterable interface to efficiently extract patches from a
    WholeSlideImage. It manages grid generation, overlap, scaling based on MPP,
    and filtering based on a supplied geometry mask.

    This is an abstract base class; concrete implementations must provide the
    _prepare_patching method.

    Attributes:
        wsi (WholeSlideImage): The WSI object to patch.
        patch_size_target (int): The final size of the patches after any resizing.
        patch_size_src (int): The size of the patch to read from the source WSI at
            level 0 resolution.
        downsample (float): The calculated downsampling factor.
        level (int): The optimal WSI level to read from.
        valid_coords (np.ndarray): An array of (x, y) coordinates for the
            patches that will be yielded by the iterator.
    """

    def __init__(
        self,
        wsi: WholeSlideImage,
        patch_size: int,
        src_mpp: float,
        dst_mpp: Optional[float] = None,
        overlap: int = 0,
        mask: Optional[gpd.GeoDataFrame] = None,
        coords_only: bool = False,
        custom_coords: Optional[np.ndarray] = None,
    ):
        """Initializes the WSIPatcher.

        Args:
            wsi: The WholeSlideImage object to process.
            patch_size: The desired output size of each patch in pixels.
            src_mpp: The microns-per-pixel (MPP) of the source WSI.
            dst_mpp: The desired MPP for the output patches. If provided, patches
                are read at a higher resolution and downscaled. If None,
                no rescaling occurs.
            overlap: The number of overlapping pixels between adjacent patches.
            mask: A GeoDataFrame containing polygon geometries. Only patches that
                intersect with these geometries will be processed.
            coords_only: If True, the iterator yields only (x, y) coordinates
                instead of the full patch data.
            custom_coords: An optional NumPy array of (x, y) coordinates to
                extract patches from, bypassing the default grid generation.
        """
        self.wsi = wsi
        self.overlap = overlap
        self.width, self.height = self.wsi.get_dimensions()
        self.patch_size_target = patch_size
        self.mask = mask
        self.coords_only = coords_only
        self.custom_coords = custom_coords
        self.current_index = 0

        self.downsample = 1.0 if dst_mpp is None else dst_mpp / src_mpp
        self.patch_size_src = round(patch_size * self.downsample)

        self.level, self.patch_size_level, self.overlap_level = self._prepare_patching()

        if custom_coords is None:
            self.cols, self.rows = self._calculate_cols_rows()
            grid_coords = np.array(
                [[col, row] for col in range(self.cols) for row in range(self.rows)]
            )
            self.all_coords = np.array(
                [self._grid_to_coordinates(col, row) for col, row in grid_coords]
            )
        else:
            self.all_coords = custom_coords

        if self.mask is not None:
            self.valid_coords = self._filter_coords_with_mask(self.all_coords)
        else:
            self.valid_coords = self.all_coords

    @abstractmethod
    def _prepare_patching(self) -> Tuple[int, int, int]:
        """Prepares patching parameters specific to the WSI backend.

        This method must be implemented by subclasses to calculate the optimal
        WSI level and the corresponding patch and overlap sizes at that level.

        Returns:
            A tuple of (level, patch_size_level, overlap_level).
        """
        pass

    def _filter_coords_with_mask(self, coords: np.ndarray) -> np.ndarray:
        """Filters patch coordinates to keep only those intersecting the mask."""
        union_mask = self.mask.unary_union
        patch_polygons = [
            gpd.box(x, y, x + self.patch_size_src, y + self.patch_size_src)
            for x, y in coords
        ]
        patches_gdf = gpd.GeoDataFrame(geometry=patch_polygons)
        intersects = patches_gdf.intersects(union_mask)
        return coords[intersects.values]

    def __len__(self) -> int:
        """Returns the total number of valid patches."""
        return len(self.valid_coords)

    def __iter__(self) -> "WSIPatcher":
        """Returns the iterator object itself."""
        self.current_index = 0
        return self

    def __next__(self) -> Tuple[np.ndarray, int, int] | Tuple[int, int]:
        """Returns the next patch or coordinate."""
        if self.current_index >= len(self):
            raise StopIteration
        item = self[self.current_index]
        self.current_index += 1
        return item

    def __getitem__(self, index: int) -> Tuple[np.ndarray, int, int] | Tuple[int, int]:
        """Gets a patch or coordinate by its index.

        Args:
            index: The index of the valid patch coordinate.

        Returns:
            If `coords_only` is False, returns a tuple of (patch_image, x, y).
            If `coords_only` is True, returns a tuple of (x, y).

        Raises:
            IndexError: If the index is out of range.
        """
        if 0 <= index < len(self):
            x, y = self.valid_coords[index]
            if self.coords_only:
                return x, y
            return self.get_patch_at(x, y)
        else:
            raise IndexError("Index out of range")

    def _grid_to_coordinates(self, col: int, row: int) -> Tuple[int, int]:
        """Converts grid indices (col, row) to pixel coordinates (x, y)."""
        x = col * self.patch_size_src - self.overlap * max(col - 1, 0)
        y = row * self.patch_size_src - self.overlap * max(row - 1, 0)
        return x, y

    def _calculate_cols_rows(self) -> Tuple[int, int]:
        """Calculates the number of columns and rows in the patch grid."""
        cols = (self.width + self.patch_size_src - 1) // (
            self.patch_size_src - self.overlap
        )
        rows = (self.height + self.patch_size_src - 1) // (
            self.patch_size_src - self.overlap
        )
        return cols, rows

    def get_patch_at(self, x: int, y: int) -> Tuple[np.ndarray, int, int]:
        """Reads and resizes a single patch at the given level 0 coordinates.

        Args:
            x: The level 0 x-coordinate of the top-left corner.
            y: The level 0 y-coordinate of the top-left corner.

        Returns:
            A tuple containing (patch_image, x, y). The patch image is a
            NumPy array resized to `patch_size_target`.
        """
        patch_image = self.wsi.read_region(
            location=(x, y),
            level=self.level,
            size=(self.patch_size_level, self.patch_size_level),
        )
        if self.patch_size_target != self.patch_size_level:
            patch_image = cv2.resize(
                patch_image, (self.patch_size_target, self.patch_size_target)
            )
        return patch_image[:, :, :3], x, y

    def save_visualization(self, path: str, vis_width: int = 1000, dpi: int = 150):
        """Saves a visualization of the WSI with patch locations and masks.

        Args:
            path: The file path to save the visualization image.
            vis_width: The target width of the visualization image.
            dpi: The dots-per-inch for the saved image.
        """
        visualization = generate_visualization(
            self,  # Pass the patcher instance itself
            self.wsi,
            self.mask,
            self.valid_coords,
            line_color=(0, 255, 0),
            line_thickness=2,
            target_width=vis_width,
        )
        visualization.save(path, dpi=(dpi, dpi))


class OpenSlideWSIPatcher(WSIPatcher):
    """A WSIPatcher implementation for OpenSlide-backed WSIs."""

    def _prepare_patching(self) -> Tuple[int, int, int]:
        """Calculates patching parameters using OpenSlide's level metadata."""
        level = self.wsi.get_best_level_for_downsample(self.downsample)
        level_downsample = self.wsi.level_downsamples()[level]
        patch_size_level = round(self.patch_size_src / level_downsample)
        overlap_level = round(self.overlap / level_downsample)
        return level, patch_size_level, overlap_level


class CuImageWSIPatcher(WSIPatcher):
    """A WSIPatcher implementation for cuCIM-backed WSIs."""

    def _prepare_patching(self) -> Tuple[int, int, int]:
        """Calculates patching parameters using cuCIM's level metadata."""
        level = self.wsi.get_best_level_for_downsample(self.downsample)
        level_downsample = self.wsi.level_downsamples()[level]
        patch_size_level = round(self.patch_size_src / level_downsample)
        overlap_level = round(self.overlap / level_downsample)
        return level, patch_size_level, overlap_level


class NumpyWSIPatcher(WSIPatcher):
    """A WSIPatcher implementation for NumPy array-backed WSIs."""

    def _prepare_patching(self) -> Tuple[int, int, int]:
        """Sets patching parameters for a NumPy array.

        Since NumPy arrays are single-resolution, level is set to -1 (N/A) and
        sizes are not scaled by a level downsample factor.
        """
        patch_size_level = self.patch_size_src
        overlap_level = self.overlap
        level = -1  # Not applicable for numpy arrays
        return level, patch_size_level, overlap_level


def draw_contours_on_image(
    contours: gpd.GeoDataFrame,
    image: np.ndarray,
    line_color: Tuple[int, int, int] = (0, 255, 0),
    line_thickness: int = 1,
    downsample_factor: float = 1.0,
) -> np.ndarray:
    """Draws polygon contours from a GeoDataFrame onto a NumPy image.

    Args:
        contours: A GeoDataFrame with a 'geometry' column of Polygons.
        image: The NumPy array image on which to draw.
        line_color: The BGR color tuple for the contour lines.
        line_thickness: The thickness of the contour lines.
        downsample_factor: A factor to scale the contour coordinates to match
            the image's resolution.

    Returns:
        The image with contours drawn on it.
    """
    for _, row in contours.iterrows():
        exterior_coords = np.array(
            [
                [int(x * downsample_factor), int(y * downsample_factor)]
                for x, y in row.geometry.exterior.coords
            ]
        )
        interiors = [
            np.array(
                [
                    [int(x * downsample_factor), int(y * downsample_factor)]
                    for x, y in interior.coords
                ]
            )
            for interior in row.geometry.interiors
        ]
        cv2.drawContours(
            image, [exterior_coords], -1, line_color, line_thickness, cv2.LINE_8
        )
        for hole in interiors:
            cv2.drawContours(image, [hole], -1, (0, 0, 0), cv2.FILLED, cv2.LINE_8)
    return image


def generate_visualization(
    patcher: WSIPatcher,
    wsi: WholeSlideImage,
    tissue_contours: Optional[gpd.GeoDataFrame],
    patch_coords: np.ndarray,
    line_color: Tuple[int, int, int] = (0, 255, 0),
    line_thickness: int = 2,
    target_width: int = 1000,
) -> Image:
    """Generates a visualization image with WSI thumbnail, contours, and patches.

    Args:
        patcher: The WSIPatcher instance used for tiling. It provides patch size info.
        wsi: The WholeSlideImage object.
        tissue_contours: An optional GeoDataFrame with tissue polygons to draw.
        patch_coords: A NumPy array of (x, y) coordinates for the patches to draw.
        line_color: The color for the tissue contour lines.
        line_thickness: The line thickness for contours and patch rectangles.
        target_width: The desired width of the output visualization thumbnail.

    Returns:
        A PIL Image object of the combined visualization.
    """
    width, height = wsi.get_dimensions()
    downsample_factor = target_width / width

    thumbnail = wsi.get_thumbnail(
        int(width * downsample_factor), int(height * downsample_factor)
    )
    overlay = np.zeros_like(thumbnail)

    if tissue_contours is not None:
        draw_contours_on_image(
            tissue_contours, overlay, line_color, line_thickness, downsample_factor
        )

    for x, y in patch_coords:
        x_ds, y_ds = int(x * downsample_factor), int(y * downsample_factor)
        ps_ds = int(patcher.patch_size_src * downsample_factor)
        cv2.rectangle(
            overlay,
            (x_ds, y_ds),
            (x_ds + ps_ds, y_ds + ps_ds),
            (255, 0, 0),
            line_thickness,
        )

    alpha = 0.4
    combined_image = cv2.addWeighted(thumbnail, 1 - alpha, overlay, alpha, 0)
    return Image.fromarray(combined_image.astype(np.uint8))


def get_pixel_spacing(slide: openslide.OpenSlide) -> float:
    """Extracts the pixel spacing (in micrometers per pixel) from a whole slide image.

    Args:
        slide: The slide object.

    Returns:
        Pixel spacing in micrometers.

    Raises:
        ValueError: If pixel spacing information is missing, zero, or cannot
            be retrieved from the slide metadata.
    """
    try:
        pixel_spacing = float(slide.properties[openslide.PROPERTY_NAME_MPP_X])
        if pixel_spacing == 0:
            raise ValueError("Pixel spacing information is zero in the slide metadata.")
        return pixel_spacing
    except KeyError:
        raise ValueError("Pixel spacing information is not available in the slide.")
    except Exception as e:
        raise ValueError(f"Could not retrieve pixel spacing: {e}")



================================================
File: sauron/utils/callbacks.py
================================================
import os
from typing import Tuple

import numpy as np
import torch
import torch.nn as nn


class AccuracyLogger:
    def __init__(self, n_classes: int):
        self.n_classes = n_classes
        self.data = [{"count": 0, "correct": 0} for _ in range(self.n_classes)]

    def log(self, y_hat: int, y: int):
        self.data[y]["count"] += 1
        self.data[y]["correct"] += int(y_hat == y)

    def log_batch(self, y_hat: np.ndarray, y: np.ndarray):
        y_hat = y_hat.astype(int)
        y = y.astype(int)
        for label_class in range(self.n_classes):
            cls_mask = y == label_class
            self.data[label_class]["count"] += cls_mask.sum()
            self.data[label_class]["correct"] += (y_hat[cls_mask] == y[cls_mask]).sum()

    def get_summary(self, c: int) -> Tuple[float, int, int]:
        count = self.data[c]["count"]
        correct = self.data[c]["correct"]
        return (float(correct) / count if count else 0.0, correct, count)


class EarlyStopping:
    """Early stops the training if validation loss doesn't improve after a given patience."""

    def __init__(self, warmup=5, patience=15, stop_epoch=20, verbose=False):
        """
        Args:
            patience (int): How long to wait after last time validation loss improved.
                            Default: 20
            stop_epoch (int): Earliest epoch possible for stopping
            verbose (bool): If True, prints a message for each validation loss improvement.
                            Default: False
        """
        self.warmup = warmup
        self.patience = patience
        self.stop_epoch = stop_epoch
        self.verbose = verbose
        self.counter = 0
        self.best_score = float("inf")
        self.early_stop = False
        self.val_loss_min = float("inf")

    def __call__(self, epoch, val_loss, model, ckpt_name="checkpoint.pt"):
        score = val_loss

        if epoch < self.warmup:
            pass
        elif score < self.best_score:
            self.best_score = score
            self.save_checkpoint(val_loss, model, ckpt_name)
            self.counter = 0
        else:
            self.counter += 1
            print(f"EarlyStopping counter: {self.counter} out of {self.patience}")
            if self.counter >= self.patience or epoch > self.stop_epoch:
                self.early_stop = True

    def save_checkpoint(self, val_loss, model, ckpt_name):
        """Saves model when validation loss decrease."""
        if self.verbose:
            print(
                f"Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}). Saving model ..."
            )

        # Ensure the directory exists
        os.makedirs(os.path.dirname(ckpt_name), exist_ok=True)

        torch.save(model.state_dict(), ckpt_name)
        self.val_loss_min = val_loss


class EarlyStopping_cindex:
    """Early stops the training if validation loss doesn't improve after a given patience."""

    def __init__(self, warmup=5, patience=15, stop_epoch=20, verbose=False):
        """
        Args:
            patience (int): How long to wait after last time validation loss improved.
                            Default: 20
            stop_epoch (int): Earliest epoch possible for stopping
            verbose (bool): If True, prints a message for each validation loss improvement.
                            Default: False
        """
        self.warmup = warmup
        self.patience = patience
        self.stop_epoch = stop_epoch
        self.verbose = verbose
        self.counter = 0
        self.best_score = None
        self.early_stop = False
        self.val_loss_min = np.Inf

    def __call__(self, epoch, val_loss, model, ckpt_name="checkpoint.pt"):
        score = val_loss
        # score = -val_loss

        if epoch < self.warmup:
            pass
        elif self.best_score is None:
            self.best_score = score
            self.save_checkpoint(val_loss, model, ckpt_name)
        elif score <= self.best_score:
            self.counter += 1
            print(f"EarlyStopping counter: {self.counter} out of {self.patience}")
            if self.counter >= self.patience and epoch > self.stop_epoch:
                self.early_stop = True
        else:
            self.best_score = score
            self.save_checkpoint(val_loss, model, ckpt_name)
            self.counter = 0

    def save_checkpoint(self, val_loss, model, ckpt_name):
        """Saves model when validation loss decrease."""
        if self.verbose:
            print(
                f"Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ..."
            )
        torch.save(model.state_dict(), ckpt_name)
        self.val_loss_min = val_loss


class Monitor_CIndex:
    """Early stops the training if validation loss doesn't improve after a given patience."""

    def __init__(self):
        """
        Args:
            patience (int): How long to wait after last time validation loss improved.
                            Default: 20
            stop_epoch (int): Earliest epoch possible for stopping
            verbose (bool): If True, prints a message for each validation loss improvement.
                            Default: False
        """
        self.best_score = None

    def __call__(self, val_cindex, model, ckpt_name: str = "checkpoint.pt"):
        score = val_cindex

        if self.best_score is None:
            self.best_score = score
            self.save_checkpoint(model, ckpt_name)
        elif score > self.best_score:
            self.best_score = score
            self.save_checkpoint(model, ckpt_name)
        else:
            pass

    def save_checkpoint(self, model, ckpt_name):
        """Saves model when validation loss decrease."""
        torch.save(model.state_dict(), ckpt_name)



================================================
File: sauron/utils/drawing_utils.py
================================================
from typing import Union

import cv2
import geopandas as gpd
import numpy as np
import openslide
from PIL import Image

from sauron.utils.WSIObjects import WholeSlideImage, wsi_factory


def draw_contours(
    image: np.ndarray,
    contours: gpd.GeoDataFrame,
    draw_outline: bool = False,
    line_thickness: int = 1,
    scale_factor: float = 1.0,
    contour_color: tuple = (0, 255, 0),
) -> np.ndarray:
    """
    Draw contours on an image.

    Args:
        image (np.ndarray): Image on which to draw.
        contours (gpd.GeoDataFrame): Contours to draw.
        draw_outline (bool): Whether to draw the outline of contours.
        line_thickness (int): Thickness of the contour lines.
        scale_factor (float): Scaling factor for the contours.
        contour_color (tuple): Color of the contours.

    Returns:
        np.ndarray: Image with contours drawn.
    """
    for _, group in contours.groupby("tissue_id"):
        for _, row in group.iterrows():
            exterior = np.array(
                [
                    [
                        int(round(x * scale_factor)),
                        int(round(y * scale_factor)),
                    ]
                    for x, y in row.geometry.exterior.coords
                ]
            )
            interiors = [
                np.array(
                    [
                        [
                            int(round(x * scale_factor)),
                            int(round(y * scale_factor)),
                        ]
                        for x, y in interior.coords
                    ]
                )
                for interior in row.geometry.interiors
            ]

            cv2.drawContours(
                image,
                [exterior],
                contourIdx=-1,
                color=contour_color,
                thickness=cv2.FILLED,
            )
            for hole in interiors:
                cv2.drawContours(
                    image,
                    [hole],
                    contourIdx=-1,
                    color=(0, 0, 0),
                    thickness=cv2.FILLED,
                )
            if draw_outline:
                cv2.drawContours(
                    image,
                    [exterior],
                    contourIdx=-1,
                    color=contour_color,
                    thickness=line_thickness,
                )
    return image


def visualize_tissue(
    wsi: Union[np.ndarray, openslide.OpenSlide, WholeSlideImage],
    tissue_contours: gpd.GeoDataFrame,
    contour_color: tuple = (0, 255, 0),
    line_thickness: int = 5,
    target_width: int = 1000,
) -> Image.Image:
    """
    Visualize tissue contours on a whole slide image.

    Args:
        wsi (Union[np.ndarray, openslide.OpenSlide, WSI]): The whole slide image.
        tissue_contours (gpd.GeoDataFrame): Contours of the tissue regions.
        contour_color (tuple): Color of the contour lines.
        line_thickness (int): Thickness of the contour lines.
        target_width (int): Target width for the visualization image.

    Returns:
        Image.Image: The visualization image.
    """
    wsi = wsi_factory(wsi)
    width, height = wsi.get_dimensions()
    scale_factor = target_width / width

    thumbnail = wsi.get_thumbnail(
        width=int(width * scale_factor), height=int(height * scale_factor)
    )

    if tissue_contours.empty:
        return Image.fromarray(thumbnail)

    overlay = np.zeros_like(thumbnail, dtype=np.uint8)
    overlay = draw_contours(
        overlay,
        tissue_contours,
        draw_outline=True,
        line_thickness=line_thickness,
        scale_factor=scale_factor,
        contour_color=contour_color,
    )

    alpha = 0.4
    blended = cv2.addWeighted(thumbnail, 1 - alpha, overlay, alpha, 0)
    return Image.fromarray(blended)



================================================
File: sauron/utils/environment_setup.py
================================================
# sauron/utils/environment_setup.py
import argparse
import os
import random
from typing import Optional

import numpy as np
import torch


def setup_device() -> torch.device:
    """Sets up and returns the device (CUDA or CPU)."""
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"Current device is set to: {device}")
    return device


def seed_everything(seed: int):
    """Seeds random number generators for reproducibility."""
    random.seed(seed)
    os.environ["PYTHONHASHSEED"] = str(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed(seed)
        torch.cuda.manual_seed_all(seed)  # if you are using multi-GPU.
        # The following two lines are often recommended for reproducibility with CuDNN
        # However, they can impact performance. Use with caution.
        # torch.backends.cudnn.deterministic = True
        # torch.backends.cudnn.benchmark = False
    print(f"Seeded everything with seed: {seed}")


def create_results_directory(
    base_results_dir: str, exp_code: str, seed: int, fold_num: Optional[int] = None
) -> str:
    """
    Creates the results directory for the experiment.
    If fold_num is provided, creates a subdirectory for that fold.
    """
    experiment_path = os.path.join(base_results_dir, f"{exp_code}_s{seed}")
    if fold_num is not None:
        results_path = os.path.join(experiment_path, f"fold_{fold_num}")
    else:
        results_path = experiment_path  # Main experiment directory

    os.makedirs(results_path, exist_ok=True)
    return results_path


def log_experiment_details(args: argparse.Namespace, results_dir: str):
    """Logs experiment arguments to a file."""
    # Ensure results_dir here is the main experiment directory, not a fold-specific one
    # if log_experiment_details is called once per experiment.
    # If called per fold, then it's fine if results_dir is fold-specific.
    # For now, assuming it's called once for the main experiment.

    # If results_dir might be a fold-specific path, get the parent experiment path
    # e.g., experiment_base_path = os.path.dirname(results_dir) if "fold_" in os.path.basename(results_dir) else results_dir
    experiment_base_path = (
        results_dir  # Assuming results_dir passed is already the main exp dir
    )

    filepath = os.path.join(experiment_base_path, "experiment_args.txt")
    with open(filepath, "w") as f:
        for key, val in sorted(vars(args).items()):  # Sort for consistent order
            f.write(f"{key}: {val}\n")
    print(f"Experiment arguments logged to: {filepath}")



================================================
File: sauron/utils/filehandler.py
================================================
import os
from typing import Any, Dict, List, Optional

import pandas as pd

# File extensions for slide images
EXTENSIONS: List[str] = [".svs", ".mrxs", ".tiff", ".tif", ".TIFF", ".ndpi"]


class PatientFolder:
    def __init__(
        self, patients_dir: str, labels: Optional[pd.DataFrame] = None
    ) -> None:
        self.patients_dir: str = patients_dir
        self.labels: Optional[pd.DataFrame] = labels
        self.data: pd.DataFrame = pd.DataFrame(columns=["pid", "slide_id", "label"])

    def analyze(self) -> None:
        try:
            new_data: List[Dict[str, Any]] = []
            for folder_name in os.listdir(self.patients_dir):
                folder_path: str = os.path.join(self.patients_dir, folder_name)
                if os.path.isdir(folder_path):
                    label = self._get_label(folder_name)
                    self._process_folder(folder_name, folder_path, label, new_data)
            if new_data:
                self.data = pd.concat(
                    [self.data, pd.DataFrame(new_data)], ignore_index=True
                )
        except FileNotFoundError as e:
            raise RuntimeError(
                f"Error analyzing patient folders: Directory '{self.patients_dir}' not found. {e}"
            )
        except PermissionError as e:
            raise RuntimeError(
                f"Error analyzing patient folders: Permission denied when accessing '{self.patients_dir}'. {e}"
            )
        except Exception as e:
            raise RuntimeError(f"Unexpected error analyzing patient folders: {e}")

    def _get_label(self, folder_name: str) -> Any:
        if self.labels is not None and folder_name in self.labels.index:
            return self.labels.loc[folder_name]
        elif folder_name.isdigit():
            return int(folder_name)
        else:
            raise ValueError(
                f"Label for folder '{folder_name}' not found. Ensure that the folder name is either a digit or is present in the provided labels."
            )

    def _process_folder(
        self,
        folder_name: str,
        folder_path: str,
        label: Any,
        new_data: List[Dict[str, Any]],
    ) -> None:
        for file_name in os.listdir(folder_path):
            if any(file_name.endswith(ext) for ext in EXTENSIONS):
                slide_id: str = file_name
                new_data.append(
                    {
                        "pid": folder_name,
                        "slide_id": slide_id,
                        "label": label,
                    }
                )

    def __len__(self) -> int:
        return len(self.data)

    def get_data(self) -> pd.DataFrame:
        return self.data



================================================
File: sauron/utils/generic_utils.py
================================================
import argparse
import os
import pickle
from typing import List

import numpy as np
import pandas as pd
import torch
import torch.nn as nn
from torch.utils.data import Dataset
from torch.utils.tensorboard import SummaryWriter


def save_pkl(filename: str, save_object: object) -> None:
    with open(filename, "wb") as f:
        pickle.dump(save_object, f)


def load_pkl(filename: str) -> object:
    with open(filename, "rb") as file:
        return pickle.load(file)


def seed_everything(seed: int = 42) -> None:
    import random

    random.seed(seed)
    os.environ["PYTHONHASHSEED"] = str(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed_all(seed)
    torch.backends.cudnn.benchmark = False
    torch.backends.cudnn.deterministic = True


def calculate_error(Y_hat: torch.Tensor, Y: torch.Tensor) -> float:
    return 1.0 - Y_hat.float().eq(Y.float()).float().mean().item()


def save_splits(
    split_datasets: List[Dataset],
    column_keys: List[str],
    filename: str,
    boolean_style: bool = False,
) -> None:
    splits = [
        split_datasets[i].slide_data["slide_id"] for i in range(len(split_datasets))
    ]
    if not boolean_style:
        df = pd.concat(splits, ignore_index=True, axis=1)
        df.columns = column_keys
    else:
        df = pd.concat(splits, ignore_index=True, axis=0)
        index = df.values.tolist()
        one_hot = np.eye(len(split_datasets)).astype(bool)
        bool_array = np.repeat(one_hot, [len(dset) for dset in split_datasets], axis=0)
        df = pd.DataFrame(bool_array, index=index, columns=["train", "val", "test"])

    df.to_csv(filename)
    print(f"Splits saved to {filename}")


def initialize_weights(module):
    for m in module.modules():
        if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):
            nn.init.xavier_normal_(m.weight)
            if m.bias is not None:
                m.bias.data.zero_()
        elif isinstance(m, nn.LayerNorm):
            nn.init.constant_(m.bias, 0)
            nn.init.constant_(m.weight, 1.0)


def log_results(
    df: pd.DataFrame, args: argparse.Namespace, writer: SummaryWriter
) -> None:
    mean_metrics = df.mean()
    std_metrics = df.std()

    for metric in ["test_auc", "val_auc", "test_acc", "val_acc"]:
        writer.add_scalar(f"mean_{metric}", mean_metrics[metric], args.k_end)
        writer.add_scalar(f"std_{metric}", std_metrics[metric], args.k_end)

    df_append = pd.DataFrame(
        {
            "folds": ["mean", "std"],
            **{
                metric: [mean_metrics[metric], std_metrics[metric]]
                for metric in ["test_auc", "val_auc", "test_acc", "val_acc"]
            },
        }
    )

    final_df = pd.concat([df, df_append])
    save_name = (
        f"summary_partial_{args.k_start}_{args.k_end}.csv"
        if args.k_end - args.k_start != args.k
        else "summary.csv"
    )
    final_df.to_csv(os.path.join(args.results_dir, save_name), index=False)



================================================
File: sauron/utils/gpd_utils.py
================================================
from typing import Dict, List, Optional, Tuple

import cv2
import geopandas as gpd
import numpy as np
from shapely.geometry import Polygon


def apply_mask_to_image(image: np.ndarray, mask: np.ndarray) -> np.ndarray:
    if image.shape[:2] != mask.shape:
        raise ValueError("Image and mask dimensions do not match.")

    masked_image = np.where(mask[..., None], image, 255)
    return masked_image.astype(np.uint8)


def filter_and_group_contours(
    contours: List[np.ndarray],
    hierarchy: np.ndarray,
    filter_params: Dict[str, float],
    scaling_factor: float,
    pixel_size: float,
) -> Tuple[List[np.ndarray], List[List[np.ndarray]]]:
    foreground_indices: List[int] = []
    hole_indices_per_contour: List[List[int]] = []
    top_level_indices = np.where(hierarchy[:, 1] == -1)[0] if hierarchy.size > 0 else []

    for idx in top_level_indices:
        contour = contours[idx]
        hole_indices = np.where(hierarchy[:, 1] == idx)[0]
        area = cv2.contourArea(contour) - sum(
            cv2.contourArea(contours[hole_idx]) for hole_idx in hole_indices
        )
        area *= (pixel_size**2) / (scaling_factor**2)

        if area > filter_params["min_area"]:
            foreground_indices.append(idx)
            hole_indices_per_contour.append(
                [
                    hole_idx
                    for hole_idx in hole_indices
                    if cv2.contourArea(contours[hole_idx]) * pixel_size**2
                    > filter_params["min_hole_area"]
                ]
            )

    foreground_contours = [contours[idx] for idx in foreground_indices]
    hole_contours = [
        [contours[hole_idx] for hole_idx in holes] for holes in hole_indices_per_contour
    ]

    return foreground_contours, hole_contours


def mask_to_geodataframe(
    mask: np.ndarray,
    keep_ids: Optional[List[int]] = None,
    exclude_ids: Optional[List[int]] = None,
    max_holes: int = 0,
    min_contour_area: int = 1000,
    pixel_size: float = 1.0,
    contour_scale: float = 1.0,
) -> gpd.GeoDataFrame:
    TARGET_SIZE = 2000
    scaling_factor = TARGET_SIZE / mask.shape[0]
    resized_mask = cv2.resize(
        mask, (int(mask.shape[1] * scaling_factor), int(mask.shape[0] * scaling_factor))
    )

    contours, hierarchy = cv2.findContours(
        resized_mask,
        cv2.RETR_TREE if max_holes == 0 else cv2.RETR_CCOMP,
        cv2.CHAIN_APPROX_NONE,
    )
    hierarchy = (
        hierarchy.squeeze(axis=0)[:, 2:] if hierarchy is not None else np.array([])
    )

    filter_params: Dict[str, float] = {
        "min_area": min_contour_area * (pixel_size**2),
        "min_hole_area": 4000 * (pixel_size**2),
    }

    foreground_contours, hole_contours = filter_and_group_contours(
        contours, hierarchy, filter_params, scaling_factor, pixel_size
    )

    if not foreground_contours:
        raise ValueError("No contours detected.")

    # Scale contours back to original size
    scaled_foreground_contours = [
        (contour * (contour_scale / scaling_factor)).astype(int)
        for contour in foreground_contours
    ]
    scaled_hole_contours = [
        [(hole * (contour_scale / scaling_factor)).astype(int) for hole in holes]
        for holes in hole_contours
    ]

    contour_indices = (
        set(keep_ids) - set(exclude_ids)
        if keep_ids
        else set(range(len(scaled_foreground_contours))) - set(exclude_ids or [])
    )

    polygons: List[Polygon] = []
    for idx in contour_indices:
        holes = (
            [hole.squeeze(1) for hole in scaled_hole_contours[idx]]
            if scaled_hole_contours[idx]
            else None
        )
        exterior = scaled_foreground_contours[idx].squeeze(1)
        polygon = Polygon(exterior, holes)
        if not polygon.is_valid:
            polygon = fix_invalid_polygon(polygon)
        polygons.append(polygon)

    return gpd.GeoDataFrame({"tissue_id": list(contour_indices)}, geometry=polygons)


def fix_invalid_polygon(polygon: Polygon) -> Polygon:
    for buffer_value in [0, 0.1, -0.1, 0.2]:
        new_polygon = polygon.buffer(buffer_value)
        if new_polygon.is_valid:
            return new_polygon
    raise ValueError("Failed to create a valid polygon")



================================================
File: sauron/utils/hdf5_utils.py
================================================
import pickle

import h5py
import numpy as np


def save_pickle(filename: str, obj: object) -> None:
    """
    Save an object to a pickle file.

    Args:
        filename (str): Filename for the pickle file.
        obj (object): Object to save.
    """
    with open(filename, "wb") as f:
        pickle.dump(obj, f)


def load_pickle(filename):
    with open(filename, "rb") as file:
        return pickle.load(file)


def save_hdf5(
    output_path, data_dict, attributes=None, mode="a", auto_chunk=True, chunk_size=None
):
    """
    Save data to an HDF5 file.

    Parameters:
    - output_path (str): Path to save the HDF5 file.
    - data_dict (dict): Dictionary of data to save.
    - attributes (dict, optional): Dictionary of attributes for each dataset.
    - mode (str): File open mode.
    - auto_chunk (bool): Use automatic chunking.
    - chunk_size (int, optional): Chunk size if auto_chunk is False.
    """
    with h5py.File(output_path, mode) as h5_file:
        for key, value in data_dict.items():
            value_shape = value.shape
            if value.ndim == 1:
                value = np.expand_dims(value, axis=1)
                value_shape = value.shape

            if key not in h5_file:
                data_type = (
                    h5py.string_dtype(encoding="utf-8")
                    if value.dtype == np.object_
                    else value.dtype
                )
                chunks = True if auto_chunk else (chunk_size,) + value_shape[1:]
                try:
                    dataset = h5_file.create_dataset(
                        key,
                        shape=value_shape,
                        chunks=chunks,
                        maxshape=(None,) + value_shape[1:],
                        dtype=data_type,
                    )
                    if attributes and key in attributes:
                        for attr_key, attr_value in attributes[key].items():
                            dataset.attrs[attr_key] = attr_value
                    dataset[:] = value
                except Exception as e:
                    print(f"Error encoding {key} of dtype {data_type} into HDF5: {e}")
            else:
                dataset = h5_file[key]
                dataset.resize(len(dataset) + value_shape[0], axis=0)
                assert dataset.dtype == value.dtype, "Data type mismatch"
                dataset[-value_shape[0] :] = value

    return output_path



================================================
File: sauron/utils/load_encoders.py
================================================
import traceback
from abc import ABC, abstractmethod

import torch
from torchvision import transforms

from sauron.utils.transform_utils import create_eval_transforms, get_normalization_stats


class BaseInferenceModel(torch.nn.Module, ABC):
    def __init__(self, weights_path=None, **build_kwargs):
        super().__init__()
        self.weights_path = weights_path
        self.model, self.eval_transforms, self.precision = self._build_model(
            weights_path, **build_kwargs
        )

    @abstractmethod
    def _build_model(self, weights_path, **build_kwargs):
        pass

    def forward(self, x):
        return self.model(x)


class CustomEncoder(BaseInferenceModel):
    def __init__(self, model, transforms, precision, weights_path=None):
        self.model = model
        self.eval_transforms = transforms
        self.precision = precision
        super().__init__(weights_path)

    def _build_model(self, weights_path, **kwargs):
        return self.model, self.eval_transforms, self.precision


class ConchEncoder(BaseInferenceModel):
    def _build_model(self, weights_path=None, **kwargs):
        try:
            from conch.open_clip_custom import create_model_from_pretrained
        except ImportError as e:
            traceback.print_exc()
            raise ImportError(
                "Please install CONCH via `pip install git+https://github.com/Mahmoodlab/CONCH.git`"
            ) from e

        try:
            model, preprocess = create_model_from_pretrained(
                "conch_ViT-B-16", "hf_hub:MahmoodLab/conch"
            )
        except Exception as e:
            traceback.print_exc()
            raise RuntimeError(
                "Failed to download CONCH model. Ensure you have access and your token is correctly registered."
            ) from e

        return model, preprocess, torch.float32

    def forward(self, x):
        return self.model.encode_image(x, proj_contrast=False, normalize=False)


class CTransPathEncoder(BaseInferenceModel):
    def _build_model(self, weights_path, **kwargs):
        from torch import nn

        from .ctranspath.ctran import ctranspath

        model = ctranspath(img_size=224)
        model.head = nn.Identity()

        state_dict = torch.load(weights_path)["model"]
        state_dict = {k: v for k, v in state_dict.items() if "attn_mask" not in k}
        model.load_state_dict(state_dict, strict=False)

        mean, std = get_normalization_stats("imagenet")
        eval_transforms = create_eval_transforms(mean, std)
        return model, eval_transforms, torch.float32


class HOptimus0Encoder(BaseInferenceModel):
    def _build_model(self, weights_path=None, **kwargs):
        import timm

        model = timm.create_model(
            "hf-hub:bioptimus/H-optimus-0",
            pretrained=True,
            init_values=1e-5,
            dynamic_img_size=False,
            **kwargs,
        )

        eval_transforms = transforms.Compose(
            [
                transforms.ToTensor(),
                transforms.Normalize(
                    mean=(0.707223, 0.578729, 0.703617),
                    std=(0.211883, 0.230117, 0.177517),
                ),
            ]
        )

        return model, eval_transforms, torch.float16


class PhikonEncoder(BaseInferenceModel):
    def _build_model(self, weights_path=None, **kwargs):
        from transformers import ViTModel

        model = ViTModel.from_pretrained("owkin/phikon", add_pooling_layer=False)
        mean, std = get_normalization_stats("imagenet")
        eval_transforms = create_eval_transforms(mean, std)
        return model, eval_transforms, torch.float32

    def forward(self, x):
        outputs = self.model(pixel_values=x)
        return outputs.last_hidden_state[:, 0, :]


class PlipEncoder(BaseInferenceModel):
    def _build_model(self, weights_path=None, **kwargs):
        from transformers import CLIPImageProcessor, CLIPVisionModel

        model_name = "vinid/plip"
        image_processor = CLIPImageProcessor.from_pretrained(model_name)
        model = CLIPVisionModel.from_pretrained(model_name)

        def eval_transform(img):
            return image_processor(img, return_tensors="pt")["pixel_values"].squeeze(0)

        return model, eval_transform, torch.float32

    def forward(self, x):
        return self.model(x).pooler_output


class RemedisEncoder(BaseInferenceModel):
    def _build_model(self, weights_path, **kwargs):
        from .remedis.remedis_models import resnet152_remedis

        model = resnet152_remedis(ckpt_path=weights_path, pretrained=True)
        return model, None, torch.float32

    def forward(self, x):
        x = x.permute((0, 3, 1, 2))
        return self.model(x)


class ResNet50Encoder(BaseInferenceModel):
    def _build_model(self, weights_path=None, pretrained=True, pool=True, **kwargs):
        import timm

        model = timm.create_model(
            "resnet50.tv_in1k",
            pretrained=pretrained,
            features_only=True,
            out_indices=[3],
            num_classes=0,
            **kwargs,
        )
        mean, std = get_normalization_stats("imagenet")
        eval_transforms = create_eval_transforms(mean, std)
        self.pool = torch.nn.AdaptiveAvgPool2d(1) if pool else None
        return model, eval_transforms, torch.float32

    def forward(self, x):
        features = self.model(x)
        if isinstance(features, list):
            features = features[0]
        if self.pool:
            features = self.pool(features).squeeze(-1).squeeze(-1)
        return features


class UniEncoder(BaseInferenceModel):
    def _build_model(self, weights_path=None, **kwargs):
        import timm
        from timm.data import resolve_data_config
        from timm.data.transforms_factory import create_transform

        model = timm.create_model(
            "hf-hub:MahmoodLab/uni",
            pretrained=True,
            dynamic_img_size=True,
            num_classes=0,
            init_values=1.0,
            **kwargs,
        )

        config = resolve_data_config(model.pretrained_cfg, model=model)
        eval_transforms = create_transform(**config)
        return model, eval_transforms, torch.float16


class GigaPathEncoder(BaseInferenceModel):
    def _build_model(self, weights_path=None, **kwargs):
        import timm

        model = timm.create_model(
            "hf_hub:prov-gigapath/prov-gigapath", pretrained=True, **kwargs
        )

        eval_transforms = transforms.Compose(
            [
                transforms.Resize(
                    256, interpolation=transforms.InterpolationMode.BICUBIC
                ),
                transforms.CenterCrop(224),
                transforms.ToTensor(),
                transforms.Normalize(
                    mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)
                ),
            ]
        )
        return model, eval_transforms, torch.float32


class VirchowEncoder(BaseInferenceModel):
    def _build_model(self, weights_path=None, return_cls=False, **kwargs):
        import timm
        from timm.data import resolve_data_config
        from timm.data.transforms_factory import create_transform

        model = timm.create_model(
            "hf-hub:paige-ai/Virchow",
            pretrained=True,
            mlp_layer=timm.layers.SwiGLUPacked,
            act_layer=torch.nn.SiLU,
            **kwargs,
        )
        config = resolve_data_config(model.pretrained_cfg, model=model)
        eval_transforms = create_transform(**config)
        self.return_cls = return_cls
        return model, eval_transforms, torch.float32

    def forward(self, x):
        output = self.model(x)
        class_token = output[:, 0]
        if self.return_cls:
            return class_token
        else:
            patch_tokens = output[:, 1:]
            embeddings = torch.cat([class_token, patch_tokens.mean(dim=1)], dim=-1)
            return embeddings


class Virchow2Encoder(BaseInferenceModel):
    def _build_model(self, weights_path=None, return_cls=False, **kwargs):
        import timm
        from timm.data import resolve_data_config
        from timm.data.transforms_factory import create_transform

        model = timm.create_model(
            "hf-hub:paige-ai/Virchow2",
            pretrained=True,
            mlp_layer=timm.layers.SwiGLUPacked,
            act_layer=torch.nn.SiLU,
            **kwargs,
        )
        config = resolve_data_config(model.pretrained_cfg, model=model)
        eval_transforms = create_transform(**config)
        self.return_cls = return_cls
        return model, eval_transforms, torch.float16

    def forward(self, x):
        output = self.model(x)
        class_token = output[:, 0]
        if self.return_cls:
            return class_token
        else:
            patch_tokens = output[:, 5:]
            embedding = torch.cat([class_token, patch_tokens.mean(dim=1)], dim=-1)
            return embedding


ENCODER_REGISTRY = {
    "remedis": RemedisEncoder,
    "resnet50": ResNet50Encoder,
    "gigapath": GigaPathEncoder,
    "virchow": VirchowEncoder,
    "virchow2": Virchow2Encoder,
    "hoptimus0": HOptimus0Encoder,
    "conch_v1": ConchEncoder,
    "uni_v1": UniEncoder,
    "uni_v2": UniEncoder,
    "ctranspath": CTransPathEncoder,
    "phikon": PhikonEncoder,
    "plip": PlipEncoder,
}


def get_encoder_class(encoder_name):
    try:
        return ENCODER_REGISTRY[encoder_name]
    except KeyError:
        raise ValueError(f"Unknown encoder name {encoder_name}")



================================================
File: sauron/utils/metrics.py
================================================
from typing import Tuple

import numpy as np
from sklearn.metrics import (
    roc_auc_score,
)
from sklearn.preprocessing import label_binarize
from sksurv.metrics import concordance_index_censored


# --- Metric Calculation Helpers ---
def _calculate_classification_auc(
    all_labels_np: np.ndarray, all_probs_np: np.ndarray, n_classes: int
) -> float:
    """Calculates AUC for classification."""
    if n_classes == 2:
        # Ensure there are at least two classes in labels for AUC calculation
        if len(np.unique(all_labels_np)) < 2:
            print(
                f"Warning: Only one class present in labels for binary AUC calculation. AUC set to 0.0."
            )
            return 0.0
        try:
            return roc_auc_score(all_labels_np, all_probs_np[:, 1])
        except ValueError as e:
            print(
                f"Warning: Could not calculate AUC (binary): {e}. Check if all_probs_np has two columns."
            )
            return 0.0
    else:  # Multi-class
        try:
            # Binarize labels against all potential classes (0 to n_classes-1)
            # This ensures consistent shape for roc_auc_score's `average` parameter.
            all_labels_bin = label_binarize(all_labels_np, classes=range(n_classes))

            # Check if any class has no instances after binarization.
            # Some classes might not be present in this specific batch/dataset split.
            # roc_auc_score (ovr) can handle cases where some classes specified in `labels`
            # are not present, as long as y_true and y_score shapes match on the number of classes.
            # However, if all_labels_bin has fewer columns than n_classes (e.g. max label is 1 for n_classes=3)
            # then roc_auc_score needs y_score to match that shape or careful slicing.
            # Assuming all_probs_np always has n_classes columns.
            if all_labels_bin.shape[1] < n_classes:
                # This can happen if the max label in all_labels_np is less than n_classes-1.
                # Pad all_labels_bin with zero columns for missing classes up to n_classes.
                padding = np.zeros(
                    (all_labels_bin.shape[0], n_classes - all_labels_bin.shape[1])
                )
                all_labels_bin = np.hstack((all_labels_bin, padding))

            # Ensure at least two classes are present in the actual data for meaningful OvR AUC
            if len(np.unique(all_labels_np)) < 2:
                print(
                    f"Warning: Less than 2 unique classes present in labels for multi-class AUC. AUC set to 0.0."
                )
                return 0.0

            return roc_auc_score(
                all_labels_bin, all_probs_np, multi_class="ovr", average="weighted"
            )
        except ValueError as e:
            # This can happen if all_probs_np doesn't have n_classes columns, or other inconsistencies.
            print(f"Warning: Could not calculate AUC (multi-class, weighted OvR): {e}")
            return 0.0


def _calculate_survival_c_index(
    all_event_times_np: np.ndarray,
    all_censorships_np: np.ndarray,
    all_risks_np: np.ndarray,
) -> float:
    """Calculates C-Index for survival."""
    if (
        len(all_event_times_np) == 0
        or len(all_censorships_np) == 0
        or len(all_risks_np) == 0
    ):
        print("Warning: Empty arrays provided for C-index calculation. Returning 0.0.")
        return 0.0
    event_observed = (1 - all_censorships_np).astype(bool)
    try:
        # Check for trivial cases that concordance_index_censored might not handle well
        if (
            len(np.unique(event_observed)) == 1 and not event_observed[0]
        ):  # All censored
            print(
                "Warning: All samples are censored. C-index is undefined, returning 0.0."
            )
            return 0.0
        if len(np.unique(all_event_times_np[event_observed])) < 1 and np.any(
            event_observed
        ):  # All observed events at the same time
            pass  # This case is handled by sksurv

        c_index, _, _, _, _ = concordance_index_censored(
            event_observed, all_event_times_np, all_risks_np
        )
        return c_index
    except Exception as e:  # Catch more general exceptions from sksurv if any
        print(f"Warning: Could not calculate C-index: {e}")
        return 0.0



================================================
File: sauron/utils/optimizers.py
================================================
from typing import Any

import torch
import torch.nn as nn
import torch.optim as optim


def get_optim(model: nn.Module, args: Any) -> optim.Optimizer:
    if args.opt == "adam":
        return optim.Adam(
            filter(lambda p: p.requires_grad, model.parameters()),
            lr=args.lr,
            weight_decay=args.reg,
        )
    elif args.opt == "adamw":
        return optim.AdamW(
            filter(lambda p: p.requires_grad, model.parameters()),
            lr=args.lr,
            weight_decay=args.reg,
        )
    elif args.opt == "sgd":
        return optim.SGD(
            filter(lambda p: p.requires_grad, model.parameters()),
            lr=args.lr,
            momentum=0.9,
            weight_decay=args.reg,
        )
    raise NotImplementedError(f"Optimizer {args.opt} not implemented")



================================================
File: sauron/utils/process_args.py
================================================
import argparse


def main(args):
    """Main function for training the model."""
    # Placeholder for actual training logic
    print(f"Starting training with the following configurations:\n{args}")


def get_args():
    """
    Parse command line arguments for Whole Slide Image (WSI) Training configurations.

    Returns:
        argparse.Namespace: Parsed command line arguments.
    """
    parser = argparse.ArgumentParser(
        description="Configurations for Whole Slide Image (WSI) Training",
        formatter_class=argparse.ArgumentDefaultsHelpFormatter,
    )

    # Dataset and paths
    parser.add_argument(
        "--data_root_dir",
        type=str,
        default=None,
        help="Root directory containing the dataset",
    )
    parser.add_argument(
        "--results_dir",
        type=str,
        default="./results",
        help="Directory to save training results and model checkpoints",
    )
    parser.add_argument(
        "--split_dir",
        type=str,
        default=None,
        help=(
            "Directory containing custom data splits. If not specified, splits will be "
            "inferred from the task and label_frac arguments"
        ),
    )
    parser.add_argument(
        "--dataset",
        type=str,
        default=None,
        help="Name of the dataset to use",
    )
    parser.add_argument(
        "--csv_fpath",
        type=str,
        default=None,
        help="Path to CSV file containing labels",
    )
    parser.add_argument(
        "--cohort",
        type=str,
        default=None,
        help="Cohort or disease model identifier",
    )

    # Training hyperparameters
    parser.add_argument(
        "--max_epochs",
        type=int,
        default=100,
        help="Maximum number of epochs to train",
    )
    parser.add_argument(
        "--lr",
        type=float,
        default=1e-4,
        help="Initial learning rate for the optimizer",
    )
    parser.add_argument(
        "--weight_decay",
        type=float,
        default=1e-5,
        help="Weight decay (L2 regularization) factor",
    )
    parser.add_argument(
        "--opt",
        type=str,
        choices=["adam", "adamW", "sgd"],
        default="adamW",
        help="Optimizer to use for training",
    )
    parser.add_argument(
        "--drop_out",
        type=float,
        default=0.25,
        help="Dropout probability for regularization",
    )
    parser.add_argument(
        "--early_stopping",
        action="store_true",
        help="Enable early stopping to prevent overfitting",
    )
    parser.add_argument(
        "--weighted_sample",
        action="store_true",
        help="Enable weighted sampling to handle class imbalance",
    )
    parser.add_argument(
        "--batch_size",
        type=int,
        default=32,
        help="Batch size for training",
    )
    parser.add_argument(
        "--n_subsamples",
        type=int,
        default=-1,
        help="Number of patches to sample during training",
    )
    parser.add_argument(
        "--scheduler",
        type=str,
        default=None,
        help="Scheduler used for training",
    )
    parser.add_argument(
        "--num_workers",
        type=int,
        default=1,
        help="Number of CPU workers for data loading",
    )
    parser.add_argument(
        "--temperature",
        type=float,
        default=0.001,
        help="Temperature parameter for training",
    )
    parser.add_argument(
        "--warmup",
        action="store_true",
        default=False,
        help="Enable warmup",
    )
    parser.add_argument(
        "--warmup_epochs",
        type=int,
        default=5,
        help="Number of epochs for warmup",
    )
    parser.add_argument(
        "--end_learning_rate",
        type=float,
        default=1e-8,
        help="End learning rate for scheduler",
    )
    parser.add_argument(
        "--num_gpus",
        type=int,
        default=1,
        help="Number of GPUs to use",
    )
    parser.add_argument(
        "--precision",
        type=str,
        default="float32",
        help="Precision for training (e.g., float32, float64)",
    )

    # Model configuration
    parser.add_argument(
        "--model_type",
        type=str,
        default="att_mil",
        choices=[
            "att_mil",
            "trans_mil",
            "diff_att_mil",
            "mean_mil",
            "max_mil",
            "Mamba",
            "BiMamba",
            "SRMamba",
        ],
        help="Type of model architecture (e.g., 'att_mil')",
    )
    parser.add_argument(
        "--wsi_encoder",
        type=str,
        default="abmil",
        help="WSI encoder to use",
    )
    parser.add_argument(
        "--backbone",
        type=str,
        default="resnet50",
        help="Backbone network for feature extraction",
    )
    parser.add_argument(
        "--activation",
        type=str,
        default="softmax",
        help="Activation function to use",
    )
    parser.add_argument(
        "--in_dim",
        type=int,
        default=1024,
        help="Input dimension for the model",
    )
    parser.add_argument(
        "--wsi_encoder_hidden_dim",
        type=int,
        default=512,
        help="Hidden dimension for WSI encoder",
    )
    parser.add_argument(
        "--n_heads",
        type=int,
        default=4,
        help="Number of heads in attention mechanism",
    )
    parser.add_argument(
        "--add_stain_encoding",
        action="store_true",
        default=False,
        help="Include stain encodings in the model",
    )
    parser.add_argument(
        "--mambamil_rate",
        type=int,
        default=10,
        help="Rate parameter for MambaMIL",
    )
    parser.add_argument(
        "--mambamil_layer",
        type=int,
        default=2,
        help="Number of layers in MambaMIL",
    )
    # Experiment settings
    parser.add_argument(
        "--task",
        type=str,
        required=True,
        help="Task name or identifier for the current experiment",
    )
    parser.add_argument(
        "--seed",
        type=int,
        default=1,
        help="Random seed for reproducibility",
    )
    parser.add_argument(
        "--label_frac",
        type=float,
        default=1.0,
        help="Fraction of training labels to use",
    )
    parser.add_argument(
        "--k",
        type=int,
        default=10,
        help="Total number of folds for cross-validation",
    )
    parser.add_argument(
        "--k_start",
        type=int,
        default=-1,
        help="Starting fold for cross-validation (-1 for last fold)",
    )
    parser.add_argument(
        "--k_end",
        type=int,
        default=-1,
        help="Ending fold for cross-validation (-1 for first fold)",
    )

    # Data processing
    parser.add_argument(
        "--patch_size",
        type=str,
        default="",
        help="Size of image patches (format: [height]x[width])",
    )
    parser.add_argument(
        "--resolution",
        type=str,
        default="20x",
        help=(
            "Magnification level to work with, e.g., '10x' or '10x_40x' "
            "(for multiple levels)"
        ),
    )
    parser.add_argument(
        "--early_fusion",
        action="store_true",
        default=False,
        help=(
            "Create an early fusion instead of a late fusion of models with "
            "multiple magnification levels"
        ),
    )
    parser.add_argument(
        "--preloading",
        type=str,
        default="no",
        choices=["yes", "no"],
        help="Whether to preload data into memory",
    )
    parser.add_argument(
        "--num_workers",
        type=int,
        default=4,
        help="Number of worker threads for data loading",
    )

    # Logging and debugging
    parser.add_argument(
        "--log_data",
        action="store_true",
        help="Enable logging of training data using TensorBoard",
    )
    parser.add_argument(
        "--testing",
        action="store_true",
        help="Enable testing/debugging mode",
    )
    parser.add_argument(
        "--log_ml",
        action="store_true",
        help="Enable logging of results in MLflow and TensorBoard",
    )
    parser.add_argument(
        "--wandb_project_name",
        type=str,
        default="WSI_Project",
        help="Project name to use for logging to WANDB",
    )
    parser.add_argument(
        "--wandb_entity",
        type=str,
        default="wsi_entity",
        help="Entity to use for logging to WANDB",
    )

    # Loss functions
    parser.add_argument(
        "--symmetric_cl",
        action="store_true",
        default=False,
        help="Use symmetric contrastive loss",
    )
    parser.add_argument(
        "--global_loss",
        type=str,
        default="-1",
        help="Loss used for global alignment of different WSI",
    )
    parser.add_argument(
        "--local_loss",
        type=str,
        default="-1",
        help="Loss used for local alignment of different WSI",
    )
    parser.add_argument(
        "--intra_modality_loss",
        type=str,
        default="-1",
        help="Info-NCE loss for comparing different views of the same WSI",
    )
    parser.add_argument(
        "--local_loss_weight",
        type=float,
        default=1.0,
        help="Weight for local loss",
    )

    # Pretrained model
    parser.add_argument(
        "--pretrained",
        type=str,
        default=None,
        help="Path to directory with checkpoint",
    )

    args = parser.parse_args()
    return args



================================================
File: sauron/utils/segmentation.py
================================================
import os
import pickle
from pathlib import Path
from typing import Optional, Union

import geopandas as gpd
import numpy as np
import openslide
import torch
from huggingface_hub import snapshot_download
from torch import nn
from torch.utils.data import DataLoader
from torchvision import transforms
from tqdm import tqdm

from sauron.data.GenericDataset import WholeSlideImageDataset
from sauron.utils.drawing_utils import visualize_tissue
from sauron.utils.gpd_utils import mask_to_geodataframe
from sauron.utils.WSIObjects import WholeSlideImage, wsi_factory


class TissueSegmenter:
    """
    A class for segmenting tissue in whole slide images using a DeepLabV3 model.
    """

    def __init__(
        self,
        model_name: str = "deeplabv3_resnet50_seg_v4.ckpt",
        batch_size: int = 8,
        download_model: bool = True,
        num_workers: int = 8,
        output_dir: Optional[str] = None,
    ):
        """
        Initialize the TissueSegmenter.

        Args:
            model_name (str): The name of the model checkpoint file.
            batch_size (int): Batch size for processing.
            download_model (bool): Whether to download the model if not present.
            num_workers (int): Number of worker threads for data loading.
            output_dir (Optional[str]): Directory to save output files.
        """
        self.model_name = model_name
        self.batch_size = batch_size
        self.download_model = download_model
        self.num_workers = num_workers
        self.output_dir = output_dir
        self.model = self._load_model()

    def _load_model(self) -> nn.Module:
        """
        Load the DeepLabV3 model with the specified weights.

        Returns:
            nn.Module: The loaded and initialized model.
        """
        model = torch.hub.load("pytorch/vision:v0.10.0", "deeplabv3_resnet50")
        model.classifier[4] = nn.Conv2d(in_channels=256, out_channels=2, kernel_size=1)

        model_dir = Path(__file__).resolve().parents[3] / "models"

        if self.download_model:
            snapshot_download(
                repo_id="MahmoodLab/hest-tissue-seg",
                repo_type="model",
                local_dir=model_dir,
                allow_patterns=self.model_name,
            )

        weights_path = model_dir / self.model_name

        if not weights_path.exists():
            raise FileNotFoundError(f"Model weights not found at {weights_path}")

        device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        checkpoint = torch.load(weights_path, map_location=device)

        # Clean up state dict
        state_dict = {
            key.replace("model.", ""): value
            for key, value in checkpoint["state_dict"].items()
            if "aux" not in key
        }
        model.load_state_dict(state_dict)
        model.to(device)
        model.eval()

        return model

    def segment_tissue(
        self,
        wsi: Union[np.ndarray, openslide.OpenSlide, WholeSlideImage],
        pixel_size: float,
        save_basename: Optional[str] = None,
        fast_mode: bool = False,
        output_pixel_size: float = 1.0,
        patch_size_micrometers: int = 512,
    ) -> gpd.GeoDataFrame:
        """
        Segment tissue regions from a whole slide image.

        Args:
            wsi (Union[np.ndarray, openslide.OpenSlide, WSI]): The whole slide image to process.
            pixel_size (float): Pixel size of the input image.
            save_basename (Optional[str]): Base name for saving output files.
            fast_mode (bool): If True, enables fast processing at lower resolution.
            output_pixel_size (float): Desired pixel size for output.
            patch_size_micrometers (int): Size of patches in micrometers.

        Returns:
            gpd.GeoDataFrame: GeoDataFrame containing the segmented tissue contours.
        """
        if fast_mode and output_pixel_size == 1.0:
            output_pixel_size = 2.0

        deeplab_patch_size = 512
        scale_factor = pixel_size / output_pixel_size
        source_patch_size = round(patch_size_micrometers / scale_factor)

        wsi = wsi_factory(wsi)
        patcher = wsi.create_patcher(deeplab_patch_size, pixel_size, output_pixel_size)

        eval_transforms = transforms.Compose(
            [
                transforms.ToTensor(),
                transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),
            ]
        )

        dataset = WholeSlideImageDataset(patcher, eval_transforms)
        dataloader = DataLoader(
            dataset, batch_size=self.batch_size, num_workers=self.num_workers
        )

        cols, rows = patcher.get_cols_rows()
        mask_width = deeplab_patch_size * cols
        mask_height = deeplab_patch_size * rows
        stitched_mask = np.zeros((mask_height, mask_width), dtype=np.uint8)
        scaling_ratio = deeplab_patch_size / source_patch_size

        device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

        with torch.inference_mode():
            for images, coordinates in tqdm(dataloader, total=len(dataloader)):
                images = images.to(device)
                outputs = self.model(images)["out"]
                predictions = outputs.argmax(dim=1).byte().cpu().numpy()
                x_coords, y_coords = coordinates[0], coordinates[1]

                for pred_mask, x, y in zip(predictions, x_coords, y_coords):
                    x_scaled = round(x.item() * scaling_ratio)
                    y_scaled = round(y.item() * scaling_ratio)
                    y_end = min(y_scaled + deeplab_patch_size, mask_height)
                    x_end = min(x_scaled + deeplab_patch_size, mask_width)
                    mask_slice = pred_mask[: y_end - y_scaled, : x_end - x_scaled]
                    stitched_mask[y_scaled:y_end, x_scaled:x_end] += mask_slice

        binary_mask = (stitched_mask > 0).astype(np.uint8)
        tissue_contours = mask_to_geodataframe(
            binary_mask,
            max_holes=5,
            pixel_size=pixel_size,
            contour_scale=1 / scaling_ratio,
        )

        if self.output_dir and save_basename:
            self._save_outputs(wsi, tissue_contours, save_basename)

        return tissue_contours

    def _save_outputs(
        self,
        wsi: WholeSlideImage,
        tissue_contours: gpd.GeoDataFrame,
        save_basename: str,
    ):
        """
        Save segmentation outputs to files.

        Args:
            wsi (WSI): The whole slide image object.
            tissue_contours (gpd.GeoDataFrame): GeoDataFrame of tissue contours.
            save_basename (str): Base name for output files.
        """
        os.makedirs(self.output_dir, exist_ok=True)

        # Save visualization
        vis_image = visualize_tissue(
            wsi, tissue_contours, line_thickness=5, target_width=1000
        )
        vis_path = os.path.join(self.output_dir, f"{save_basename}_tissue_mask.jpeg")
        vis_image.save(vis_path)

        # Save GeoJSON
        geojson_path = os.path.join(
            self.output_dir, f"{save_basename}_tissue_mask.geojson"
        )
        tissue_contours.to_file(geojson_path, driver="GeoJSON")

        # Save pickle
        pickle_path = os.path.join(self.output_dir, f"{save_basename}_tissue_mask.pkl")
        with open(pickle_path, "wb") as f:
            pickle.dump(tissue_contours, f)



================================================
File: sauron/utils/transform_utils.py
================================================
from typing import List, Optional, Tuple, Union

from torchvision import transforms

# Define normalization constants
NORMALIZATION_STATS: dict[str, Tuple[List[float], List[float]]] = {
    "imagenet": ([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),
    "openai_clip": (
        [0.48145466, 0.4578275, 0.40821073],
        [0.26862954, 0.26130258, 0.27577711],
    ),
    "none": (None, None),
}


def get_normalization_stats(
    norm_type: str = "imagenet",
) -> Tuple[Optional[List[float]], Optional[List[float]]]:
    try:
        return NORMALIZATION_STATS[norm_type]
    except KeyError:
        raise ValueError(f"Invalid normalization type: {norm_type}")


def create_eval_transforms(
    mean: Optional[List[float]],
    std: Optional[List[float]],
    img_size: int = -1,
    center_crop: bool = False,
) -> transforms.Compose:
    transform_list: List[transforms.Transform] = []

    if img_size > 0:
        transform_list.append(transforms.Resize(img_size))
    if center_crop:
        assert img_size > 0, "img_size must be set if center_crop is True"
        transform_list.append(transforms.CenterCrop(img_size))

    transform_list.append(transforms.ToTensor())
    if mean is not None and std is not None:
        transform_list.append(transforms.Normalize(mean, std))

    return transforms.Compose(transform_list)



================================================
File: sauron/utils/warnings.py
================================================



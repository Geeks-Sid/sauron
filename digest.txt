Here's the updated code for your `sauron` project, incorporating features and models from `mahmoodlab-trident`. I've focused on moving `trident`'s WSI handling, segmentation, and encoder factories into `sauron`'s `feature_extraction` module, and updated core scripts to use these new components.

I've made the following structural changes:

1.  **New `sauron/feature_extraction/concurrency.py`**: For multi-process batch caching from `trident`.
2.  **New `sauron/feature_extraction/wsi/` module**:
    *   `base.py`, `openslide.py`, `image.py`, `cucim.py`, `factory.py`, `patching.py`, `dataset.py` adapted from `trident/wsi_objects`.
    *   `__init__.py` to make it a package.
3.  **New `sauron/feature_extraction/models/` modules**:
    *   `segmentation/`: `factory.py`, `checkpoints.json`, `__init__.py` adapted from `trident/segmentation_models`.
    *   `patch_encoders/`: `factory.py`, `checkpoints.json`, `__init__.py`, `zoo/`, `utils/` adapted from `trident/patch_encoder_models`.
    *   `slide_encoders/`: `factory.py`, `checkpoints.json`, `__init__.py`, `zoo/` adapted from `trident/slide_encoder_models`.
    *   `__init__.py` for `sauron/feature_extraction/models`.
4.  **Updated `sauron/feature_extraction/processor.py`**: This is the core orchestration class, now utilizing all the new WSI and model factories. It includes `trident`'s robust error handling, logging, and configuration saving.
5.  **Updated `sauron/feature_extraction/utils/` module**:
    *   `io.py`: Now combines/replaces `sauron`'s old `gpd_utils.py` and `hdf5_utils.py`, integrating `trident`'s more comprehensive I/O utilities (locks, `save_h5`, `mask_to_gdf`, `collect_valid_slides`, `get_num_workers`, etc.).
    *   `config.py`: `JSONsaver` is aligned.
    *   `misc.py`: `deprecated` remains.
6.  **Updated `feature_extract` script**: Modified to use the new `sauron.feature_extraction.processor.Processor`.
7.  **Updated `sauron/parse/argparse.py`**: Added new arguments to support `trident`'s features (e.g., segmenter choices, caching, nested search, artifact removal).
8.  **Updated `sauron/data/classMILDataset.py` and `sauron/data/survMILDataset.py`**: Adjusted feature loading paths to align with the new output structure from `sauron.feature_extraction.processor`.
9.  **Updated `sauron/mil_models/models_factory.py`**: Updated imports to use the new encoder factories and added logic to infer `in_dim` from the chosen backbone.
10. **Updated `sauron/training/lightning_module.py` and `sauron/training/pipeline.py`**: Minor import adjustments to reflect the new module locations.
11. **Deleted/Deprecated Files**: `sauron/data/GenericDataset.py`, `sauron/data/dataloader.py`, `sauron/preprocess/`, `sauron/utils/WSIObjects.py`, `sauron/utils/segmentation.py`, `sauron/utils/load_encoders.py`, `sauron/utils/gpd_utils.py`, `sauron/utils/hdf5_utils.py`, `sauron/utils/transform_utils.py`, `sauron/utils/warnings.py`. Some general utilities from `sauron/utils/generic_utils.py` were kept, and some moved to `feature_extraction.utils.io.py`.

---

**Directory structure after changes:**

```
└── sauron/
    ├── README.md
    ├── LICENSE
    ├── feature_extract  # Updated
    ├── feature_extraction_plan
    ├── requirements.txt
    ├── train_mil.py     # Updated
    └── sauron/
        ├── requirements.txt
        ├── data/
        │   ├── classMILDataset.py  # Updated
        │   ├── data_utils.py       # Updated
        │   └── survMILDataset.py   # Updated
        ├── feature_extraction/
        │   ├── concurrency.py          # NEW
        │   ├── models/                 # NEW MODULE
        │   │   ├── __init__.py
        │   │   ├── patch_encoders/     # NEW MODULE
        │   │   │   ├── __init__.py
        │   │   │   ├── factory.py
        │   │   │   ├── checkpoints.json
        │   │   │   ├── zoo/
        │   │   │   │   ├── conchv1_5/
        │   │   │   │   │   ├── __init__.py
        │   │   │   │   │   └── conchv1_5.py
        │   │   │   │   └── ctranspath/
        │   │   │   │       ├── __init__.py
        │   │   │   │       └── ctran.py
        │   │   │   └── utils/
        │   │   │       ├── __init__.py
        │   │   │       ├── constants.py
        │   │   │       └── transform_utils.py
        │   │   ├── segmentation/       # NEW MODULE
        │   │   │   ├── __init__.py
        │   │   │   ├── factory.py
        │   │   │   └── checkpoints.json
        │   │   └── slide_encoders/     # NEW MODULE
        │   │       ├── __init__.py
        │   │       ├── factory.py
        │   │       ├── checkpoints.json
        │   │       └── zoo/
        │   │           ├── __init__.py
        │   │           └── reusable_blocks/
        │   │               ├── __init__.py
        │   │               └── ABMIL.py
        │   ├── processor.py            # Updated
        │   └── utils/                  # Updated
        │       ├── config.py           # Updated
        │       ├── io.py               # Updated (Major)
        │       └── misc.py             # Kept
        ├── losses/
        │   └── surv_loss.py
        ├── mil_models/
        │   ├── ABMIL.py
        │   ├── DiffABMIL.py
        │   ├── MambaMIL.py
        │   ├── MaxMIL.py
        │   ├── MeanMIL.py
        │   ├── S4MIL.py
        │   ├── TransMIL.py
        │   ├── WIKGMIL.py
        │   ├── activations.py
        │   ├── models_factory.py  # Updated
        │   └── mamba_ssm/
        │       ├── __init__.py
        │       ├── models/
        │       │   ├── __init__.py
        │       │   ├── config_mamba.py
        │       │   └── mixer_seq_simple.py
        │       ├── modules/
        │       │   ├── __init__.py
        │       │   ├── bimamba.py
        │       │   ├── mamba_simple.py
        │       │   └── srmamba.py
        │       ├── ops/
        │       │   ├── __init__.py
        │       │   ├── selective_scan_interface.py
        │       │   └── triton/
        │       │       ├── __init__.py
        │       │       ├── layernorm.py
        │       │       └── selective_state_update.py
        │       └── utils/
        │           ├── __init__.py
        │           └── generation.py
        ├── parse/
        │   └── argparse.py  # Updated
        ├── training/
        │   ├── __init__.py
        │   ├── lightning_module.py # Updated
        │   └── pipeline.py         # Updated
        └── utils/
            ├── callbacks.py
            ├── drawing_utils.py
            ├── environment_setup.py
            ├── filehandler.py
            ├── generic_utils.py
            ├── metrics.py
            ├── optimizers.py
            └── process_args.py
```

---

```python
# sauron/feature_extraction/concurrency.py
# This file is new, adapted from trident/Concurrency.py

import os
import gc
import torch
import shutil
from typing import List, Callable
from queue import Queue

def cache_batch(wsis: List[str], dest_dir: str) -> List[str]:
    """
    Copies WSIs to a local cache directory. Handles .mrxs subdirectories if present.

    Returns:
        List[str]: Paths to copied WSIs.
    """
    os.makedirs(dest_dir, exist_ok=True)
    copied = []

    for wsi_path in wsis:
        dest_path = os.path.join(dest_dir, os.path.basename(wsi_path))
        shutil.copy(wsi_path, dest_path)
        copied.append(dest_path)

        # Handle .mrxs specific subdirectories
        if wsi_path.lower().endswith('.mrxs'):
            mrxs_dir = os.path.splitext(wsi_path)[0]
            if os.path.exists(mrxs_dir) and os.path.isdir(mrxs_dir):
                dest_mrxs_dir = os.path.join(dest_dir, os.path.basename(mrxs_dir))
                shutil.copytree(mrxs_dir, dest_mrxs_dir)

    return copied


def batch_producer(
    queue: Queue,
    valid_slides: List[str],
    start_idx: int,
    batch_size: int,
    cache_dir: str,
) -> None:
    """
    Produces and caches batches of slides. Sends batch IDs to a queue for downstream processing.

    Args:
        queue (Queue): Queue to communicate with the consumer.
        valid_slides (List[str]): List of valid WSI paths.
        start_idx (int): Index in `valid_slides` to start batching from.
        batch_size (int): Number of slides per batch.
        cache_dir (str): Root directory where batches will be cached.
    """
    # Ensure start_idx is correctly used for actual slice
    for i in range(0, len(valid_slides), batch_size): # Iterate through all batches
        batch_paths = valid_slides[i:i + batch_size]
        batch_id = i // batch_size
        
        # Only process if this batch is within the requested start_idx range
        if i < start_idx:
            continue

        ssd_batch_dir = os.path.join(cache_dir, f"batch_{batch_id}")
        print(f"[PRODUCER] Caching batch {batch_id}: {ssd_batch_dir}")
        cache_batch(batch_paths, ssd_batch_dir)
        queue.put(batch_id)

    queue.put(None)  # Sentinel to signal completion


def batch_consumer(
    queue: Queue,
    task: str,
    cache_dir: str,
    processor_factory: Callable[[str], object],
    run_task_fn: Callable[[object, str], None],
) -> None:
    """
    Consumes cached batches from the queue, processes them, and optionally clears cache.

    Args:
        queue (Queue): Queue from the producer.
        task (str): Task name ('seg', 'coords', 'feat', or 'all').
        cache_dir (str): Directory containing cached batches.
        processor_factory (Callable): Function that creates a processor given a WSI dir.
        run_task_fn (Callable): Function to run a task given a processor and task name.
    """

    while True:
        batch_id = queue.get()
        if batch_id is None:
            queue.task_done()
            break

        ssd_batch_dir = os.path.join(cache_dir, f"batch_{batch_id}")
        print(f"[CONSUMER] Processing batch {batch_id} in {ssd_batch_dir}")

        processor = processor_factory(ssd_batch_dir)

        try:
            if task == 'all':
                for subtask in ['seg', 'coords', 'feat']:
                    run_task_fn(processor, subtask)
            else:
                run_task_fn(processor, task)
        finally:
            # release all WSI and processor resources
            if hasattr(processor, "release"):
                processor.release()
            del processor
            gc.collect()
            torch.cuda.empty_cache()

            print(f"[CONSUMER] Clearing cache for batch {batch_id}")
            shutil.rmtree(ssd_batch_dir, ignore_errors=True)
            queue.task_done()

```

```python
# sauron/feature_extraction/models/__init__.py
# New file
# This module will contain the factories for segmentation, patch, and slide encoders.
```

```python
# sauron/feature_extraction/models/patch_encoders/__init__.py
# New file, adapted from trident/patch_encoder_models/__init__.py

from sauron.feature_extraction.models.patch_encoders.factory import (
    encoder_factory,
    CustomInferenceEncoder,
    MuskInferenceEncoder,
    Conchv1InferenceEncoder,
    CTransPathInferenceEncoder,
    PhikonInferenceEncoder,
    ResNet50InferenceEncoder,
    UNIInferenceEncoder,
    UNIv2InferenceEncoder,
    GigaPathInferenceEncoder,
    VirchowInferenceEncoder,
    Virchow2InferenceEncoder,
    HOptimus0InferenceEncoder,
    HOptimus1InferenceEncoder,
    Conchv15InferenceEncoder,
    Phikonv2InferenceEncoder,
    LunitS8InferenceEncoder,
    HibouLInferenceEncoder,
    KaikoB16InferenceEncoder,
    KaikoB8InferenceEncoder,
    KaikoS16InferenceEncoder,
    KaikoS8InferenceEncoder,
    KaikoL14InferenceEncoder,
    Midnight12kInferenceEncoder,
)

__all__ = [
    "encoder_factory",
    "CustomInferenceEncoder",
    "MuskInferenceEncoder",
    "Conchv1InferenceEncoder",
    "CTransPathInferenceEncoder",
    "PhikonInferenceEncoder",
    "ResNet50InferenceEncoder",
    "UNIInferenceEncoder",
    "UNIv2InferenceEncoder",
    "GigaPathInferenceEncoder",
    "VirchowInferenceEncoder",
    "Virchow2InferenceEncoder",
    "HOptimus0InferenceEncoder",
    "HOptimus1InferenceEncoder",
    "Conchv15InferenceEncoder",
    "Phikonv2InferenceEncoder",
    "LunitS8InferenceEncoder",
    "HibouLInferenceEncoder",
    "KaikoB16InferenceEncoder",
    "KaikoB8InferenceEncoder",
    "KaikoS16InferenceEncoder",
    "KaikoS8InferenceEncoder",
    "KaikoL14InferenceEncoder",
    "Midnight12kInferenceEncoder",
]

```

```python
# sauron/feature_extraction/models/patch_encoders/factory.py
# New file, adapted from trident/patch_encoder_models/load.py

import traceback
from abc import abstractmethod
from typing import Literal, Optional, Callable
import torch
import os 
from torchvision import transforms

from sauron.feature_extraction.models.patch_encoders.utils.constants import get_constants
from sauron.feature_extraction.models.patch_encoders.utils.transform_utils import get_eval_transforms
from sauron.feature_extraction.utils.io import get_weights_path, has_internet_connection

"""
This file contains an assortment of pretrained patch encoders, all loadable via the encoder_factory() function.
"""

def encoder_factory(model_name: str, **kwargs):
    """
    Instantiate a patch encoder model by name.

    This factory function returns a pre-configured encoder model class based on the provided
    `model_name`. Each encoder is designed for extracting representations from image patches
    using specific backbones or pretraining strategies.

    Args:
        model_name (str): Name of the encoder to instantiate. Must be one of the following:
            - "conch_v1"
            - "conch_v15"
            - "uni_v1"
            - "uni_v2"
            - "ctranspath"
            - "phikon"
            - "phikon_v2"
            - "resnet50"
            - "gigapath"
            - "virchow"
            - "virchow2"
            - "hoptimus0"
            - "hoptimus1"
            - "musk"
            - "hibou_l"
            - "kaiko-vitb8"
            - "kaiko-vitb16"
            - "kaiko-vits8"
            - "kaiko-vits16"
            - "kaiko-vitl14"
            - "lunit-vits8"

        **kwargs: Optional keyword arguments passed directly to the encoder constructor. These
            may include parameters such as:
            - weights_path (str): Path to a local checkpoint (optional)
            - normalize (bool): Whether to normalize output embeddings (default: False)
            - with_proj (bool): Whether to apply the projection head (default: True)
            - any model-specific configuration parameters

    Returns:
        torch.nn.Module: An instance of the specified encoder model.

    Raises:
        ValueError: If `model_name` is not among the recognized encoder names.
    """
    if model_name == 'conch_v1':
        enc = Conchv1InferenceEncoder
    elif model_name == 'conch_v15':
        enc = Conchv15InferenceEncoder
    elif model_name == 'uni_v1':
        enc = UNIInferenceEncoder
    elif model_name == 'uni_v2':
        enc = UNIv2InferenceEncoder
    elif model_name == 'ctranspath':
        enc = CTransPathInferenceEncoder
    elif model_name == 'phikon':
        enc = PhikonInferenceEncoder
    elif model_name == 'resnet50':
        enc = ResNet50InferenceEncoder
    elif model_name == 'gigapath':
        enc = GigaPathInferenceEncoder
    elif model_name == 'virchow':
        enc = VirchowInferenceEncoder
    elif model_name == 'virchow2':
        enc = Virchow2InferenceEncoder
    elif model_name == 'hoptimus0':
        enc = HOptimus0InferenceEncoder
    elif model_name == 'hoptimus1':
        enc = HOptimus1InferenceEncoder
    elif model_name == 'phikon_v2':
        enc = Phikonv2InferenceEncoder
    elif model_name == 'musk':
        enc = MuskInferenceEncoder
    elif model_name == 'hibou_l':
        enc = HibouLInferenceEncoder
    elif model_name == 'kaiko-vitb8':
        enc = KaikoB8InferenceEncoder
    elif model_name == 'kaiko-vitb16':
        enc = KaikoB16InferenceEncoder
    elif model_name == 'kaiko-vits8':
        enc = KaikoS8InferenceEncoder
    elif model_name == 'kaiko-vits16':
        enc = KaikoS16InferenceEncoder
    elif model_name == 'kaiko-vitl14':
        enc = KaikoL14InferenceEncoder
    elif model_name == 'lunit-vits8':
        enc = LunitS8InferenceEncoder
    elif model_name == 'midnight12k':
        enc = Midnight12kInferenceEncoder
    else:
        raise ValueError(f"Unknown encoder name {model_name}")

    return enc(**kwargs)


class BasePatchEncoder(torch.nn.Module):

    _has_internet = has_internet_connection()
    
    def __init__(self, weights_path: Optional[str] = None, **build_kwargs):
        """
        Initialize BasePatchEncoder.

        Args:
            weights_path (Optional[str]): 
                Optional path to local model weights. If None, the model is loaded from the model registry or downloaded from Hugging Face Hub.
            **build_kwargs: 
                Additional keyword arguments passed to the `_build()` method to customize model creation.

        Attributes:
            enc_name (Optional[str]): Name of the encoder architecture (set during `_build()`).
            weights_path (Optional[str]): Path to local model weights (if provided).
            model (nn.Module): The instantiated encoder model.
            eval_transforms (Callable): Evaluation-time preprocessing transforms.
            precision (torch.dtype): Precision used for inference.
        """

        super().__init__()
        self.enc_name: Optional[str] = None
        self.weights_path: Optional[str] = weights_path
        self.model, self.eval_transforms, self.precision, self.embedding_dim = self._build(**build_kwargs)

    def ensure_valid_weights_path(self, weights_path):
        if weights_path and not os.path.isfile(weights_path):
            raise FileNotFoundError(f"Expected checkpoint at '{weights_path}', but the file was not found.")
    
    def ensure_has_internet(self, enc_name):
        if not BasePatchEncoder._has_internet:
            raise FileNotFoundError(
                f"Internet connection does seem not available. Auto checkpoint download is disabled."
                f"To proceed, please manually download: {enc_name},\n"
                f"and place it in the model registry in:\n`sauron/feature_extraction/models/patch_encoders/checkpoints.json`"
            )
        
    def _get_weights_path(self):
        """
        If self.weights_path is provided, use it. 
        If not provided, check the model registry. 
            If path in model registry is empty, auto-download from huggingface
            else, use the path from the registry.
        """
        if self.weights_path:
            self.ensure_valid_weights_path(self.weights_path)
            return self.weights_path
        else:
            weights_path = get_weights_path('patch', self.enc_name)
            self.ensure_valid_weights_path(weights_path)
            return weights_path

    def forward(self, x):
        """
        Can be overwritten if model requires special forward pass.
        """
        z = self.model(x)
        return z
        
    @abstractmethod
    def _build(self, **build_kwargs):
        # Returns: model, eval_transforms, precision, embedding_dim
        pass


class CustomInferenceEncoder(BasePatchEncoder):

    def __init__(self, enc_name, model, transforms, precision, embedding_dim):
        """
        Initialize a CustomInferenceEncoder from user-defined components.

        This class is used when the model, transforms, and precision are pre-instantiated externally 
        and should be injected directly into the encoder wrapper.

        Args:
            enc_name (str): 
                A unique name or identifier for the encoder (used for registry or logging).
            model (torch.nn.Module): 
                A PyTorch model instance to use for inference.
            transforms (Callable): 
                A callable (e.g., torchvision or timm transform) to preprocess input images for evaluation.
            precision (torch.dtype): 
                The precision to use for inference (e.g., torch.float32, torch.float16).
            embedding_dim (int): The output embedding dimension of the model.
        """
        super().__init__() # This calls BasePatchEncoder's init, but we override here.
        self.enc_name = enc_name
        self.model = model
        self.eval_transforms = transforms
        self.precision = precision
        self.embedding_dim = embedding_dim
        
    def _build(self):
        return self.model, self.eval_transforms, self.precision, self.embedding_dim


class MuskInferenceEncoder(BasePatchEncoder):
    
    def __init__(self, **build_kwargs):
        """
        MUSK initialization.
        """
        super().__init__(**build_kwargs)

    def _build(self, inference_aug=False, with_proj=False, out_norm=False, return_global=True):
        """
        Args:
            inference_aug (bool): Whether to use test-time multiscale augmentation. Default is False to allow for fair comparison with other models.
        """
        import timm
        
        self.enc_name = 'musk'
        self.inference_aug = inference_aug
        self.with_proj = with_proj
        self.out_norm = out_norm
        self.return_global = return_global
    
        try:
            from musk import utils, modeling
        except:
            traceback.print_exc()
            raise Exception("Please install MUSK `pip install fairscale git+https://github.com/lilab-stanford/MUSK`")

        weights_path = self._get_weights_path()

        if weights_path:
            raise NotImplementedError("MUSK doesn't support local model loading. PR welcome!")
        else:
            self.ensure_has_internet(self.enc_name)
            try:
                model = timm.create_model("musk_large_patch16_384")
                utils.load_model_and_may_interpolate("hf_hub:xiangjx/musk", model, 'model|module', '')
            except:
                traceback.print_exc()
                raise Exception("Failed to download MUSK model, make sure that you were granted access and that you correctly registered your token")
        
        from timm.data.constants import IMAGENET_INCEPTION_MEAN, IMAGENET_INCEPTION_STD
        from torchvision.transforms import InterpolationMode
        eval_transform = get_eval_transforms(IMAGENET_INCEPTION_MEAN, IMAGENET_INCEPTION_STD, target_img_size = 384, center_crop = True, interpolation=InterpolationMode.BICUBIC, antialias=True)
        precision = torch.float16
        embedding_dim = 1024 # MUSK outputs 1024-dim embeddings

        return model, eval_transform, precision, embedding_dim
    
    def forward(self, x):
        # Forward pass yields (vision_cls, text_cls). We only need vision_cls.
        return self.model(
                image=x,
                with_head=self.with_proj,
                out_norm=self.out_norm,
                ms_aug=self.inference_aug,
                return_global=self.return_global  
                )[0]  


class Conchv1InferenceEncoder(BasePatchEncoder):

    def __init__(self, **build_kwargs):
        """
        CONCH initialization.
        """
        super().__init__(**build_kwargs)

    def _build(self, with_proj=False, normalize=False):
        self.enc_name = 'conch_v1'
        self.with_proj = with_proj
        self.normalize = normalize

        try:
            from conch.open_clip_custom import create_model_from_pretrained
        except:
            traceback.print_exc()
            raise Exception("Please install CONCH `pip install git+https://github.com/Mahmoodlab/CONCH.git`")
        
        weights_path = self._get_weights_path()

        if weights_path:
            try:
                model, eval_transform = create_model_from_pretrained('conch_ViT-B-16', checkpoint_path=weights_path)
            except:
                traceback.print_exc()
                raise Exception(
                    f"Failed to create CONCH v1 model from local checkpoint at '{weights_path}'. "
                    "You can download the required `pytorch_model.bin` from: https://huggingface.co/MahmoodLab/CONCH."
                )
        else:
            self.ensure_has_internet(self.enc_name)
            try:
                model, eval_transform = create_model_from_pretrained('conch_ViT-B-16', checkpoint_path="hf_hub:MahmoodLab/conch")
            except:
                traceback.print_exc()
                raise Exception("Failed to download CONCH v1 model, make sure that you were granted access and that you correctly registered your token")
    
        precision = torch.float32
        embedding_dim = 512 # CONCH v1 outputs 512-dim embeddings
        
        return model, eval_transform, precision, embedding_dim
    
    def forward(self, x):
        return self.model.encode_image(x, proj_contrast=self.with_proj, normalize=self.normalize)
    

class CTransPathInferenceEncoder(BasePatchEncoder):

    def __init__(self, **build_kwargs):
        """
        CTransPath initialization.
        """
        super().__init__(**build_kwargs)

    def _build(self):
        from torchvision.transforms import InterpolationMode
        from torch import nn

        try:
            from sauron.feature_extraction.models.patch_encoders.zoo.ctranspath.ctran import ctranspath
        except:
            traceback.print_exc()
            raise Exception("Failed to import CTransPath model, make sure timm_ctp is installed. `pip install timm_ctp`")
        
        self.enc_name = 'ctranspath'
        weights_path = self._get_weights_path()

        model = ctranspath(img_size=224)
        model.head = nn.Identity()

        if not weights_path:
            self.ensure_has_internet(self.enc_name)
            try:
                from huggingface_hub import hf_hub_download   
                weights_path = hf_hub_download(
                    repo_id="MahmoodLab/hest-bench",
                    repo_type="dataset",
                    filename="CHIEF_CTransPath.pth",
                    subfolder="fm_v1/ctranspath",
                )
            except:
                traceback.print_exc()
                raise Exception("Failed to download CTransPath model, make sure that you were granted access and that you correctly registered your token")

        try:
            state_dict = torch.load(weights_path, weights_only=True)['model']
        except:
                traceback.print_exc()
                raise Exception(
                    f"Failed to create CTransPath model from local checkpoint at '{weights_path}'. "
                    "You can download the required `CHIEF_CTransPath.pth` from: https://huggingface.co/datasets/MahmoodLab/hest-bench/tree/main/fm_v1/ctranspath."
                )
        state_dict = {key: val for key, val in state_dict.items() if 'attn_mask' not in key}
        missing, unexpected = model.load_state_dict(state_dict, strict=False)
        assert len(unexpected) == 0, f"Unexpected keys found in state dict: {unexpected}"
        assert missing == ['layers.0.blocks.1.attn_mask', 'layers.1.blocks.1.attn_mask', 'layers.2.blocks.1.attn_mask', 'layers.2.blocks.3.attn_mask', 'layers.2.blocks.5.attn_mask'], f"Unexpected missing keys: {missing}"

        mean, std = get_constants('imagenet')
        eval_transform = get_eval_transforms(mean, std, target_img_size=224, interpolation=InterpolationMode.BILINEAR, max_size=None, antialias=True)

        precision = torch.float32
        embedding_dim = 768 # CTransPath outputs 768-dim embeddings

        return model, eval_transform, precision, embedding_dim


class PhikonInferenceEncoder(BasePatchEncoder):

    def __init__(self, **build_kwargs):
        """
        Phikon initialization.
        """
        super().__init__(**build_kwargs)

    def _build(self):
        from transformers import ViTModel
        from torchvision.transforms import InterpolationMode

        self.enc_name = 'phikon'
        weights_path = self._get_weights_path()

        if weights_path:
            try:
                model_dir = os.path.dirname(weights_path)
                model = ViTModel.from_pretrained(model_dir, add_pooling_layer=False, local_files_only=True)
            except:
                traceback.print_exc()
                raise Exception(
                    f"Failed to create Phikon model from local checkpoint at '{weights_path}'. "
                    "You can download the required `pytorch_model.bin` from: https://huggingface.co/owkin/phikon."
                )
        else:
            self.ensure_has_internet(self.enc_name)
            try:
                model = ViTModel.from_pretrained("owkin/phikon", add_pooling_layer=False)
            except:
                traceback.print_exc()
                raise Exception("Failed to download Phikon model, make sure that you were granted access and that you correctly registered your token")

        mean, std = get_constants('imagenet')
        eval_transform = get_eval_transforms(mean, std, target_img_size=224, interpolation=InterpolationMode.BILINEAR, max_size=None, antialias=True)
        precision = torch.float32
        embedding_dim = 768 # Phikon outputs 768-dim embeddings

        return model, eval_transform, precision, embedding_dim
    
    def forward(self, x):
        out = self.model(pixel_values=x)
        out = out.last_hidden_state[:, 0, :]
        return out
    
    def forward_features(self, x):
        out = self.model(pixel_values=x)
        return out
    

class HibouLInferenceEncoder(BasePatchEncoder):

    def __init__(self, **build_kwargs):
        """
        Hibou initialization.
        """
        super().__init__(**build_kwargs)

    def _build(self):
        from transformers import AutoModel
        from torchvision.transforms import InterpolationMode

        self.enc_name = 'hibou_l'
        weights_path = self._get_weights_path()

        if weights_path:
            raise NotImplementedError("Hibou-L doesn't support local model loading. PR welcome!")
        else:
            self.ensure_has_internet(self.enc_name)
            try:
                model = AutoModel.from_pretrained("histai/hibou-L", trust_remote_code=True)
            except:
                traceback.print_exc()
                raise Exception("Failed to download Hibou-L model, make sure that you were granted access and that you correctly registered your token")
        
        mean, std = get_constants('hibou')
        eval_transform = get_eval_transforms(mean, std, target_img_size=224, interpolation=InterpolationMode.BICUBIC, max_size=None, antialias=True)
        precision = torch.float32
        embedding_dim = 1024 # Hibou-L outputs 1024-dim embeddings

        return model, eval_transform, precision, embedding_dim
    
    def forward(self, x):
        out = self.model(pixel_values=x)
        out = out.pooler_output
        return out
    
    def forward_features(self, x):
        out = self.model(pixel_values=x)
        return out


class KaikoInferenceEncoder(BasePatchEncoder):
    MODEL_NAME = None  # set in subclasses
    HF_HUB_ID = None # set in subclasses
    IMG_SIZE = None

    def __init__(self, **build_kwargs):
        """
        Kaiko initialization.
        """
        super().__init__(**build_kwargs)

    def _build(self):
        import timm
        from torchvision.transforms import InterpolationMode
        self.enc_name = f"kaiko-{self.MODEL_NAME}"
        weights_path = self._get_weights_path()

        if weights_path:
            try:
                model = timm.create_model(
                    f"{self.HF_HUB_ID}",
                    num_classes=0,
                    checkpoint_path=weights_path,
                    img_size=self.IMG_SIZE,
                    dynamic_img_size=True
                )
            except:
                traceback.print_exc()
                raise Exception(
                    f"Failed to create Kaiko model from local checkpoint at '{weights_path}'. "
                    "You can download the required `model.safetensors` and `config.yaml` from: https://huggingface.co/collections/1aurent/kaikoai-models-66636c99d8e1e34bc6dcf795."
                )
        else:
            self.ensure_has_internet(self.enc_name)
            try:
                model = timm.create_model(
                    model_name=f"hf-hub:1aurent/{self.HF_HUB_ID}.kaiko_ai_towards_large_pathology_fms",
                    dynamic_img_size=True,
                    pretrained=True,
                    num_classes=0,
                    img_size=self.IMG_SIZE,
                )
            except:
                traceback.print_exc()
                raise Exception("Failed to download Kaiko model.")

        mean, std = get_constants("kaiko")
        eval_transform = get_eval_transforms(mean, std, target_img_size=224, center_crop=True, interpolation=InterpolationMode.BILINEAR, max_size=None, antialias=True)
        precision = torch.float32
        
        # Determine embedding_dim based on model name
        if 'vits8' in self.MODEL_NAME or 'vits16' in self.MODEL_NAME:
            embedding_dim = 384
        elif 'vitb8' in self.MODEL_NAME or 'vitb16' in self.MODEL_NAME:
            embedding_dim = 768
        elif 'vitl14' in self.MODEL_NAME:
            embedding_dim = 1024
        else:
            embedding_dim = None # Fallback or raise error if unknown

        return model, eval_transform, precision, embedding_dim


class KaikoS16InferenceEncoder(KaikoInferenceEncoder):
    MODEL_NAME = "vits16"
    HF_HUB_ID = "vit_small_patch16_224"
    IMG_SIZE = 224

    def __init__(self, **build_kwargs):
        """
        Kaiko Small 16 initialization.
        """
        super().__init__(**build_kwargs)
    

class KaikoS8InferenceEncoder(KaikoInferenceEncoder):
    MODEL_NAME = "vits8"
    HF_HUB_ID = "vit_small_patch8_224"
    IMG_SIZE = 224

    def __init__(self, **build_kwargs):
        """
        Kaiko Small 8 initialization.
        """
        super().__init__(**build_kwargs)
    

class KaikoB16InferenceEncoder(KaikoInferenceEncoder):
    MODEL_NAME = "vitb16"
    HF_HUB_ID = "vit_base_patch16_224"
    IMG_SIZE = 224

    def __init__(self, **build_kwargs):
        """
        Kaiko Base 16 initialization.
        """
        super().__init__(**build_kwargs)
    

class KaikoB8InferenceEncoder(KaikoInferenceEncoder):
    MODEL_NAME = "vitb8"
    HF_HUB_ID = "vit_base_patch8_224"
    IMG_SIZE = 224

    def __init__(self, **build_kwargs):
        """
        Kaiko Base 8 initialization.
        """
        super().__init__(**build_kwargs)
    

class KaikoL14InferenceEncoder(KaikoInferenceEncoder):
    MODEL_NAME = "vitl14"
    HF_HUB_ID = "vit_large_patch14_reg4_dinov2"
    IMG_SIZE = 518

    def __init__(self, **build_kwargs):
        """
        Kaiko Large 14 initialization.
        """
        super().__init__(**build_kwargs)
    

class ResNet50InferenceEncoder(BasePatchEncoder):

    def __init__(self, **build_kwargs):
        """
        ResNet50-ImageNet initialization.
        """
        super().__init__(**build_kwargs)

    def _build(
        self, 
        pretrained=True, 
        timm_kwargs={"features_only": True, "out_indices": [3], "num_classes": 0},
        img_size=224,
        pool=True
    ):
        import timm
        from torchvision.transforms import InterpolationMode

        self.enc_name = 'resnet50'
        weights_path = self._get_weights_path()

        if weights_path:
            try:
                model = timm.create_model("resnet50", pretrained=False, **timm_kwargs)
                model.load_state_dict(torch.load(weights_path, map_location="cpu"), strict=False)
            except:
                traceback.print_exc()
                raise Exception(
                    f"Failed to create ResNet50 model from local checkpoint at '{weights_path}'. "
                    "You can download the required `pytorch_model.bin` from: https://huggingface.co/timm/resnet50.tv_in1k."
                )
        else:
            self.ensure_has_internet(self.enc_name)
            try:
                model = timm.create_model("resnet50.tv_in1k", pretrained=pretrained, **timm_kwargs)
            except:
                traceback.print_exc()
                raise Exception("Failed to download ResNet50 model.")

        mean, std = get_constants('imagenet')
        eval_transform = get_eval_transforms(mean, std, target_img_size=img_size, center_crop=True, interpolation=InterpolationMode.BILINEAR, max_size=None, antialias=True)

        precision = torch.float32
        embedding_dim = 1024 # ResNet50 outputs 1024-dim features at layer 3 with features_only=True
        if pool:
            self.pool = torch.nn.AdaptiveAvgPool2d(1)
        else:
            self.pool = None
        
        return model, eval_transform, precision, embedding_dim
    
    def forward(self, x):
        out = self.forward_features(x)
        if self.pool:
            out = self.pool(out).squeeze(-1).squeeze(-1)
        return out
    
    def forward_features(self, x):
        out = self.model(x)
        if isinstance(out, list):
            assert len(out) == 1
            out = out[0]
        return out


class LunitS8InferenceEncoder(BasePatchEncoder):

    def __init__(self, **build_kwargs):
        """
        Lunit initialization.
        """
        super().__init__(**build_kwargs)

    def _build(self):
        import timm
        from timm.data import resolve_model_data_config
        from timm.data.transforms_factory import create_transform

        self.enc_name = 'lunit-vits8'
        weights_path = self._get_weights_path()

        if weights_path:
            try:
                timm_kwargs = {"img_size": 224}
                model = timm.create_model("vit_small_patch8_224", checkpoint_path=weights_path, **timm_kwargs)
            except:
                traceback.print_exc()
                raise Exception(
                    f"Failed to create Lunit-Small model from local checkpoint at '{weights_path}'. "
                    "You can download the required `model.safetensors` and `config.yaml` from: https://huggingface.co/1aurent/vit_small_patch8_224.lunit_dino."
                )
        else:
            self.ensure_has_internet(self.enc_name)
            try:
                model = timm.create_model("hf-hub:1aurent/vit_small_patch8_224.lunit_dino", pretrained=True)
            except:
                traceback.print_exc()
                raise Exception("Failed to download Lunit S8 model, make sure that you were granted access and that you correctly registered your token.")

        data_config = resolve_model_data_config(model)
        eval_transform = create_transform(**data_config, is_training=False)
        precision = torch.float32
        embedding_dim = 384 # Lunit-vits8 outputs 384-dim embeddings

        return model, eval_transform, precision, embedding_dim
    

class UNIInferenceEncoder(BasePatchEncoder):

    def __init__(self, **build_kwargs):
        """
        UNI initialization.
        """
        super().__init__(**build_kwargs)

    def _build(
        self, 
        timm_kwargs={"dynamic_img_size": True, "num_classes": 0, "init_values": 1e-5}
    ):
        import timm
        from torchvision import transforms

        self.enc_name = 'uni_v1'
        weights_path = self._get_weights_path()

        if weights_path:
            try:
                timm_kwargs = {
                    'img_size': 224,
                    'patch_size': 16,
                    'init_values': 1e-5,
                    'num_classes': 0,
                    'dynamic_img_size': True,
                }
                model = timm.create_model("vit_large_patch16_224", **timm_kwargs)
                model.load_state_dict(torch.load(weights_path, map_location="cpu"), strict=True)
            except:
                traceback.print_exc()
                raise Exception(
                    f"Failed to create UNI model from local checkpoint at '{weights_path}'. "
                    "You can download the required `pytorch_model.bin` from: https://huggingface.co/MahmoodLab/UNI."
                )
        else:
            self.ensure_has_internet(self.enc_name)
            try:
                model = timm.create_model("hf-hub:MahmoodLab/uni", pretrained=True, **timm_kwargs)
            except:
                traceback.print_exc()
                raise Exception("Failed to download UNI model, make sure that you were granted access and that you correctly registered your token")

        eval_transform = transforms.Compose([
            transforms.Resize(224),
            transforms.CenterCrop(224),
            transforms.ToTensor(),
            transforms.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),
        ])

        precision = torch.float16
        embedding_dim = 1024 # UNI v1 outputs 1024-dim embeddings
        
        return model, eval_transform, precision, embedding_dim
    

class UNIv2InferenceEncoder(BasePatchEncoder):

    def __init__(self, **build_kwargs):
        """
        UNIv2 initialization.
        """
        super().__init__(**build_kwargs)

    def _build(self):
        import timm
        from torchvision import transforms

        self.enc_name = 'uni_v2'
        weights_path = self._get_weights_path()

        timm_kwargs = {
            'img_size': 224,
            'patch_size': 14,
            'depth': 24,
            'num_heads': 24,
            'init_values': 1e-5,
            'embed_dim': 1536,
            'mlp_ratio': 2.66667 * 2,
            'num_classes': 0,
            'no_embed_class': True,
            'mlp_layer': timm.layers.SwiGLUPacked,
            'act_layer': torch.nn.SiLU,
            'reg_tokens': 8,
            'dynamic_img_size': True
        }

        if weights_path:
            try:
                model = timm.create_model(model_name='vit_giant_patch14_224', pretrained=False, **timm_kwargs)
                model.load_state_dict(torch.load(weights_path, map_location="cpu"), strict=True)
            except:
                traceback.print_exc()
                raise Exception(
                    f"Failed to create UNI2-h model from local checkpoint at '{weights_path}'. "
                    "You can download the required `pytorch_model.bin` from: https://huggingface.co/MahmoodLab/UNI2-h."
                )
        else:
            self.ensure_has_internet(self.enc_name)
            try:
                model = timm.create_model("hf-hub:MahmoodLab/UNI2-h", pretrained=True, **timm_kwargs)
            except:
                traceback.print_exc()
                raise Exception("Failed to download UNI v2 model, make sure that you were granted access and that you correctly registered your token")

        eval_transform = transforms.Compose([
            transforms.Resize(224),
            transforms.CenterCrop(224),
            transforms.ToTensor(),
            transforms.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),
        ])

        precision = torch.bfloat16
        embedding_dim = 1536 # UNI v2 outputs 1536-dim embeddings
        
        return model, eval_transform, precision, embedding_dim
    

class GigaPathInferenceEncoder(BasePatchEncoder):

    def __init__(self, **build_kwargs):
        """
        GigaPath initialization.
        """
        super().__init__(**build_kwargs)

    def _build(
        self, 
    ):
        import timm
        # NOTE: Original trident code hardcoded timm==0.9.16. Consider if this is still a strict requirement or a warning.
        # assert timm.__version__ == '0.9.16', f"Gigapath requires timm version 0.9.16, but found {timm.__version__}. Please install the correct version using `pip install timm==0.9.16`"
        from torchvision import transforms

        self.enc_name = 'gigapath'
        weights_path = self._get_weights_path()

        if weights_path:
            try:
                timm_kwargs = {
                    "img_size": 224,
                    "in_chans": 3,
                    "patch_size": 16,
                    "embed_dim": 1536,
                    "depth": 40,
                    "num_heads": 24,
                    "mlp_ratio": 5.33334,
                    "num_classes": 0
                }
                model = timm.create_model("vit_giant_patch14_dinov2", pretrained=False, **timm_kwargs)
                model.load_state_dict(torch.load(weights_path, map_location="cpu"), strict=True)
            except:
                traceback.print_exc()
                raise Exception(
                    f"Failed to create GigaPath model from local checkpoint at '{weights_path}'. "
                    "You can download the required `pytorch_model.bin` from: https://huggingface.co/prov-gigapath/prov-gigapath."
                )
        else:
            self.ensure_has_internet(self.enc_name)
            try:
                model = timm.create_model("hf_hub:prov-gigapath/prov-gigapath", pretrained=True)
            except:
                traceback.print_exc()
                raise Exception("Failed to download GigaPath model, make sure that you were granted access and that you correctly registered your token")

        mean, std = get_constants('imagenet')
        eval_transform = transforms.Compose(
            [
                transforms.Resize(256, interpolation=transforms.InterpolationMode.BICUBIC),
                transforms.CenterCrop(224),
                transforms.ToTensor(),
                transforms.Normalize(mean, std),
            ]
        )
        precision = torch.float32
        embedding_dim = 1536 # GigaPath outputs 1536-dim embeddings

        return model, eval_transform, precision, embedding_dim

    
class VirchowInferenceEncoder(BasePatchEncoder):
    import timm # Keep local import for scope
    
    def __init__(self, **build_kwargs):
        """
        Virchow initialization.
        """
        super().__init__(**build_kwargs)

    def _build(
        self,
        return_cls=False,
        timm_kwargs={'mlp_layer': timm.layers.SwiGLUPacked, 'act_layer': torch.nn.SiLU}
    ):
        import timm
        import torchvision
        from torchvision import transforms

        self.enc_name = 'virchow'
        weights_path = self._get_weights_path()

        if weights_path:
            try:
                timm_kwargs = {
                    "img_size": 224,
                    "init_values": 1e-5,
                    "num_classes": 0,
                    "mlp_ratio": 5.3375,
                    "global_pool": "",
                    "dynamic_img_size": True,
                    'mlp_layer': timm.layers.SwiGLUPacked,
                    'act_layer': torch.nn.SiLU,
                }
                model = timm.create_model("vit_huge_patch14_224", **timm_kwargs)
                model.load_state_dict(state_dict=torch.load(weights_path, map_location="cpu"), strict=True)
            except:
                traceback.print_exc()
                raise Exception(
                    f"Failed to create Virchow model from local checkpoint at '{weights_path}'. "
                    "You can download the required `pytorch_model.bin` from: https://huggingface.co/paige-ai/Virchow."
                )
        else:
            self.ensure_has_internet(self.enc_name)
            try:
                model = timm.create_model("hf-hub:paige-ai/Virchow", pretrained=True, **timm_kwargs)
            except:
                traceback.print_exc()
                raise Exception("Failed to download Virchow model, make sure that you were granted access and that you correctly registered your token")

        eval_transform = transforms.Compose(
            [
                transforms.Resize(224, interpolation=torchvision.transforms.InterpolationMode.BICUBIC),
                transforms.ToTensor(),
                transforms.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),
            ]
        )
        precision = torch.float16
        embedding_dim = 2560 # Virchow outputs 2560-dim embeddings
        self.return_cls = return_cls
        
        return model, eval_transform, precision, embedding_dim

    def forward(self, x):
        output = self.model(x)
        class_token = output[:, 0]

        if self.return_cls:
            return class_token
        else:
            patch_tokens = output[:, 1:]
            embeddings = torch.cat([class_token, patch_tokens.mean(1)], dim=-1)
            return embeddings


class Virchow2InferenceEncoder(BasePatchEncoder):
    import timm # Keep local import for scope
    
    def __init__(self, **build_kwargs):
        """
        Virchow 2 initialization.
        """
        super().__init__(**build_kwargs)

    def _build(
        self,
        return_cls=False,
        timm_kwargs={'mlp_layer': timm.layers.SwiGLUPacked, 'act_layer': torch.nn.SiLU}
    ):
        import timm
        import torchvision
        from torchvision import transforms

        self.enc_name = 'virchow2'
        weights_path = self._get_weights_path()

        if weights_path:
            try:
                timm_kwargs = {
                    "img_size": 224,
                    "init_values": 1e-5,
                    "num_classes": 0,
                    "reg_tokens": 4,
                    "mlp_ratio": 5.3375,
                    "global_pool": "",
                    "dynamic_img_size": True,
                    'mlp_layer': timm.layers.SwiGLUPacked,
                    'act_layer': torch.nn.SiLU,
                }
                model = timm.create_model("vit_huge_patch14_224", **timm_kwargs)
                model.load_state_dict(torch.load(weights_path, map_location="cpu"), strict=True)
            except:
                traceback.print_exc()
                raise Exception(
                    f"Failed to create Virchow2 model from local checkpoint at '{weights_path}'. "
                    "You can download the required `pytorch_model.bin` from: https://huggingface.co/paige-ai/Virchow2."
                )
        else:
            self.ensure_has_internet(self.enc_name)
            try:
                model = timm.create_model("hf-hub:paige-ai/Virchow2", pretrained=True, **timm_kwargs)
            except:
                traceback.print_exc()
                raise Exception("Failed to download Virchow-2 model, make sure that you were granted access and that you correctly registered your token")
        
        eval_transform = transforms.Compose(
            [
                transforms.Resize(224, interpolation=torchvision.transforms.InterpolationMode.BICUBIC),
                transforms.ToTensor(),
                transforms.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),
            ]
        )
        precision = torch.float16
        embedding_dim = 2560 # Virchow2 outputs 2560-dim embeddings
        self.return_cls = return_cls
        
        return model, eval_transform, precision, embedding_dim

    def forward(self, x):
        output = self.model(x)
    
        class_token = output[:, 0]
        if self.return_cls:
            return class_token
        
        patch_tokens = output[:, 5:] # Virchow2 uses first 5 tokens for reg_tokens
        embedding = torch.cat([class_token, patch_tokens.mean(1)], dim=-1)
        return embedding


class HOptimus0InferenceEncoder(BasePatchEncoder):

    def __init__(self, **build_kwargs):
        """
        H-Optimus0 initialization.
        """
        super().__init__(**build_kwargs)

    def _build(
        self,
        timm_kwargs={'init_values': 1e-5, 'dynamic_img_size': False}
    ):
        import timm
        # NOTE: Original trident code hardcoded timm==0.9.16. Consider if this is still a strict requirement or a warning.
        # assert timm.__version__ == '0.9.16', f"H-Optimus requires timm version 0.9.16, but found {timm.__version__}. Please install the correct version using `pip install timm==0.9.16`"
        from torchvision import transforms

        self.enc_name = 'hoptimus0'
        weights_path = self._get_weights_path()

        if weights_path:
            try:
                timm_kwargs = {
                    "num_classes": 0,
                    "img_size": 224,
                    "global_pool": "token",
                    'init_values': 1e-5,
                    'dynamic_img_size': False
                }
                model = timm.create_model("vit_giant_patch14_reg4_dinov2", **timm_kwargs)
                model.load_state_dict(torch.load(weights_path, map_location="cpu"), strict=True)
            except:
                traceback.print_exc()
                raise Exception(
                    f"Failed to create H-Optimus-0 model from local checkpoint at '{weights_path}'. "
                    "You can download the required `pytorch_model.bin` from: https://huggingface.co/bioptimus/H-optimus-0."
                )
        else:
            self.ensure_has_internet(self.enc_name)
            try:
                model = timm.create_model("hf-hub:bioptimus/H-optimus-0", pretrained=True, **timm_kwargs)
            except:
                traceback.print_exc()
                raise Exception("Failed to download HOptimus-0 model, make sure that you were granted access and that you correctly registered your token")

        eval_transform = transforms.Compose([
            transforms.Resize(224),  
            transforms.ToTensor(),
            transforms.Normalize(
                mean=(0.707223, 0.578729, 0.703617), 
                std=(0.211883, 0.230117, 0.177517)
            ),
        ])
        
        precision = torch.float16
        embedding_dim = 1536 # H-Optimus-0 outputs 1536-dim embeddings

        return model, eval_transform, precision, embedding_dim


class HOptimus1InferenceEncoder(BasePatchEncoder):

    def __init__(self, **build_kwargs):
        """
        H-Optimus1 initialization.
        """
        super().__init__(**build_kwargs)

    def _build(
        self,
        timm_kwargs={'init_values': 1e-5, 'dynamic_img_size': False},
        **kwargs
    ):
        import timm
        # NOTE: Original trident code hardcoded timm==0.9.16. Consider if this is still a strict requirement or a warning.
        # assert timm.__version__ == '0.9.16', f"H-Optimus requires timm version 0.9.16, but found {timm.__version__}. Please install the correct version using `pip install timm==0.9.16`"
        from torchvision import transforms

        self.enc_name = 'hoptimus1'
        weights_path = self._get_weights_path()

        if weights_path:
            try:
                timm_kwargs = {
                    "num_classes": 0,
                    "img_size": 224,
                    "global_pool": "token",
                    'init_values': 1e-5,
                    'dynamic_img_size': False
                }
                model = timm.create_model("vit_giant_patch14_reg4_dinov2", **timm_kwargs)
                model.load_state_dict(torch.load(weights_path, map_location="cpu"), strict=True)
            except:
                traceback.print_exc()
                raise Exception(
                    f"Failed to create H-Optimus-1 model from local checkpoint at '{weights_path}'. "
                    "You can download the required `pytorch_model.bin` from: https://huggingface.co/bioptimus/H-optimus-1."
                )
        else:
            self.ensure_has_internet(self.enc_name)
            try:
                model = timm.create_model("hf-hub:bioptimus/H-optimus-1", pretrained=True, **timm_kwargs)
            except:
                traceback.print_exc()
                raise Exception("Failed to download HOptimus-1 model, make sure that you were granted access and that you correctly registered your token")

        eval_transform = transforms.Compose([
            transforms.Resize(224),  
            transforms.ToTensor(),
            transforms.Normalize(
                mean=(0.707223, 0.578729, 0.703617), 
                std=(0.211883, 0.230117, 0.177517)
            ),
        ])
        
        precision = torch.float16
        embedding_dim = 1536 # H-Optimus-1 outputs 1536-dim embeddings

        return model, eval_transform, precision, embedding_dim


class Phikonv2InferenceEncoder(BasePatchEncoder):

    def __init__(self, **build_kwargs):
        """
        Phikonv2 initialization.
        """
        super().__init__(**build_kwargs)

    def _build(self):
        from transformers import AutoModel
        import torchvision.transforms as T
        from sauron.feature_extraction.models.patch_encoders.utils.constants import IMAGENET_MEAN, IMAGENET_STD

        self.enc_name = 'phikon_v2'
        weights_path = self._get_weights_path()

        if weights_path:
            try:
                model_dir = os.path.dirname(weights_path)
                model = AutoModel.from_pretrained(model_dir)
            except:
                traceback.print_exc()
                raise Exception(
                    f"Failed to create Phikonv2 model from local checkpoint at '{weights_path}'. "
                    "You can download the required `model.safetensors` and `config.json` from: https://huggingface.co/owkin/phikon-v2."
                )
        else:
            self.ensure_has_internet(self.enc_name)
            try:
                model = AutoModel.from_pretrained("owkin/phikon-v2")
            except:
                traceback.print_exc()
                raise Exception("Failed to download Phikon v2 model, make sure that you were granted access and that you correctly registered your token")

        eval_transform = T.Compose([
            T.Resize(224),  
            T.CenterCrop(224),  
            T.ToTensor(),
            T.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD)  # Normalize with specified mean and std
        ])

        precision = torch.float32
        embedding_dim = 1024 # Phikon v2 outputs 1024-dim embeddings

        return model, eval_transform, precision, embedding_dim
    
    def forward(self, x):
        out = self.model(x)
        out = out.last_hidden_state[:, 0, :]
        return out


class Conchv15InferenceEncoder(BasePatchEncoder):

    def __init__(self, **build_kwargs):
        """
        CONCHv1.5 initialization.
        """
        super().__init__(**build_kwargs)

    def _build(self, img_size=448):
        from sauron.feature_extraction.models.patch_encoders.zoo.conchv1_5.conchv1_5 import create_model_from_pretrained

        self.enc_name = 'conch_v15'
        weights_path = self._get_weights_path()

        if weights_path:
            try:
                model, eval_transform = create_model_from_pretrained(checkpoint_path=weights_path, img_size=img_size)
            except:
                traceback.print_exc()
                raise Exception(
                    f"Failed to create CONCH v1.5 model from local checkpoint at '{weights_path}'. "
                    "You can download the required `pytorch_model_vision.bin` and `config.json` from: https://huggingface.co/MahmoodLab/conchv1_5."
                )
        else:
            self.ensure_has_internet(self.enc_name)
            try:
                model, eval_transform = create_model_from_pretrained(checkpoint_path="hf_hub:MahmoodLab/conchv1_5", img_size=img_size)
            except:
                traceback.print_exc()
                raise Exception("Failed to download CONCH v1.5 model, make sure that you were granted access and that you correctly registered your token")

        precision = torch.float16
        embedding_dim = 768 # CONCH v1.5 outputs 768-dim embeddings

        return model, eval_transform, precision, embedding_dim


class Midnight12kInferenceEncoder(BasePatchEncoder):

    def __init__(self, **build_kwargs):
        """
        Midnight 12-k initialization by Kaiko.
        """
        super().__init__(**build_kwargs)

    def _build(self, return_type: Literal["cls_token", "cls+mean"] = "cls_token"):
        from transformers import AutoModel
        from sauron.feature_extraction.models.patch_encoders.utils.constants import KAIKO_MEAN, KAIKO_STD
        from torchvision import transforms

        self.enc_name = "midnight12k"
        weights_path = self._get_weights_path()

        if weights_path:
            try:
                model_dir = os.path.dirname(weights_path)
                model = AutoModel.from_pretrained(model_dir)
            except:
                traceback.print_exc()
                raise Exception(
                    f"Failed to create Midnight-12k model from local checkpoint at '{weights_path}'. "
                    "You can download the required `model.safetensors` and `config.json` from: https://huggingface.co/kaiko-ai/midnight."
                )
        else:
            self.ensure_has_internet(self.enc_name)
            try:
                model = AutoModel.from_pretrained("kaiko-ai/midnight")
            except:
                traceback.print_exc()
                raise Exception("Failed to download Midnight-12k model")

        eval_transform = transforms.Compose(
            [
                transforms.Resize(224),
                transforms.CenterCrop(224),
                transforms.ToTensor(),
                transforms.Normalize(mean=KAIKO_MEAN, std=KAIKO_STD),
            ]
        )

        precision = torch.float32
        embedding_dim = 3072 # Midnight-12k outputs 3072-dim embeddings (cls+mean)
        self.return_type = return_type

        return model, eval_transform, precision, embedding_dim

    def forward(self, x):
        out = self.model(x).last_hidden_state
        cls_token = out[:, 0, :]
        if self.return_type == "cls_token":
            return cls_token
        elif self.return_type == "cls+mean":
            patch_embeddings = out[:, 1:, :]
            return torch.cat([cls_token, patch_embeddings.mean(1)], dim=-1)
        else:
            raise ValueError(
                f"expected return_type to be one of 'cls_token' or 'cls+mean', but got '{self.return_type}'"
            )

```

```json
# sauron/feature_extraction/models/patch_encoders/checkpoints.json
# New file, adapted from trident/patch_encoder_models/local_ckpts.json

{
    "conch_v1": "",
    "uni_v1": "",
    "uni_v2": "",
    "ctranspath": "",
    "phikon": "",
    "resnet50": "",
    "gigapath": "",
    "virchow": "",
    "virchow2": "",
    "hoptimus0": "",
    "hoptimus1": "",
    "phikon_v2": "",
    "hibou_l": "",
    "kaiko-vitb8": "",
    "kaiko-vitb16": "",
    "kaiko-vits8": "",
    "kaiko-vits16": "",
    "kaiko-vitl14": "",
    "lunit-vits8": "",
    "conch_v15": "",
    "musk": "",
    "custom_encoder": ""
}
```

```python
# sauron/feature_extraction/models/patch_encoders/zoo/__init__.py
# New empty file, to make it a package
```

```python
# sauron/feature_extraction/models/patch_encoders/zoo/conchv1_5/__init__.py
# New empty file, to make it a package
```

```python
# sauron/feature_extraction/models/patch_encoders/zoo/conchv1_5/conchv1_5.py
# New file, copied from trident/patch_encoder_models/model_zoo/conchv1_5/conchv1_5.py
# No changes here, assumes relative imports for timm/einops are handled by pip install.

""" Modified based on https://github.com/bytedance/ibot/blob/da316d82636a7a7356835ef224b13d5f3ace0489/models/vision_transformer.py and timm (https://github.com/huggingface/pytorch-image-models) v0.9.2
"""
import math
from collections import OrderedDict
from functools import partial
from typing import Callable, List, Optional, Sequence, Tuple, Union

import timm
from timm.layers import Mlp, DropPath, trunc_normal_, PatchDropout, use_fused_attn
from timm.layers.helpers import to_2tuple
from timm.models._manipulate import named_apply, checkpoint_seq 
from timm.models.vision_transformer import init_weights_vit_timm, get_init_weights_vit, _load_weights 

from enum import Enum
from typing import Union
from typing import List, Optional, Callable

import torch
from torch import nn, einsum
import torch.nn.functional as F
import torch.utils.checkpoint
from torch.jit import Final

from einops import rearrange, repeat
from einops_exts import rearrange_many


class Format(str, Enum):
    NCHW = 'NCHW'
    NHWC = 'NHWC'
    NCL = 'NCL'
    NLC = 'NLC'


FormatT = Union[str, Format]

def get_spatial_dim(fmt: FormatT):
    fmt = Format(fmt)
    if fmt is Format.NLC:
        dim = (1,)
    elif fmt is Format.NCL:
        dim = (2,)
    elif fmt is Format.NHWC:
        dim = (1, 2)
    else:
        dim = (2, 3)
    return dim


def get_channel_dim(fmt: FormatT):
    fmt = Format(fmt)
    if fmt is Format.NHWC:
        dim = 3
    elif fmt is Format.NLC:
        dim = 2
    else:
        dim = 1
    return dim


def nchw_to(x: torch.Tensor, fmt: Format):
    if fmt == Format.NHWC:
        x = x.permute(0, 2, 3, 1)
    elif fmt == Format.NLC:
        x = x.flatten(2).transpose(1, 2)
    elif fmt == Format.NCL:
        x = x.flatten(2)
    return x


def nhwc_to(x: torch.Tensor, fmt: Format):
    if fmt == Format.NCHW:
        x = x.permute(0, 3, 1, 2)
    elif fmt == Format.NLC:
        x = x.flatten(1, 2)
    elif fmt == Format.NCL:
        x = x.flatten(1, 2).transpose(1, 2)
    return x

class PatchEmbed(nn.Module):
    """ 2D Image to Patch Embedding
    """
    output_fmt: Format

    def __init__(
            self,
            img_size: int = 224,
            patch_size: int = 16,
            in_chans: int = 3,
            embed_dim: int = 768,
            norm_layer: Optional[Callable] = None,
            flatten: bool = True,
            output_fmt: Optional[str] = None,
            bias: bool = True,
            masked_im_modeling: bool = False
    ):
        super().__init__()
        img_size = to_2tuple(img_size)
        patch_size = to_2tuple(patch_size)
        self.img_size = img_size
        self.patch_size = patch_size
        self.grid_size = (img_size[0] // patch_size[0], img_size[1] // patch_size[1])
        self.num_patches = self.grid_size[0] * self.grid_size[1]
        if output_fmt is not None:
            self.flatten = False
            self.output_fmt = Format(output_fmt)
        else:
            # flatten spatial dim and transpose to channels last, kept for bwd compat
            self.flatten = flatten
            self.output_fmt = Format.NCHW

        self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size, bias=bias)
        self.norm = norm_layer(embed_dim) if norm_layer else nn.Identity()

        ### Mask Image Modeling
        self.masked_im_modeling = masked_im_modeling
        if self.masked_im_modeling:
            self.masked_embed = nn.Parameter(torch.zeros(1, embed_dim))

    def forward(self, x, mask=None):
        B, C, H, W = x.shape
        x = self.proj(x)

        if mask is not None:
            x = self.mask_model(x, mask)

        if self.flatten:
            x = x.flatten(2).transpose(1, 2)  # NCHW -> NLC
        elif self.output_fmt != Format.NCHW:
            x = nchw_to(x, self.output_fmt)
        x = self.norm(x)
        return x

    def mask_model(self, x, mask):
        x.permute(0, 2, 3, 1)[mask, :] = self.masked_embed.to(x.dtype)
        return x

class Attention(nn.Module):
    fused_attn: Final[bool]

    def __init__(
            self,
            dim,
            num_heads=8,
            qkv_bias=False,
            qk_norm=False,
            attn_drop=0.,
            proj_drop=0.,
            norm_layer=nn.LayerNorm,
    ):
        super().__init__()
        assert dim % num_heads == 0, 'dim should be divisible by num_heads'
        self.num_heads = num_heads
        self.head_dim = dim // num_heads
        self.scale = self.head_dim ** -0.5
        self.fused_attn = use_fused_attn()
        self.fast_attn = self.fused_attn # legacy support

        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)
        self.q_norm = norm_layer(self.head_dim) if qk_norm else nn.Identity()
        self.k_norm = norm_layer(self.head_dim) if qk_norm else nn.Identity()
        self.attn_drop = nn.Dropout(attn_drop)
        self.proj = nn.Linear(dim, dim)
        self.proj_drop = nn.Dropout(proj_drop)

    def forward(self, x, return_attention=False):
        B, N, C = x.shape
        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, self.head_dim).permute(2, 0, 3, 1, 4)
        q, k, v = qkv.unbind(0)
        q, k = self.q_norm(q), self.k_norm(k)

        if self.fused_attn and (return_attention == False):
            x = F.scaled_dot_product_attention(
                q, k, v,
                dropout_p=self.attn_drop.p,
            )
        else:
            q = q * self.scale
            attn = q @ k.transpose(-2, -1)
            attn = attn.softmax(dim=-1)
            attn = self.attn_drop(attn)
            x = attn @ v

        x = x.transpose(1, 2).reshape(B, N, C)
        x = self.proj(x)
        x = self.proj_drop(x)

        if return_attention:
            return x, attn
        return x

class LayerScale(nn.Module):
    def __init__(self, dim, init_values=1e-5, inplace=False):
        super().__init__()
        self.inplace = inplace
        self.gamma = nn.Parameter(init_values * torch.ones(dim))

    def forward(self, x):
        return x.mul_(self.gamma) if self.inplace else x * self.gamma

class Block(nn.Module):
    def __init__(
            self,
            dim,
            num_heads,
            mlp_ratio=4.,
            qkv_bias=False,
            qk_norm=False,
            proj_drop=0., # proj -> proj_drop renamed
            attn_drop=0.,
            init_values=None,
            drop_path=0.,
            act_layer=nn.GELU,
            norm_layer=nn.LayerNorm,
            mlp_layer=Mlp,
    ):
        super().__init__()
        self.norm1 = norm_layer(dim)
        self.attn = Attention(
            dim,
            num_heads=num_heads,
            qkv_bias=qkv_bias,
            qk_norm=qk_norm,
            attn_drop=attn_drop,
            proj_drop=proj_drop,
            norm_layer=norm_layer,
        )
        self.ls1 = LayerScale(dim, init_values=init_values) if init_values else nn.Identity()
        self.drop_path1 = DropPath(drop_path) if drop_path > 0. else nn.Identity()

        self.norm2 = norm_layer(dim)
        self.mlp = mlp_layer( ### Mlp -> mlp_layer
            in_features=dim,
            hidden_features=int(dim * mlp_ratio),
            act_layer=act_layer,
            drop=proj_drop,
        )
        self.ls2 = LayerScale(dim, init_values=init_values) if init_values else nn.Identity()
        self.drop_path2 = DropPath(drop_path) if drop_path > 0. else nn.Identity()

    def forward(self, x):
        x = x + self.drop_path1(self.ls1(self.attn(self.norm1(x))))
        x = x + self.drop_path2(self.ls2(self.mlp(self.norm2(x))))
        return x

    def forward_with_attention(self, x):
        x_input = x
        x_postattn, attn = self.attn(self.norm1(x_input), return_attention=True)
        x_postls1 = x_input + self.drop_path1(self.ls1(x_postattn))
        x_postmlp = self.mlp(self.norm2(x_postls1))
        x_postls2 = x_postls1 + self.drop_path2(self.ls2(x_postmlp))
        return x_postls2, attn


class VisionTransformer(nn.Module):
    """ Vision Transformer
    A PyTorch impl of : `An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale`
        - https://arxiv.org/abs/2010.11929
    Adapted entirely from: https://github.com/huggingface/pytorch-image-models/blob/main/timm/models/vision_transformer.py
    """

    def __init__(
            self,
            img_size: Union[int, Tuple[int, int]] = 224,
            patch_size: Union[int, Tuple[int, int]] = 16,
            in_chans: int = 3,
            num_classes: int = 0,
            global_pool: str = 'token',
            embed_dim: int = 768,
            depth: int = 12,
            num_heads: int = 12,
            mlp_ratio: float = 4.,
            qkv_bias: bool = True,
            qk_norm: bool = False,
            init_values: Optional[float] = None,
            class_token: bool = True,
            no_embed_class: bool = False,
            pre_norm: bool = False,
            fc_norm: Optional[bool] = None,
            drop_rate: float = 0.,
            pos_drop_rate: float = 0.,   # new
            patch_drop_rate: float = 0., # new
            proj_drop_rate: float = 0.,  # renamed
            attn_drop_rate: float = 0.,  # same
            drop_path_rate: float = 0.,  # same
            weight_init: str = '',
            embed_layer: Callable = PatchEmbed,
            norm_layer: Optional[Callable] = None,
            act_layer: Optional[Callable] = None,
            block_fn: Callable = Block,     
            mlp_layer: Callable = Mlp,  # New
            return_all_tokens=False,    # iBOT
            masked_im_modeling=False    # iBOT
    ):
        """
        Args:
            img_size: Input image size.
            patch_size: Patch size.
            in_chans: Number of image input channels.
            num_classes: Mumber of classes for classification head.
            global_pool: Type of global pooling for final sequence (default: 'token').
            embed_dim: Transformer embedding dimension.
            depth: Depth of transformer.
            num_heads: Number of attention heads.
            mlp_ratio: Ratio of mlp hidden dim to embedding dim.
            qkv_bias: Enable bias for qkv projections if True.
            init_values: Layer-scale init values (layer-scale enabled if not None).
            class_token: Use class token.
            fc_norm: Pre head norm after pool (instead of before), if None, enabled when global_pool == 'avg'.
            drop_rate: Head dropout rate.
            pos_drop_rate: Position embedding dropout rate.
            attn_drop_rate: Attention dropout rate.
            drop_path_rate: Stochastic depth rate.
            weight_init: Weight initialization scheme.
            embed_layer: Patch embedding layer.
            norm_layer: Normalization layer.
            act_layer: MLP activation layer.
            block_fn: Transformer block layer.
        """
        super().__init__()
        assert global_pool in ('', 'avg', 'token')
        assert class_token or global_pool != 'token'
        use_fc_norm = global_pool == 'avg' if fc_norm is None else fc_norm
        norm_layer = norm_layer or partial(nn.LayerNorm, eps=1e-6)
        act_layer = act_layer or nn.GELU
        self.return_all_tokens = return_all_tokens     # from ibot
        self.masked_im_modeling = masked_im_modeling   # from ibot

        self.num_classes = num_classes
        self.global_pool = global_pool
        self.num_features = self.embed_dim = embed_dim  # num_features for consistency with other models
        self.num_prefix_tokens = 1 if class_token else 0
        self.no_embed_class = no_embed_class
        self.grad_checkpointing = False

        self.patch_embed = embed_layer(
            img_size=img_size,
            patch_size=patch_size,
            in_chans=in_chans,
            embed_dim=embed_dim,
            bias=not pre_norm,  # disable bias if pre-norm is used (e.g. CLIP)
            masked_im_modeling=masked_im_modeling,
        )
        num_patches = self.patch_embed.num_patches

        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim)) if class_token else None
        embed_len = num_patches if no_embed_class else num_patches + self.num_prefix_tokens
        self.pos_embed = nn.Parameter(torch.randn(1, embed_len, embed_dim) * .02)
        self.pos_drop = nn.Dropout(p=pos_drop_rate)

        if patch_drop_rate > 0:
            self.patch_drop = PatchDropout(
                patch_drop_rate,
                num_prefix_tokens=self.num_prefix_tokens,
            )
        else:
            self.patch_drop = nn.Identity()
        self.norm_pre = norm_layer(embed_dim) if pre_norm else nn.Identity()

        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, depth)]  # stochastic depth decay rule
        self.blocks = nn.Sequential(*[
            block_fn(
                dim=embed_dim,
                num_heads=num_heads,
                mlp_ratio=mlp_ratio,
                qkv_bias=qkv_bias,
                qk_norm=qk_norm,
                init_values=init_values,
                proj_drop=proj_drop_rate, # renamed
                attn_drop=attn_drop_rate,
                drop_path=dpr[i],
                norm_layer=norm_layer,
                act_layer=act_layer,
                mlp_layer=mlp_layer,  # new
            )
            for i in range(depth)])
        self.norm = norm_layer(embed_dim) if not use_fc_norm else nn.Identity()

        # Classifier Head
        self.fc_norm = norm_layer(embed_dim) if use_fc_norm else nn.Identity()
        self.head_drop = nn.Dropout(drop_rate) # new
        self.head = nn.Linear(self.embed_dim, num_classes) if num_classes > 0 else nn.Identity()

        if weight_init != 'skip':
            self.init_weights(weight_init)

    def init_weights(self, mode=''):
        assert mode in ('jax', 'jax_nlhb', 'moco', '')
        head_bias = -math.log(self.num_classes) if 'nlhb' in mode else 0.
        trunc_normal_(self.pos_embed, std=.02)
        if self.cls_token is not None:
            nn.init.normal_(self.cls_token, std=1e-6)
        named_apply(get_init_weights_vit(mode, head_bias), self)

    def _init_weights(self, m):
        # this fn left here for compat with downstream users
        init_weights_vit_timm(m)

    @torch.jit.ignore()
    def load_pretrained(self, checkpoint_path, prefix=''):
        _load_weights(self, checkpoint_path, prefix)

    @torch.jit.ignore
    def no_weight_decay(self):
        return {'pos_embed', 'cls_token', 'dist_token'}

    @torch.jit.ignore
    def group_matcher(self, coarse=False):
        return dict(
            stem=r'^cls_token|pos_embed|patch_embed',  # stem and embed
            blocks=[(r'^blocks\.(\d+)', None), (r'^norm', (99999,))]
        )

    @torch.jit.ignore
    def set_grad_checkpointing(self, enable=True):
        self.grad_checkpointing = enable

    @torch.jit.ignore
    def get_classifier(self):
        return self.head

    def reset_classifier(self, num_classes: int, global_pool=None):
        self.num_classes = num_classes
        if global_pool is not None:
            assert global_pool in ('', 'avg', 'token')
            self.global_pool = global_pool
        self.head = nn.Linear(self.embed_dim, num_classes) if num_classes > 0 else nn.Identity()

    def _pos_embed(self, x, w, h):
        if self.no_embed_class:
            # deit-3, updated JAX (big vision)
            # position embedding does not overlap with class token, add then concat
            x = x + self.interpolate_pos_encoding(x, w, h)
            if self.cls_token is not None:
                x = torch.cat((self.cls_token.expand(x.shape[0], -1, -1), x), dim=1)
        else:
            # original timm, JAX, and deit vit impl
            # pos_embed has entry for class token, concat then add
            if self.cls_token is not None:
                x = torch.cat((self.cls_token.expand(x.shape[0], -1, -1), x), dim=1)
            x = x + self.interpolate_pos_encoding(x, w, h)
        return self.pos_drop(x)

    def _intermediate_layers(
            self,
            x: torch.Tensor,
            n: Union[int, Sequence] = 1,
    ):
        outputs, num_blocks = [], len(self.blocks)
        take_indices = set(range(num_blocks - n, num_blocks) if isinstance(n, int) else n)

        # forward pass
        x = self.patch_embed(x)
        x = self._pos_embed(x)
        x = self.patch_drop(x)
        x = self.norm_pre(x)
        for i, blk in enumerate(self.blocks):
            x = blk(x)
            if i in take_indices:
                outputs.append(x)

        return outputs

    def get_intermediate_layers(
            self,
            x: torch.Tensor,
            n: Union[int, Sequence] = 1,
            reshape: bool = False,
            return_class_token: bool = False,
            norm: bool = False,
    ) -> Tuple[Union[torch.Tensor, Tuple[torch.Tensor]]]:
        # take last n blocks if n is an int, if in is a sequence, select by matching indices
        outputs = self._intermediate_layers(x, n)
        if norm:
            outputs = [self.norm(out) for out in outputs]
        class_tokens = [out[:, 0:self.num_prefix_tokens] for out in outputs]
        outputs = [out[:, self.num_prefix_tokens:] for out in outputs]

        if reshape:
            grid_size = self.patch_embed.grid_size
            outputs = [
                out.reshape(x.shape[0], grid_size[0], grid_size[1], -1).permute(0, 3, 1, 2).contiguous()
                for out in outputs
            ]

        if return_class_token:
            return tuple(zip(outputs, class_tokens))
        return tuple(outputs)

    def forward_features(self, x):
        B, nc, w, h = x.shape
        x = self.patch_embed(x)

        x = self._pos_embed(x, w, h)
        x = self.patch_drop(x)
        x = self.norm_pre(x)
        if self.grad_checkpointing and not torch.jit.is_scripting():
            x = checkpoint_seq(self.blocks, x)
        else:
            x = self.blocks(x)
        x = self.norm(x)
        return x

    def forward_head(self, x, pre_logits: bool = False):
        if self.global_pool:
            x = x[:, self.num_prefix_tokens:].mean(dim=1) if self.global_pool == 'avg' else x[:, 0]
        x = self.fc_norm(x)
        x = self.head_drop(x) # new
        return x if pre_logits else self.head(x)

    def get_attention(self, x, block_num: int=-1):
        B, nc, w, h = x.shape
        x = self.patch_embed(x)
        x = self._pos_embed(x, w, h)
        x = self.patch_drop(x)
        x = self.norm_pre(x)

        if block_num < 0:
            block_num = len(self.blocks) + block_num

        if self.grad_checkpointing and not torch.jit.is_scripting():
            raise NotImplementedError
        else:
            for i, blk in enumerate(self.blocks):
                if i < block_num:
                    x = blk(x)
                else:
                    x, attn = blk.forward_with_attention(x)
                    return attn

    def forward(self, x, return_all_tokens=None):
        x = self.forward_features(x)

        return_all_tokens = self.return_all_tokens if \
            return_all_tokens is None else return_all_tokens
        if return_all_tokens:
            return x

        x = self.forward_head(x)
        return x

    def interpolate_pos_encoding(self, x, w, h):
        npatch = x.shape[1] - 1
        N = self.pos_embed.shape[1] - 1
        if npatch == N and w == h:
            return self.pos_embed
        class_pos_embed = self.pos_embed[:, 0]
        patch_pos_embed = self.pos_embed[:, 1:]
        dim = x.shape[-1]
        w0 = w // self.patch_embed.patch_size[0]
        h0 = h // self.patch_embed.patch_size[0]
        # we add a small number to avoid floating point error in the interpolation
        # see discussion at https://github.com/facebookresearch/dino/issues/8
        w0, h0 = w0 + 0.1, h0 + 0.1
        patch_pos_embed = nn.functional.interpolate(
            patch_pos_embed.reshape(1, int(math.sqrt(N)), int(math.sqrt(N)), dim).permute(0, 3, 1, 2),
            scale_factor=(w0 / math.sqrt(N), h0 / math.sqrt(N)),
            mode='bicubic',
        )
        assert int(w0) == patch_pos_embed.shape[-2] and int(h0) == patch_pos_embed.shape[-1]
        patch_pos_embed = patch_pos_embed.permute(0, 2, 3, 1).view(1, -1, dim)
        return torch.cat((class_pos_embed.unsqueeze(0), patch_pos_embed), dim=1)

def vit_large(patch_size=16, **kwargs):
    model = VisionTransformer(
        patch_size=patch_size, embed_dim=1024, depth=24, num_heads=16, mlp_ratio=4, qkv_bias=True, **kwargs)
    return model

def resize_pos_embed(model, pos_embed_w, verbose=True):
    resized = False
    if pos_embed_w.shape != model.pos_embed.shape:
        # see https://github.com/rwightman/pytorch-image-models/blob/624266148d8fa5ddb22a6f5e523a53aaf0e8a9eb/timm/models/vision_transformer.py#L509
        interpolation = 'bilinear'
        antialias = False
        try:
            from timm.layers import resample_abs_pos_embed
        except ImportError:
            print(f'{__file__}: import timm utility functions failed with version {timm.__version__}!')
        num_prefix_tokens = 0 if getattr(model, 'no_embed_class', False) else getattr(model, 'num_prefix_tokens', 1)
        pos_embed_w = resample_abs_pos_embed(  # resize pos embedding when different size from pretrained weights
                    pos_embed_w,
                    new_size=model.patch_embed.grid_size,
                    num_prefix_tokens=num_prefix_tokens,
                    interpolation=interpolation,
                    antialias=antialias,
                    verbose=verbose,
                )

        resized = True
    if not resized and verbose:
        print('pos embedding not resized.')
    return pos_embed_w


class AttentionalPooler(nn.Module):

    def __init__(
            self,
            d_model: int,
            context_dim: int,
            n_head: int = 8,
            n_queries: int = 256,
            norm_layer: Callable = nn.LayerNorm
    ):
        super().__init__()
        self.query = nn.Parameter(torch.randn(n_queries, d_model))
        dim_head = d_model // n_head
        self.scale = dim_head ** -0.5
        self.heads = n_head
        inner_dim = dim_head * n_head
        self.ln_k = norm_layer(context_dim)
        self.ln_q = norm_layer(d_model)
        self.to_q = nn.Linear(d_model, inner_dim, bias=False)
        self.to_kv = nn.Linear(context_dim, inner_dim * 2, bias=False)
        self.to_out = nn.Linear(inner_dim, d_model, bias=False)
    
    def forward(self, x: torch.Tensor):
        if x.ndim == 3:
            x = rearrange(x, 'b n d -> b 1 n d')
        q = repeat(self.query, 'n d -> b m n d', b=x.shape[0], m=x.shape[1])
        x = self.ln_k(x)
        q = self.ln_q(q)
        b, m, h = *x.shape[:2], self.heads
        q = self.to_q(q)
        kv_input = x
        k, v = self.to_kv(kv_input).chunk(2, dim=-1)
        q, k, v = rearrange_many((q, k, v), 'b t n (h d) -> b h t n d', h=h)
        q = q * self.scale
        # attention
        sim = einsum('... i d, ... j d  -> ... i j', q, k)
        sim = sim - sim.amax(dim=-1, keepdim=True).detach()
        attn = sim.softmax(dim=-1)
        out = einsum('... i j, ... j d -> ... i d', attn, v)
        out = rearrange(out, 'b h t n d -> b t n (h d)', h=h)
        return self.to_out(out).squeeze(dim=1)


class CONCHVisionTower(nn.Module):

    def __init__(self):
        super().__init__()
        self.trunk = vit_large(init_values=1.0)
        self.attn_pool_contrast = AttentionalPooler(d_model=768, context_dim=1024, n_head=8, n_queries=1)
        self.ln_contrast = nn.LayerNorm(768)

    def forward(self, x):
        x = self.trunk.forward_features(x)
        x = self.attn_pool_contrast(x)[:, 0]
        x = self.ln_contrast(x)
        return x


def create_model_from_pretrained(
        checkpoint_path: str,
        cache_dir: Optional[str] = None,
        img_size: int = 448
    ):

    import torchvision.transforms as T
    from sauron.feature_extraction.models.patch_encoders.utils.constants import IMAGENET_MEAN, IMAGENET_STD
    model = CONCHVisionTower()
    # download checkpoint from huggingface if providing hub address 
    if checkpoint_path.startswith("hf_hub:"): 
        from huggingface_hub import hf_hub_download
        _ = hf_hub_download(
            checkpoint_path[len("hf_hub:"):], 
            cache_dir=cache_dir,
            filename="meta.yaml",
        )
        checkpoint_path = hf_hub_download(
            checkpoint_path[len("hf_hub:"):], 
            cache_dir=cache_dir,
            filename="pytorch_model_vision.bin",
        )

    # restore checkpoint 
    state_dict = torch.load(checkpoint_path, map_location="cpu", weights_only=True)
    state_dict['trunk.pos_embed'] = resize_pos_embed(model.trunk, state_dict['trunk.pos_embed'], verbose=False)
    model.load_state_dict(state_dict, strict=True)

    eval_transform = T.Compose([
            T.Resize(img_size, interpolation=T.InterpolationMode.BILINEAR),
            T.CenterCrop(img_size),
            T.ToTensor(),
            T.Normalize(IMAGENET_MEAN, IMAGENET_STD)
    ])


    return model, eval_transform

```

```python
# sauron/feature_extraction/models/patch_encoders/zoo/ctranspath/__init__.py
# New empty file, to make it a package
```

```python
# sauron/feature_extraction/models/patch_encoders/zoo/ctranspath/ctran.py
# New file, copied from trident/patch_encoder_models/model_zoo/ctranspath/ctran.py
# No changes here.

"""
Credits to original CTransPath implementation: https://github.com/Xiyue-Wang/TransPath/blob/main/ctran.py
"""

from timm_ctp.models.layers.helpers import to_2tuple
from timm_ctp import create_model as ctp_create_model
import torch.nn as nn
import pdb


class ConvStem(nn.Module):

    def __init__(self, img_size=224, patch_size=4, in_chans=3, embed_dim=768, norm_layer=None, flatten=True):
        super().__init__()

        assert patch_size == 4
        assert embed_dim % 8 == 0

        img_size = to_2tuple(img_size)
        patch_size = to_2tuple(patch_size)
        self.img_size = img_size
        self.patch_size = patch_size
        self.grid_size = (img_size[0] // patch_size[0], img_size[1] // patch_size[1])
        self.num_patches = self.grid_size[0] * self.grid_size[1]
        self.flatten = flatten


        stem = []
        input_dim, output_dim = 3, embed_dim // 8
        for l in range(2):
            stem.append(nn.Conv2d(input_dim, output_dim, kernel_size=3, stride=2, padding=1, bias=False))
            stem.append(nn.BatchNorm2d(output_dim))
            stem.append(nn.ReLU(inplace=True))
            input_dim = output_dim
            output_dim *= 2
        stem.append(nn.Conv2d(input_dim, embed_dim, kernel_size=1))
        self.proj = nn.Sequential(*stem)

        self.norm = norm_layer(embed_dim) if norm_layer else nn.Identity()

    def forward(self, x):
        B, C, H, W = x.shape
        # assert H == self.img_size[0] and W == self.img_size[1], \
        #     f"Input image size ({H}*{W}) doesn't match model ({self.img_size[0]}*{self.img_size[1]})."
        x = self.proj(x)
        if self.flatten:
            x = x.flatten(2).transpose(1, 2)  # BCHW -> BNC
        x = self.norm(x)
        return x

def ctranspath(img_size = 224, **kwargs):
    model = ctp_create_model('swin_tiny_patch4_window7_224', 
                                  embed_layer=ConvStem, 
                                  pretrained=False,
                                  img_size=img_size,
                                  **kwargs)
    return model

```

```python
# sauron/feature_extraction/models/patch_encoders/utils/__init__.py
# New empty file, to make it a package
```

```python
# sauron/feature_extraction/models/patch_encoders/utils/constants.py
# New file, copied from trident/patch_encoder_models/utils/constants.py

IMAGENET_MEAN = [0.485, 0.456, 0.406]
IMAGENET_STD = [0.229, 0.224, 0.225]
OPENAI_MEAN = [0.48145466, 0.4578275, 0.40821073]
OPENAI_STD = [0.26862954, 0.26130258, 0.27577711]
HIBOU_MEAN = [0.7068, 0.5755, 0.722]
HIBOU_STD = [0.195, 0.2316, 0.1816]
KAIKO_MEAN = [0.5, 0.5, 0.5]
KAIKO_STD = [0.5, 0.5, 0.5]
NONE_MEAN = None
NONE_STD = None

def get_constants(norm='imagenet'):
    if norm == 'imagenet':
        return IMAGENET_MEAN, IMAGENET_STD
    elif norm == 'openai_clip':
        return OPENAI_MEAN, OPENAI_STD
    elif norm == 'hibou':
        return HIBOU_MEAN, HIBOU_STD
    elif norm == 'none':
        return NONE_MEAN, NONE_STD
    elif norm == 'kaiko':
        return KAIKO_MEAN, KAIKO_STD
    else:
        raise ValueError(f"Invalid norm: {norm}")

```

```python
# sauron/feature_extraction/models/patch_encoders/utils/transform_utils.py
# New file, copied from trident/patch_encoder_models/utils/transform_utils.py

from torchvision import transforms

def get_eval_transforms(mean, std, target_img_size = -1, center_crop = False, **resize_kwargs):
    trsforms = []
    
    if target_img_size > 0:
        trsforms.append(transforms.Resize(target_img_size, **resize_kwargs))
    if center_crop:
        assert target_img_size > 0, "target_img_size must be set if center_crop is True"
        trsforms.append(transforms.CenterCrop(target_img_size))
        
    
    trsforms.append(transforms.ToTensor())
    if mean is not None and std is not None:
        trsforms.append(transforms.Normalize(mean, std))
    trsforms = transforms.Compose(trsforms)

    return trsforms
```

```python
# sauron/feature_extraction/models/segmentation/__init__.py
# New file, adapted from trident/segmentation_models/__init__.py

from sauron.feature_extraction.models.segmentation.factory import (
    segmentation_model_factory,
    HESTSegmenter,
    GrandQCSegmenter,
    GrandQCArtifactSegmenter
)

__all__ = [
    "segmentation_model_factory",
    "HESTSegmenter",
    "GrandQCSegmenter",
    "GrandQCArtifactSegmenter",
    ]
```

```python
# sauron/feature_extraction/models/segmentation/factory.py
# New file, adapted from trident/segmentation_models/load.py

import os
import torch
import torch.nn.functional as F
from torch import nn
from torchvision import transforms
from abc import abstractmethod

from sauron.feature_extraction.utils.io import get_dir, get_weights_path, has_internet_connection


class SegmentationModel(torch.nn.Module):

    _has_internet = has_internet_connection()

    def __init__(self, freeze=True, confidence_thresh=0.5, **build_kwargs):
        """
        Initialize Segmentation model wrapper.

        Args:
            freeze (bool, optional): If True, the model's parameters are frozen 
                (i.e., not trainable) and the model is set to evaluation mode. 
                Defaults to True.
            confidence_thresh (float, optional): Threshold for prediction confidence. 
                Predictions below this threshold may be filtered out or ignored. 
                Default is 0.5. Set to 0.4 to keep more tissue.
            **build_kwargs: Additional keyword arguments passed to the internal 
                `_build` method.

        Attributes:
            model (torch.nn.Module): The constructed model.
            eval_transforms (Callable): Transformations to apply to input data during inference.
        """
        super().__init__()
        self.model, self.eval_transforms, self.input_size, self.precision, self.target_mag = self._build(**build_kwargs)
        self.confidence_thresh = confidence_thresh
        self.model_name = self.__class__.__name__ # Store model name for config

        # Set all parameters to be non-trainable
        if freeze and self.model is not None:
            for param in self.model.parameters():
                param.requires_grad = False
            self.model.eval()
            
    def forward(self, image):
        """
        Can be overwritten if model requires special forward pass.
        """
        z = self.model(image)
        return z
        
    @abstractmethod
    def _build(self, **build_kwargs) -> tuple[nn.Module, transforms.Compose, int, torch.dtype, int]:
        """
        Build the segmentation model and preprocessing transforms.
        Returns: model, eval_transforms, input_size, precision, target_mag
        """
        pass


class HESTSegmenter(SegmentationModel):

    def __init__(self, **build_kwargs):
        """
        HESTSegmenter initialization.
        """
        super().__init__(**build_kwargs)

    def _build(self):
        """
        Build and load HESTSegmenter model.

        Returns:
            Tuple[nn.Module, transforms.Compose, int, torch.dtype, int]: Model, transforms, input size, precision, target magnification.
        """

        from torchvision.models.segmentation import deeplabv3_resnet50

        model_ckpt_name = 'deeplabv3_seg_v4.ckpt'
        weights_path = get_weights_path('seg', 'hest')

        # Check if a path is provided but doesn't exist
        if weights_path and not os.path.isfile(weights_path):
            raise FileNotFoundError(f"Expected checkpoint at '{weights_path}', but the file was not found.")

        # Initialize base model
        model = deeplabv3_resnet50(weights=None)
        model.classifier[4] = nn.Conv2d(256, 2, kernel_size=1, stride=1)

        if not weights_path:
            if not SegmentationModel._has_internet:
                raise FileNotFoundError(
                    f"Internet connection not available and checkpoint not found locally in model registry at sauron/feature_extraction/models/segmentation/checkpoints.json.\n\n"
                    f"To proceed, please manually download {model_ckpt_name} from:\n"
                    f"https://huggingface.co/MahmoodLab/hest-tissue-seg/\n"
                    f"and place it at:\ncheckpoints.json" # refers to the local_ckpts.json in this directory
                )

            # If internet is available, download from HuggingFace
            from huggingface_hub import snapshot_download
            checkpoint_dir = snapshot_download(
                repo_id="MahmoodLab/hest-tissue-seg",
                repo_type='model',
                local_dir=get_dir(), # Cache directory resolved by get_dir()
                cache_dir=get_dir(),
                allow_patterns=[model_ckpt_name]
            )

            weights_path = os.path.join(checkpoint_dir, model_ckpt_name)

        # Load and clean checkpoint
        checkpoint = torch.load(weights_path, map_location='cpu')
        state_dict = {
            k.replace('model.', ''): v
            for k, v in checkpoint.get('state_dict', {}).items()
            if 'aux' not in k
        }

        model.load_state_dict(state_dict)

        # Store configuration
        input_size = 512
        precision = torch.float16
        target_mag = 10

        eval_transforms = transforms.Compose([
            transforms.ToTensor(),
            transforms.Normalize(mean=(0.485, 0.456, 0.406),
                                 std=(0.229, 0.224, 0.225))
        ])

        return model, eval_transforms, input_size, precision, target_mag
    
    def forward(self, image: torch.Tensor) -> torch.Tensor:
        # input should be of shape (batch_size, C, H, W)
        assert len(image.shape) == 4, f"Input must be 4D image tensor (shape: batch_size, C, H, W), got {image.shape} instead"
        logits = self.model(image)['out']
        softmax_output = F.softmax(logits, dim=1)
        predictions = (softmax_output[:, 1, :, :] > self.confidence_thresh).to(torch.uint8)  # Shape: [bs, 512, 512]
        return predictions
        

class JpegCompressionTransform:
    def __init__(self, quality=80):
        self.quality = quality

    def __call__(self, image):
        import cv2
        import numpy as np
        from PIL import Image
        # Convert PIL Image to NumPy array
        image = np.array(image)

        # Apply JPEG compression
        encode_param = [int(cv2.IMWRITE_JPEG_QUALITY), self.quality]
        _, image = cv2.imencode('.jpg', image, encode_param)
        image = cv2.imdecode(image, cv2.IMREAD_COLOR)

        # Convert back to PIL Image
        return Image.fromarray(image)


class GrandQCArtifactSegmenter(SegmentationModel):

    _class_mapping = {
        1: "Normal Tissue",
        2: "Fold",
        3: "Darkspot & Foreign Object",
        4: "PenMarking",
        5: "Edge & Air Bubble",
        6: "OOF",
        7: "Background"
    }

    def __init__(self, **build_kwargs):
        """
        GrandQCArtifactSegmenter initialization.
        """
        super().__init__(**build_kwargs)

    def _build(self, remove_penmarks_only=False):
        """
        Load the GrandQC artifact removal segmentation model.
        Credit: https://www.nature.com/articles/s41467-024-54769-y
        """

        import segmentation_models_pytorch as smp

        self.remove_penmarks_only = remove_penmarks_only  # ignore all other artifacts than penmakrs.
        model_ckpt_name = 'GrandQC_MPP1_state_dict.pth'
        encoder_name = 'timm-efficientnet-b0'
        encoder_weights = 'imagenet'
        weights_path = get_weights_path('seg', 'grandqc_artifact')

        # Verify that user-provided weights_path is valid
        if weights_path and not os.path.isfile(weights_path):
            raise FileNotFoundError(
                f"Expected checkpoint at '{weights_path}', but the file was not found."
            )

        # Initialize model
        model = smp.Unet(
            encoder_name=encoder_name,
            encoder_weights=encoder_weights,
            classes=8,
            activation=None,
        )

        # Attempt to download if file is missing and not already available
        if not weights_path:
            if not SegmentationModel._has_internet:
                raise FileNotFoundError(
                    f"Internet connection not available and checkpoint not found locally.\n\n"
                    f"To proceed, please manually download {model_ckpt_name} from:\n"
                    f"https://huggingface.co/MahmoodLab/hest-tissue-seg/\n"
                    f"and place it at:\ncheckpoints.json" # refers to the local_ckpts.json in this directory
                )

            from huggingface_hub import snapshot_download
            checkpoint_dir = snapshot_download(
                repo_id="MahmoodLab/hest-tissue-seg",
                repo_type='model',
                local_dir=get_dir(),
                cache_dir=get_dir(),
                allow_patterns=[model_ckpt_name],
            )

            weights_path = os.path.join(checkpoint_dir, model_ckpt_name)

        # Load checkpoint
        state_dict = torch.load(weights_path, map_location='cpu', weights_only=True)
        model.load_state_dict(state_dict)

        # Model config
        input_size = 512
        precision = torch.float32
        target_mag = 10 # This model is often used on 10x or 1x (MPP1)

        # Evaluation transforms
        eval_transforms = transforms.Compose([
            transforms.ToTensor(),
            transforms.Normalize(mean=[0.485, 0.456, 0.406],
                                 std=[0.229, 0.224, 0.225])
        ])

        return model, eval_transforms, input_size, precision, target_mag

    def forward(self, image: torch.Tensor) -> torch.Tensor:
        """
        Custom forward pass.
        """
        logits = self.model.predict(image)
        probs = torch.softmax(logits, dim=1)  
        _, predicted_classes = torch.max(probs, dim=1)  
        if self.remove_penmarks_only:
            # Class 4 is PenMarking, Class 7 is Background (from GrandQC paper)
            predictions = torch.where((predicted_classes == 4) | (predicted_classes == 7), 0, 1)
        else:
            # Classes > 1 typically represent artifacts or background.
            # Assuming GrandQC artifact model maps normal tissue to 1.
            predictions = torch.where(predicted_classes > 1, 0, 1)
        predictions = predictions.to(torch.uint8)

        return predictions


class GrandQCSegmenter(SegmentationModel):
    
    def __init__(self, **build_kwargs):
        """
        GrandQCSegmenter initialization.
        """
        super().__init__(**build_kwargs)

    def _build(self):
        """
        Load the GrandQC tissue detection segmentation model.
        Credit: https://www.nature.com/articles/s41467-024-54769-y
        """
        import segmentation_models_pytorch as smp

        model_ckpt_name = 'Tissue_Detection_MPP10.pth'
        encoder_name = 'timm-efficientnet-b0'
        encoder_weights = 'imagenet'
        weights_path = get_weights_path('seg', 'grandqc') 

        # Verify that user-provided weights_path is valid
        if weights_path and not os.path.isfile(weights_path):
            raise FileNotFoundError(
                f"Expected checkpoint at '{weights_path}', but the file was not found."
            )

        # Verify checkpoint path
        if not weights_path:
            if not SegmentationModel._has_internet:
                raise FileNotFoundError(
                    f"Internet connection not available and checkpoint not found locally at '{weights_path}'.\n\n"
                    f"To proceed, please manually download {model_ckpt_name} from:\n"
                    f"https://huggingface.co/MahmoodLab/hest-tissue-seg/\n"
                    f"and place it at:\ncheckpoints.json" # refers to the local_ckpts.json in this directory
                )

            from huggingface_hub import snapshot_download
            checkpoint_dir = snapshot_download(
                repo_id="MahmoodLab/hest-tissue-seg",
                repo_type='model',
                local_dir=get_dir(),
                cache_dir=get_dir(),
                allow_patterns=[model_ckpt_name],
            )
            weights_path = os.path.join(checkpoint_dir, model_ckpt_name)

        # Initialize model
        model = smp.UnetPlusPlus(
            encoder_name=encoder_name,
            encoder_weights=encoder_weights,
            classes=2,
            activation=None,
        )

        # Load checkpoint
        state_dict = torch.load(weights_path, map_location='cpu', weights_only=True)
        model.load_state_dict(state_dict)

        # Model config
        input_size = 512
        precision = torch.float32
        target_mag = 10 # GrandQC tissue detection is typically for 10x or 1x (MPP1)

        # Evaluation transforms
        eval_transforms = transforms.Compose([
            JpegCompressionTransform(quality=80),
            transforms.ToTensor(),
            transforms.Normalize(mean=[0.485, 0.456, 0.406],
                                 std=[0.229, 0.224, 0.225])
        ])

        return model, eval_transforms, input_size, precision, target_mag

    def forward(self, image: torch.Tensor) -> torch.Tensor:
        """
        Custom forward pass.
        """
        logits = self.model.predict(image)
        probs = torch.softmax(logits, dim=1)  
        max_probs, predicted_classes = torch.max(probs, dim=1)  
        # In GrandQC, class 0 is tissue, class 1 is background.
        # We want to keep class 0 (tissue) if its probability is above threshold.
        # So, if max_probs >= threshold AND predicted_class == 0.
        # (1 - predicted_classes) converts 0 to 1 and 1 to 0. So, we want predictions == 1.
        predictions = (max_probs >= self.confidence_thresh) * (1 - predicted_classes)
        predictions = predictions.to(torch.uint8)
 
        return predictions


def segmentation_model_factory(
    model_name: str, 
    confidence_thresh: float = 0.5, 
    freeze: bool = True,
    **build_kwargs,
) -> SegmentationModel:
    """
    Factory function to build a segmentation model by name.
    """

    if "device" in build_kwargs:
        import warnings
        warnings.warn(
            "Passing `device` to `segmentation_model_factory` is deprecated as of version 0.1.0 "
            "Please pass `device` when segmenting the tissue, e.g., `slide.segment_tissue(..., device='cuda:0')`.",
            DeprecationWarning,
            stacklevel=2
        )

    if model_name == 'hest':
        return HESTSegmenter(freeze=freeze, confidence_thresh=confidence_thresh, **build_kwargs)
    elif model_name == 'grandqc':
        return GrandQCSegmenter(freeze=freeze, confidence_thresh=confidence_thresh, **build_kwargs)
    elif model_name == 'grandqc_artifact':
        return GrandQCArtifactSegmenter(freeze=freeze, **build_kwargs)
    else:
        raise ValueError(f"Model type {model_name} not supported")
```

```json
# sauron/feature_extraction/models/segmentation/checkpoints.json
# New file, adapted from trident/segmentation_models/local_ckpts.json

{
    "hest": "",
    "grandqc": "",
    "grandqc_artifact": ""
}
```

```python
# sauron/feature_extraction/models/slide_encoders/__init__.py
# New file, adapted from trident/slide_encoder_models/__init__.py

from sauron.feature_extraction.models.slide_encoders.factory import (
    encoder_factory,
    MeanSlideEncoder,
    ABMILSlideEncoder,
    PRISMSlideEncoder,
    CHIEFSlideEncoder,
    GigaPathSlideEncoder,
    TitanSlideEncoder,
    ThreadsSlideEncoder,
    MadeleineSlideEncoder,
)

__all__ = [
    "encoder_factory",
    "TitanSlideEncoder",
    "ThreadsSlideEncoder",
    "MadeleineSlideEncoder",
    "MeanSlideEncoder",
    "ABMILSlideEncoder",
    "PRISMSlideEncoder",
    "CHIEFSlideEncoder",
    "GigaPathSlideEncoder",
]

```

```python
# sauron/feature_extraction/models/slide_encoders/factory.py
# New file, adapted from trident/slide_encoder_models/load.py

import sys
import os
import torch
import traceback
from abc import abstractmethod
from einops import rearrange
from typing import Optional, Tuple, Dict, Any

from sauron.feature_extraction.utils.io import get_weights_path, has_internet_connection

"""
This file contains an assortment of pretrained slide encoders, all loadable via the encoder_factory() function.
"""

def encoder_factory(model_name: str, pretrained: bool = True, freeze: bool = True, **kwargs) -> torch.nn.Module:
        """
        Build a slide encoder model.

        Args:
            model_name (str): Name of the model to build.
            pretrained (bool): Whether to load pretrained weights.
            freeze (bool): Whether to freeze the weights of the model.
            **kwargs: Additional arguments to pass to the model constructor.

        Returns:
            torch.nn.Module: The slide encoder model.
        """

        if model_name.startswith('mean-'):
            enc = MeanSlideEncoder
            return enc(model_name = model_name)
        elif 'threads' in model_name:
            enc = ThreadsSlideEncoder
        elif 'titan' in model_name:
            enc = TitanSlideEncoder
        elif 'prism' in model_name:
            enc = PRISMSlideEncoder
        elif 'chief' in model_name:
            enc = CHIEFSlideEncoder
        elif 'gigapath' in model_name:
            enc = GigaPathSlideEncoder
        elif 'madeleine' in model_name:
            enc = MadeleineSlideEncoder
        elif 'abmil' in model_name: # This is a generic ABMIL, not a specific pretrained one.
            enc = ABMILSlideEncoder
        else:
            raise ValueError(f"Model type {model_name} not supported")
        
        return enc(pretrained=pretrained, freeze=freeze, **kwargs)


# Map from slide encoder to required patch encoder
# Used in Processor.py to load the correct patch encoder for a given slide encoder
slide_to_patch_encoder_name = {
    'threads': 'conch_v15',
    'titan': 'conch_v15',
    'tcga': 'conch_v15', # Not explicit in trident models, but often used for TCGA data
    'prism': 'virchow',
    'chief': 'ctranspath',
    'gigapath': 'gigapath',
    'madeleine': 'conch_v1',
    # Mean-pooling models infer their patch encoder from their name.
    # The 'mean-' prefix will be stripped to get the patch encoder name.
}



class BaseSlideEncoder(torch.nn.Module):
    
    _has_internet = has_internet_connection()

    def __init__(self, freeze: bool = True, **build_kwargs: dict) -> None:
        """
        Parent class for all pretrained slide encoders.
        """
        super().__init__()
        self.enc_name = None
        self.model, self.precision, self.embedding_dim = self._build(**build_kwargs)

        # Set all parameters to be non-trainable
        if freeze and self.model is not None:
            for param in self.model.parameters():
                param.requires_grad = False
            self.model.eval()
        
    def _get_weights_path(self):
        """
        If self.weights_path is provided (via build_kwargs), use it. 
        If not provided, check the model registry. 
            If path in model registry is empty, auto-download from huggingface
            else, use the path from the registry.
        """
        weights_path = build_kwargs.get('weights_path', None)
        if weights_path:
            self.ensure_valid_weights_path(weights_path)
            return weights_path
        else:
            weights_path = get_weights_path('slide', self.enc_name)
            self.ensure_valid_weights_path(weights_path)
            return weights_path

    def ensure_valid_weights_path(self, weights_path):
        if weights_path and not os.path.isfile(weights_path) and not os.path.isdir(weights_path): # CHIEF uses a directory
            raise FileNotFoundError(f"Expected checkpoint/directory at '{weights_path}', but it was not found.")
    
    def ensure_has_internet(self, enc_name):
        if not BaseSlideEncoder._has_internet:
            raise FileNotFoundError(
                f"Internet connection does seem not available. Auto checkpoint download is disabled."
                f"To proceed, please manually download: {enc_name},\n"
                f"and place it in the model registry in:\n`sauron/feature_extraction/models/slide_encoders/checkpoints.json`"
            )

    def forward(self, batch: Dict[str, Any], device: str) -> torch.Tensor:
        """
        Can be overwritten if model requires special forward pass.
        `batch` expected to be a dictionary containing 'features', 'coords', 'attributes'.
        """
        z = self.model(batch)
        return z
        
    @abstractmethod
    def _build(self, **build_kwargs) -> Tuple[torch.nn.Module, torch.dtype, int]:
        """
        Initialization method, must be defined in child class.
        Returns: model, precision, embedding_dim
        """
        pass


class CustomSlideEncoder(BaseSlideEncoder):
    def __init__(
        self, 
        enc_name: str, 
        model: torch.nn.Module, 
        precision: torch.dtype = torch.float32, 
        embedding_dim: Optional[int] = None,
        **build_kwargs # Capture other kwargs but they won't be used by _build
    ):
        """
        CustomSlideEncoder initialization.

        This class is used when the model and precision are pre-instantiated externally 
        and should be injected directly into the encoder wrapper.

        Args:
            enc_name (str): 
                A unique name or identifier for the encoder.
            model (torch.nn.Module): 
                A PyTorch model instance to use for slide-level inference.
            precision (torch.dtype, optional): 
                The precision to use for inference (e.g., torch.float32, torch.float16).
            embedding_dim (int, optional): 
                The output embedding dimension. If not provided, will attempt to use 
                `model.embedding_dim` if it exists.
        """
        # Call BaseSlideEncoder with freeze=False because freezing is handled externally for custom models
        super().__init__(freeze=False, **build_kwargs) 
        self.enc_name = enc_name
        self.model = model
        self.precision = precision
        self.embedding_dim = embedding_dim or getattr(model, 'embedding_dim', None)
        if self.embedding_dim is None:
            raise ValueError("For CustomSlideEncoder, embedding_dim must be provided or inferable from model.embedding_dim.")

    def _build(self, **build_kwargs):
        # For CustomSlideEncoder, model, precision, embedding_dim are passed directly to __init__
        # and stored. _build just returns these.
        return self.model, self.precision, self.embedding_dim


class ABMILSlideEncoder(BaseSlideEncoder):

    def __init__(self, **build_kwargs):
        """
        ABMIL initialization.
        """
        super().__init__(**build_kwargs)
    
    def _build(
        self,
        input_feature_dim: int,
        n_heads: int = 8,
        head_dim: int = 256,
        dropout: float = 0.25,
        gated: bool = True,
        pretrained: bool = False # This model is not pretrained from a public checkpoint
    ) -> Tuple[torch.nn.ModuleDict, torch.dtype, int]:
        
        from sauron.feature_extraction.models.slide_encoders.zoo.reusable_blocks.ABMIL import ABMIL
        import torch.nn as nn

        self.enc_name = 'abmil'
        
        assert pretrained is False, "ABMILSlideEncoder has no corresponding pretrained models. Please load with pretrained=False."
                                
        pre_attention_layers = nn.Sequential(
            nn.Linear(input_feature_dim, input_feature_dim),
            nn.GELU(),
            nn.Dropout(0.1)
        )
        
        image_pooler = ABMIL(
            n_heads=n_heads,
            feature_dim=input_feature_dim,
            head_dim=head_dim,
            dropout=dropout,
            n_branches=1,
            gated=gated
        )
        
        post_attention_layers = nn.Sequential(
            nn.Linear(input_feature_dim, input_feature_dim),
            nn.GELU(),
            nn.Dropout(0.1)
        )
        
        model = nn.ModuleDict({
            'pre_attention_layers': pre_attention_layers,
            'image_pooler': image_pooler,
            'post_attention_layers': post_attention_layers
        })
        
        precision = torch.float32
        embedding_dim = input_feature_dim
        return model, precision, embedding_dim

    def forward(self, batch: Dict[str, Any], device: str, return_raw_attention=False) -> torch.Tensor:
        image_features = self.model['pre_attention_layers'](batch['features'].to(device))
        image_features, attn = self.model['image_pooler'](image_features) # Features shape: (b n_branches f), where n_branches = 1. Branching is not used in this implementation.
        image_features = rearrange(image_features, 'b 1 f -> b f')
        image_features = self.model['post_attention_layers'](image_features)# Attention scores shape: (b r h n), where h is number of attention heads 
        if return_raw_attention:
            return image_features, attn
        return image_features


class PRISMSlideEncoder(BaseSlideEncoder):

    def __init__(self, **build_kwargs):
        """
        PRISM initialization.
        """
        super().__init__(**build_kwargs)
    
    def _build(self, pretrained=True):
        
        self.enc_name = 'prism'

        if sys.version_info < (3, 10):
            raise RuntimeError("PRISM requires Python 3.10 or above. Please update your Python interpreter.")

        try:
            import environs  # weird dependencies required by PRISM
            import sacremoses # type: ignore
            from transformers import AutoModel, AutoConfig # type: ignore
        except ImportError:
            traceback.print_exc()
            raise ImportError(
                "Please run `pip install environs==11.0.0 transformers==4.42.4 sacremoses==0.1.1` "
                "and ensure Python version is 3.10 or above."
            )

        if pretrained:
            self.ensure_has_internet(self.enc_name)
            model = AutoModel.from_pretrained('paige-ai/Prism', trust_remote_code=True)
        else:
            model = AutoModel.from_config(AutoConfig.from_pretrained('paige-ai/Prism'))
        
        # Remove the text decoder as it's not needed for slide encoding
        model.text_decoder = None 
        
        precision = torch.float16
        embedding_dim = 1280
        return model, precision, embedding_dim
    
    def forward(self, batch: Dict[str, Any], device: str) -> torch.Tensor:
        # input should be of shape (batch_size, tile_seq_len, tile_embed_dim)
        x = batch['features'].to(device)
        # Assuming model.slide_representations takes a batch of features and returns a dict with 'image_embedding'
        z = self.model.slide_representations(x)
        z = z['image_embedding'] 
        return z
    

class CHIEFSlideEncoder(BaseSlideEncoder):

    def __init__(self, **build_kwargs):
        """
        CHIEF initialization.
        """
        super().__init__(**build_kwargs)

    def _build(self, pretrained=True):
        
        self.enc_name = 'chief'
        weights_dir = self._get_weights_path() # CHIEF uses a directory as its "weight path"

        # Ensure model can be built.
        try:
            sys.path.append(weights_dir)
            from models.CHIEF import CHIEF # type: ignore
        except ImportError:
            traceback.print_exc()
            raise ImportError(
                f"\nError: Unable to import the CHIEF repository from '{weights_dir}'.\n\n"
                "To resolve this issue:\n"
                "1. Ensure you have cloned the CHIEF repository to a convenient location:\n"
                "   `git clone https://github.com/hms-dbmi/CHIEF/`\n"
                "2. Set the path to CHIEF repo in `sauron/feature_extraction/models/slide_encoders/checkpoints.json`, e.g., `./CHIEF`.\n"
                "3. Verify that CHIEF dependencies are installed:\n"
                "   `pip install addict`\n\n"
            )

        # Ensure weights can be loaded.
        try:
            current_wd = os.getcwd()  # Get current working directory
            # CHIEF expects to be run from its own directory for loading weights
            os.chdir(weights_dir)  
            os.makedirs(os.path.join(weights_dir, "model_weight"), exist_ok=True)

            required_files = {
                "Text_emdding.pth": "https://drive.google.com/drive/folders/1uRv9A1HuTW5m_pJoyMzdN31bE1i-tDaV",
                "CHIEF_pretraining.pth": "https://drive.google.com/drive/folders/1uRv9A1HuTW5m_pJoyMzdN31bE1i-tDaV",
            }

            for file_name, download_link in required_files.items():
                file_path = os.path.join(weights_dir, "model_weight", file_name)
                if not os.path.exists(file_path):
                    # In a CI/CD environment or non-interactive, this means manual download/setup is required
                    raise FileNotFoundError(
                        f"\nError: Missing required file '{file_name}' for CHIEF.\n\n"
                        "To resolve this issue:\n"
                        f"1. Download the file from:\n   {download_link}\n"
                        f"2. Copy '{file_name}' to the following directory:\n   {file_path}\n\n"
                        "Ensure the file is correctly placed before retrying."
                    )
            
            # CHIEF model expects some configuration file in its directory, which is usually handled internally.
            # Assuming the cloned repo has necessary config files.
            print("All necessary files for CHIEF are present. CHIEF setup is complete!")

        except FileNotFoundError as e: # Catch FileNotFoundError specifically for missing files
            raise e
        except Exception as e:
            print("\nAn error occurred during CHIEF setup:")
            traceback.print_exc()
            raise e

        # Initialize CHIEF model
        model = CHIEF(size_arg="small", dropout=True, n_classes=2)

        # Load pretrained weights
        if pretrained:
            td = torch.load(os.path.join('model_weight', 'CHIEF_pretraining.pth'), map_location='cpu', weights_only=True)
            model.load_state_dict(td, strict=True)
            
        # Return to original working directory
        os.chdir(current_wd)
        
        precision = torch.float32
        embedding_dim = 768
        return model, precision, embedding_dim
    
    def forward(self, batch: Dict[str, Any], device: str) -> torch.Tensor:
        # CHIEF expects (N, D) where N is number of patches, D is feature dim
        # Input batch['features'] is (B, N, D). Squeeze batch dim.
        x = batch['features'].squeeze(0).to(device)
        # CHIEF forward pass expects an additional dummy `text_features` argument, usually a tensor of zeros or ones.
        # Or a batch_size list. Dummy for now for simplicity based on its usage.
        # From CHIEF code: self(x, text_features=torch.tensor([0])) 
        # text_features is often a dummy for this path as it's a multimodal model.
        z = self.model(x, text_features=torch.tensor([0], device=device)) # Pass dummy text_features
        z = z['WSI_feature']  # Shape (1,768) - Already squeezed batch dim from outside
        return z
    

class GigaPathSlideEncoder(BaseSlideEncoder):

    def __init__(self, **build_kwargs):
        """
        GigaPath initialization.
        """
        super().__init__(**build_kwargs)

    def _build(self, pretrained=True):

        self.enc_name = 'gigapath'

        try:
            from gigapath.slide_encoder import create_model # type: ignore
        except ImportError:
            traceback.print_exc()
            raise ImportError("Please install fairscale and gigapath using `pip install fairscale git+https://github.com/prov-gigapath/prov-gigapath.git`.")
        
        # Make sure flash_attn is correct version
        try:
            import flash_attn; assert flash_attn.__version__ == '2.5.8' # type: ignore
        except AssertionError:
            traceback.print_exc()
            raise ImportError("Please install flash_attn version 2.5.8 using `pip install flash_attn==2.5.8`.")
        except ImportError:
            traceback.print_exc()
            raise ImportError("flash_attn is not installed. Please install it using `pip install flash_attn` (ensure CUDA compatibility).")
        
        if pretrained:
            self.ensure_has_internet(self.enc_name)
            model = create_model("hf_hub:prov-gigapath/prov-gigapath", "gigapath_slide_enc12l768d", 1536, global_pool=True)
        else:
            # When pretrained=False, the first argument to create_model should be an empty string
            model = create_model("", "gigapath_slide_enc12l768d", 1536, global_pool=True)
        
        
        precision = torch.float16
        embedding_dim = 768
        return model, precision, embedding_dim

    def forward(self, batch: Dict[str, Any], device: str) -> torch.Tensor:
        # GigaPath requires tile_size to be set on the model
        if 'attributes' not in batch or 'patch_size_level0' not in batch['attributes']:
            raise ValueError("Batch must contain 'attributes' with 'patch_size_level0' for GigaPathSlideEncoder.")
        self.model.tile_size = batch['attributes']['patch_size_level0'] # Set dynamically
        
        # GigaPath forward: features (B, N, D), coords (B, N, 2)
        # Returns (features from different layers). Layer 11 is the last.
        z = self.model(batch['features'].to(device), batch['coords'].to(device), all_layer_embed=True)[11]
        return z


class MadeleineSlideEncoder(BaseSlideEncoder):

    def __init__(self, **build_kwargs):
        """
        Madeleine initialization.
        """
        super().__init__(**build_kwargs)

    def _build(self, pretrained=True):

        assert pretrained, "MadeleineSlideEncoder has no non-pretrained models. Please load with pretrained=True."

        self.enc_name = 'madeleine'
        weights_path = self._get_weights_path()
        embedding_dim = 512

        try:
            from madeleine.models.factory import create_model_from_pretrained # type: ignore
        except ImportError:
            traceback.print_exc()
            raise ImportError("Please install Madeleine using `pip install git+https://github.com/mahmoodlab/MADELEINE.git`")  
        
        if not weights_path:
            self.ensure_has_internet(self.enc_name)
            # Madeleine also uses hf_hub_download internally in create_model_from_pretrained if checkpoint_path is a hub ID.
            # Assuming create_model_from_pretrained can handle the hub path directly.
            model, precision = create_model_from_pretrained("hf_hub:MahmoodLab/madeleine")
        else:
            model, precision = create_model_from_pretrained(weights_path)


        return model, precision, embedding_dim
    
    def forward(self, batch: Dict[str, Any], device: str) -> torch.Tensor:
        # Madeleine expects patch features directly, not a batch dict
        # Features are (B, N, D). Squeeze batch dim.
        x_features = batch['features'].squeeze(0) 
        z = self.model.encode_he(x_features, device)
        return z


class ThreadsSlideEncoder(BaseSlideEncoder):

    def __init__(self, **build_kwargs):
        """
        Threads initialization.
        """
        super().__init__(**build_kwargs)

    def _build(self, pretrained=True):

        self.enc_name = 'threads'

        try:
            # Assuming threadsmodel might need to be installed or is available in the environment
            from threadsmodel.inference import create_model, create_model_from_pretrained # type: ignore
        except ImportError:
            traceback.print_exc()
            raise ImportError("Threads model is coming soon! Thanks for your patience. (Missing threadsmodel dependency)")
        
        # Threads model is coming soon, so no actual implementation here yet
        # Placeholder for future integration
        print("ThreadsSlideEncoder: Model definition is a placeholder (Coming Soon!)")
        model = torch.nn.Identity() # Dummy model
        precision = torch.float16 # Default precision
        embedding_dim = 768 # Expected embedding dimension based on paper/Trident

        return model, precision, embedding_dim

    def forward(self, batch: Dict[str, Any], device: str, return_raw_attention=False) -> torch.Tensor:
        # Placeholder forward for "Coming Soon" model
        print("ThreadsSlideEncoder: Forward pass is a placeholder.")
        # If it's a dummy Identity, it will just return the input features
        # For a slide encoder, it should output a single vector per slide.
        # This needs actual model implementation.
        # For now, return mean-pooled features as a dummy if it is an identity model
        return batch['features'].mean(dim=1).to(device)


class TitanSlideEncoder(BaseSlideEncoder):
    
    def __init__(self, **build_kwargs):
        """
        Titan initialization.
        """
        super().__init__(**build_kwargs)

    def _build(self, pretrained=True):
        self.enc_name = 'titan'
        assert pretrained, "TitanSlideEncoder has no non-pretrained models. Please load with pretrained=True."
        
        # Titan can be loaded from HuggingFace directly, no local path needed in Trident.
        # If it needed a local path, _get_weights_path() would have handled it.
        try:
            from transformers import AutoModel # type: ignore
            self.ensure_has_internet(self.enc_name)
            model = AutoModel.from_pretrained('MahmoodLab/TITAN', trust_remote_code=True)
        except ImportError:
            traceback.print_exc()
            raise ImportError("Please install transformers and ensure network access for TITAN model.")
        except Exception:
            traceback.print_exc()
            raise Exception("Failed to download TITAN model. Make sure you were granted access and correctly registered your token.")

        precision = torch.float16
        embedding_dim = 768
        return model, precision, embedding_dim

    def forward(self, batch: Dict[str, Any], device: str) -> torch.Tensor:
        # Titan expects patch features, coords, and patch_size_level0
        # `batch['features']` is (B, N, D), `batch['coords']` is (B, N, 2)
        # `batch['attributes']['patch_size_level0']` is scalar (int)
        
        if 'attributes' not in batch or 'patch_size_level0' not in batch['attributes']:
            raise ValueError("Batch must contain 'attributes' with 'patch_size_level0' for TitanSlideEncoder.")

        z = self.model.encode_slide_from_patch_features(
            batch['features'].to(device), 
            batch['coords'].to(device), 
            batch['attributes']['patch_size_level0']
        )        
        return z


class MeanSlideEncoder(BaseSlideEncoder):

    def __init__(self, **build_kwargs):
        """
        Mean pooling initialization.
        """
        super().__init__(**build_kwargs)

    def _build(self, model_name = 'mean-default'):
        self.enc_name = model_name
        
        # Determine embedding dimension based on the assumed patch encoder name
        # The 'mean-' prefix will be stripped to get the patch encoder name.
        patch_encoder_name = model_name.replace('mean-', '')

        # These dimensions are based on trident/patch_encoder_models/load.py
        if patch_encoder_name == 'conch_v1':
            embedding_dim = 512
        elif patch_encoder_name == 'conch_v15':
            embedding_dim = 768
        elif patch_encoder_name == 'uni_v1':
            embedding_dim = 1024
        elif patch_encoder_name == 'uni_v2':
            embedding_dim = 1536
        elif patch_encoder_name == 'ctranspath':
            embedding_dim = 768
        elif patch_encoder_name == 'phikon':
            embedding_dim = 768
        elif patch_encoder_name == 'resnet50':
            embedding_dim = 1024
        elif patch_encoder_name == 'gigapath':
            embedding_dim = 1536
        elif patch_encoder_name == 'virchow':
            embedding_dim = 2560
        elif patch_encoder_name == 'virchow2':
            embedding_dim = 2560
        elif patch_encoder_name == 'hoptimus0' or patch_encoder_name == 'hoptimus1':
            embedding_dim = 1536
        elif patch_encoder_name == 'phikon_v2':
            embedding_dim = 1024
        elif patch_encoder_name == 'musk':
            embedding_dim = 1024
        elif patch_encoder_name == 'hibou_l':
            embedding_dim = 1024
        elif 'kaiko' in patch_encoder_name: # Handle all Kaiko variants
            if 'vits8' in patch_encoder_name or 'vits16' in patch_encoder_name:
                embedding_dim = 384
            elif 'vitb8' in patch_encoder_name or 'vitb16' in patch_encoder_name:
                embedding_dim = 768
            elif 'vitl14' in patch_encoder_name:
                embedding_dim = 1024
            else:
                embedding_dim = None # Unknown Kaiko variant
        elif patch_encoder_name == 'lunit-vits8':
            embedding_dim = 384
        elif patch_encoder_name == 'midnight12k':
            embedding_dim = 3072 # Midnight's default cls+mean output
        else:
            print(f"\033[93mWARNING: Could not automatically infer embedding_dim for mean encoder {self.enc_name} from patch encoder {patch_encoder_name}. Setting to None.\033[0m")
            embedding_dim = None
            
        return None, torch.float32, embedding_dim # No actual model, precision can be float32, embedding_dim inferred.

    def forward(self, batch: Dict[str, Any], device: str) -> torch.Tensor:
        # The mean encoder simply takes the mean of the patch features
        z = batch['features'].to(device).mean(dim=1) # (B, N, D) -> (B, D)
        return z

```

```json
# sauron/feature_extraction/models/slide_encoders/checkpoints.json
# New file, adapted from trident/slide_encoder_models/local_ckpts.json

{
    "chief": "",
    "madeleine": ""
}
```

```python
# sauron/feature_extraction/models/slide_encoders/zoo/__init__.py
# New empty file, to make it a package
```

```python
# sauron/feature_extraction/models/slide_encoders/zoo/reusable_blocks/__init__.py
# New empty file, to make it a package
```

```python
# sauron/feature_extraction/models/slide_encoders/zoo/reusable_blocks/ABMIL.py
# New file, copied from trident/slide_encoder_models/model_zoo/reusable_blocks/ABMIL.py
# No changes here.

import torch
import torch.nn as nn
import torch.nn.functional as F

from einops import rearrange


class ABMIL(nn.Module):
    """
    Multi-headed attention network with optional gating. Uses tanh-attention and sigmoid-gating as in ABMIL (https://arxiv.org/abs/1802.04712).
    Note that this is different from canonical attention in that the attention scores are computed directly by a linear layer rather than by a dot product between queries and keys.

    Args:
        feature_dim (int): Input feature dimension
        head_dim (int): Hidden layer dimension for each attention head. Defaults to 256.
        n_heads (int): Number of attention heads. Defaults to 8.
        dropout (float): Dropout probability. Defaults to 0.
        n_branches (int): Number of attention branches. Defaults to 1, but can be set to n_classes to generate one set of attention scores for each class.
        gated (bool): If True, sigmoid gating is applied. Otherwise, the simple attention mechanism is used.
    """

    def __init__(self, feature_dim = 1024, head_dim = 256, n_heads = 8, dropout = 0., n_branches = 1, gated = False):
        super().__init__()
        self.gated = gated
        self.n_heads = n_heads

        # Initialize attention head(s)
        self.attention_heads = nn.ModuleList([nn.Sequential(nn.Linear(feature_dim, head_dim),
                                                               nn.Tanh(),
                                                               nn.Dropout(dropout)) for _ in range(n_heads)])
        
        # Initialize gating layers if gating is used
        if self.gated:
            self.gating_layers = nn.ModuleList([nn.Sequential(nn.Linear(feature_dim, head_dim),
                                                                   nn.Sigmoid(),
                                                                   nn.Dropout(dropout)) for _ in range(n_heads)])
        
        # Initialize branching layers
        self.branching_layers = nn.ModuleList([nn.Linear(head_dim, n_branches) for _ in range(n_heads)])

        # Initialize condensing layer if multiple heads are used
        if n_heads > 1:
            self.condensing_layer = nn.Linear(n_heads * feature_dim, feature_dim)
        
    def forward(self, features, attn_mask = None):
        """
        Forward pass

        Args:
            features (torch.Tensor): Input features, acting as queries and values. Shape: batch_size x num_images x feature_dim
            attn_mask (torch.Tensor): Attention mask to enforce zero attention on empty images. Defaults to None. Shape: batch_size x num_images

        Returns:
            aggregated_features (torch.Tensor): Attention-weighted features aggregated across heads. Shape: batch_size x n_branches x feature_dim
        """

        assert features.dim() == 3, f'Input features must be 3-dimensional (batch_size x num_images x feature_dim). Got {features.shape} instead.'
        if attn_mask is not None:
            assert attn_mask.dim() == 2, f'Attention mask must be 2-dimensional (batch_size x num_images). Got {attn_mask.shape} instead.'
            assert features.shape[:2] == attn_mask.shape, f'Batch size and number of images must match between features and mask. Got {features.shape[:2]} and {attn_mask.shape} instead.'

        # Get attention scores for each head
        head_attentions = []
        head_features = []
        for i in range(len(self.attention_heads)):
            attention_vectors = self.attention_heads[i](features)        # Main attention vectors (shape: batch_size x num_images x head_dim)
            
            if self.gated:
                gating_vectors = self.gating_layers[i](features)                # Gating vectors (shape: batch_size x num_images x head_dim)
                attention_vectors = attention_vectors.mul(gating_vectors)       # Element-wise multiplication to apply gating vectors
                
            attention_scores = self.branching_layers[i](attention_vectors)       # Attention scores for each branch (shape: batch_size x num_images x n_branches)

            # Set attention scores for empty images to -inf
            if attn_mask is not None:
                attention_scores = attention_scores.masked_fill(~attn_mask.unsqueeze(-1), -1e9) # Mask is automatically broadcasted to shape: batch_size x num_images x n_branches

            # Softmax attention scores over num_images
            attention_scores_softmax = F.softmax(attention_scores, dim=1) # Shape: batch_size x num_images x n_branches

            # Multiply features by attention scores
            weighted_features = torch.einsum('bnr,bnf->brf', attention_scores_softmax, features) # Shape: batch_size x n_branches x feature_dim

            head_attentions.append(attention_scores)
            head_features.append(weighted_features)

        # Concatenate multi-head outputs and condense
        aggregated_features = torch.cat(head_features, dim=-1) # Shape: batch_size x n_branches x (n_heads * feature_dim)
        if self.n_heads > 1:
            aggregated_features = self.condensing_layer(aggregated_features) # Shape: batch_size x n_branches x feature_dim
        
        # Stack attention scores
        head_attentions = torch.stack(head_attentions, dim=-1) # Shape: batch_size x num_images x n_branches x n_heads
        head_attentions = rearrange(head_attentions, 'b n r h -> b r h n') # Shape: batch_size x n_branches x n_heads x num_images

        return aggregated_features, head_attentions

```

```python
# sauron/feature_extraction/processor.py
# Updated file, integrating logic from trident/Processor.py

from __future__ import annotations
import os
import sys
import shutil
from tqdm import tqdm
from typing import Optional, List, Dict, Any, TypeAlias
from inspect import signature
import geopandas as gpd
import pandas as pd 
import logging

import torch 

from sauron.feature_extraction.wsi.factory import load_wsi, WSIReaderType, OPENSLIDE_EXTENSIONS, PIL_EXTENSIONS
from sauron.feature_extraction.wsi.base import WSI # For type hinting
from sauron.feature_extraction.models.segmentation.factory import segmentation_model_factory, SegmentationModel # For type hinting
from sauron.feature_extraction.models.patch_encoders.factory import encoder_factory as patch_encoder_factory, BasePatchEncoder # For type hinting
from sauron.feature_extraction.models.slide_encoders.factory import encoder_factory as slide_encoder_factory, slide_to_patch_encoder_name, BaseSlideEncoder # For type hinting
from sauron.feature_extraction.utils.io import create_lock, remove_lock, is_locked, update_log, collect_valid_slides, JSONsaver #, get_dir, has_internet_connection


# --- Setup Basic Logging ---
logger = logging.getLogger(__name__)

# --- Type Aliases for Clarity ---
PathLike: TypeAlias = str | os.PathLike


class Processor:

    def __init__(
        self,
        job_dir: PathLike,
        wsi_source: PathLike,
        wsi_ext: Optional[List[str]] = None,
        wsi_cache: Optional[PathLike] = None,
        clear_cache: bool = False,
        skip_errors: bool = False,
        custom_mpp_keys: Optional[List[str]] = None,
        custom_list_of_wsis: Optional[PathLike] = None,
        max_workers: Optional[int] = None, # Used by dataloaders for num_workers
        reader_type: Optional[WSIReaderType] = None,
        search_nested: bool = False, 
    ) -> None:
        """
        The `Processor` class handles all preprocessing steps starting from whole-slide images (WSIs). 
    
        Available methods:
            - `run_segmentation_job`: Performs tissue segmentation on all slides managed by the processor.
            - `run_patching_job`: Extracts patch coordinates from the segmented tissue regions of slides.
            - `run_patch_feature_extraction_job`: Extracts patch-level features using a specified patch encoder.
            - `run_slide_feature_extraction_job`: Extracts slide-level features using a specified slide encoder.
            
        Parameters:
            job_dir (PathLike): 
                The directory where the results of processing, including segmentations, patches, and extracted features, 
                will be saved. This should be an existing directory with sufficient storage.
            wsi_source (PathLike): 
                The directory containing the WSIs to be processed. This can either be a local directory 
                or a network-mounted drive. All slides in this directory matching the specified file 
                extensions will be considered for processing.
            wsi_ext (List[str]): 
                A list of accepted WSI file extensions, such as ['.ndpi', '.svs']. This allows for 
                filtering slides based on their format. If set to None, a default list of common extensions 
                will be used. Defaults to None.
            wsi_cache (PathLike, optional): 
                An optional directory for caching WSIs locally. If specified, slides will be copied 
                from the source directory to this local directory before processing, improving performance 
                when the source is a network drive. Defaults to None.
            clear_cache (bool, optional):
                A flag indicating whether slides in the cache should be deleted after processing. 
                This helps manage storage space. Defaults to False. 
            skip_errors (bool, optional): 
                A flag specifying whether to continue processing if an error occurs on a slide. 
                If set to False, the process will stop on the first error. Defaults to False.
            custom_mpp_keys (List[str], optional): 
                A list of custom keys in the slide metadata for retrieving the microns per pixel (MPP) value. 
                If not provided, standard keys will be used. Defaults to None.
            custom_list_of_wsis (PathLike, optional): 
                Path to a csv file with a custom list of WSIs to process in a field called 'wsi' (including extensions). If provided, only 
                these slides will be considered for processing. Defaults to None, which means all 
                slides matching the wsi_ext extensions will be processed.
                Note: If `custom_list_of_wsis` is provided, any names that do not match the available slides will be ignored, and a warning will be printed.
            max_workers (int, optional):
                Maximum number of workers for data loading. If None, the default behavior will be used.
                Defaults to None.
            reader_type (WSIReaderType, optional):
                Force the image reader engine to use. Options are are ["openslide", "image", "cucim"]. Defaults to None
                (auto-determine the right engine based on image extension).
            search_nested (bool, optional):  
                If True, the processor will recursively search for WSIs within all subdirectories of `wsi_source`.
                All matching files (based on `wsi_ext`) found at any depth within the directory  
                tree will be included. Each slide will be identified by its relative path to `wsi_source`, but only  
                the filename (excluding directory structure) will be used for downstream outputs (e.g., segmentation filenames).  
                If False, only files directly inside `wsi_source` will be considered.  
                Defaults to False.


        Returns:
            None: This method initializes the class instance and sets up the environment for processing.

        Example
        -------
        Initialize the `Processor` for a directory of WSIs:

        >>> processor = Processor(
        ...     job_dir="results/",
        ...     wsi_source="data/slides/",
        ...     wsi_ext=[".svs", ".ndpi"],
        ... )
        >>> print(f"Processor initialized for {len(processor.wsis)} slides.")

        Raises:
            AssertionError: If `wsi_ext` is not a list or if any extension does not start with a period.
        """
        
        if not (sys.version_info.major >= 3 and sys.version_info.minor >= 9):
            raise EnvironmentError("Sauron requires Python 3.9 or above. Python 3.10 is recommended.")

        self.job_dir = os.path.abspath(job_dir)
        self.wsi_source = os.path.abspath(wsi_source)
        self.wsi_ext = wsi_ext or (list(PIL_EXTENSIONS) + list(OPENSLIDE_EXTENSIONS))
        self.wsi_cache = os.path.abspath(wsi_cache) if wsi_cache else None
        self.clear_cache = clear_cache
        self.skip_errors = skip_errors
        self.custom_mpp_keys = custom_mpp_keys
        self.max_workers = max_workers
        self.reader_type = reader_type
        self.search_nested = search_nested

        # Validate extensions
        assert isinstance(self.wsi_ext, list), f'wsi_ext must be a list, got {type(self.wsi_ext)}'
        for ext in self.wsi_ext:
            assert ext.startswith('.'), f'Invalid extension: {ext} (must start with a period)'

        # === Collect slide paths and relative paths ===
        full_paths, rel_paths, mpp_values_from_csv = collect_valid_slides(
            wsi_dir=wsi_source,
            custom_list_path=custom_list_of_wsis,
            wsi_ext=self.wsi_ext,
            search_nested=search_nested,
            max_workers=max_workers,
            return_mpp_from_csv=True, # New return
            return_relative_paths=True
        )

        self.wsi_rel_paths = rel_paths if custom_list_of_wsis else None

        logger.info(f'[PROCESSOR] Found {len(full_paths)} valid slides in {wsi_source}.')

        # === Initialize WSIs ===
        self.wsis = []
        init_log_path = os.path.join(self.job_dir, "_processor_init_log.txt")
        for wsi_idx, abs_path in enumerate(full_paths):
            name = os.path.basename(abs_path) # Name for output files is just the filename, not full relative path
            original_full_path = full_paths[wsi_idx] # The true path of the WSI file on disk
            
            # Use original_full_path to determine if caching is needed
            load_path = os.path.join(self.wsi_cache, name) if self.wsi_cache else original_full_path

            tissue_seg_path = os.path.join(
                self.job_dir, 'segmentation_results', 'contours_geojson',
                f'{os.path.splitext(name)[0]}.geojson'
            )
            if not os.path.exists(tissue_seg_path):
                tissue_seg_path = None

            try:
                slide = load_wsi(
                    slide_path=load_path, # Path where WSI is expected to be *loaded from*
                    original_path=original_full_path, # Original source path for caching logic
                    name=name, # Base filename for output file naming
                    tissue_seg_path=tissue_seg_path,
                    custom_mpp_keys=self.custom_mpp_keys,
                    mpp=mpp_values_from_csv[wsi_idx] if mpp_values_from_csv is not None else None,
                    max_workers=self.max_workers,
                    reader_type=self.reader_type,
                    lazy_init=True,
                )
                self.wsis.append(slide)
                update_log(init_log_path, name, "INFO - WSI object created")

            except Exception as e:
                message = (
                    f"ERROR creating WSI object for {name} at {load_path} (original: {original_full_path}): {e}"
                )
                update_log(init_log_path, name, message)
                if self.skip_errors:
                    logger.error(message)
                else:
                    raise RuntimeError(message) from e

    def _get_job_paths(self, job_name: str, sub_dirs: List[str]) -> Dict[str, str]:
        """Helper to create and return paths for a specific processing job."""
        base_dir = os.path.join(self.job_dir, job_name)
        paths = {"base": base_dir}
        for sub in sub_dirs:
            path = os.path.join(base_dir, sub)
            os.makedirs(path, exist_ok=True)
            paths[sub] = path
        paths["config"] = os.path.join(base_dir, f"_config_{job_name}.json")
        paths["log"] = os.path.join(base_dir, f"_log_{job_name}.txt")
        return paths

    def populate_cache(self, start_idx: int = 0) -> None:
        """
        Copies WSI files from the source directory to the local cache directory.
        Only copies slides from `start_idx` onwards.
        """
        if not self.wsi_cache:
            logger.info("No cache directory specified. Skipping cache population.")
            return

        cache_log_path = os.path.join(self.wsi_cache, "_cache_log.txt")
        logger.info(f"Populating cache directory: {self.wsi_cache}")
        
        # Filter slides to process based on start_idx
        wsis_to_process = self.wsis[start_idx:]
        
        progress_bar = tqdm(
            wsis_to_process, desc="Populating cache", total=len(wsis_to_process), unit="slide"
        )

        for wsi in progress_bar:
            slide_fullname = wsi.name + wsi.ext
            cache_file_path = os.path.join(self.wsi_cache, slide_fullname)
            source_file_path = wsi.original_path # Use the true source path

            progress_bar.set_postfix_str(f"{slide_fullname}")

            if os.path.exists(cache_file_path) and not is_locked(cache_file_path):
                update_log(cache_log_path, slide_fullname, "INFO - Already in cache")
                continue

            if is_locked(cache_file_path):
                update_log(
                    cache_log_path, slide_fullname, "SKIP - Locked by another process"
                )
                continue

            try:
                create_lock(cache_file_path)
                update_log(cache_log_path, slide_fullname, "LOCK - Copying")
                shutil.copy2(source_file_path, cache_file_path)
                # Handle .mrxs subdirectories if they exist (trident's Concurrency.py)
                if source_file_path.lower().endswith('.mrxs'):
                    mrxs_dir = os.path.splitext(source_file_path)[0]
                    if os.path.exists(mrxs_dir) and os.path.isdir(mrxs_dir):
                        dest_mrxs_dir = os.path.join(self.wsi_cache, os.path.basename(mrxs_dir))
                        shutil.copytree(mrxs_dir, dest_mrxs_dir)
                update_log(cache_log_path, slide_fullname, "OK - Copied")
            except Exception as e:
                error_msg = f"ERROR copying: {e}"
                update_log(cache_log_path, slide_fullname, error_msg)
                logger.error(f"Failed to copy {source_file_path} to cache: {e}")
                # Attempt cleanup, ignore errors
                try:
                    if os.path.exists(cache_file_path + ".lock"):
                        remove_lock(cache_file_path)
                    if os.path.exists(cache_file_path):  # Remove partially copied file
                        os.remove(cache_file_path)
                except OSError:
                    pass
            finally:
                if os.path.exists(cache_file_path + ".lock"):
                    try:
                        remove_lock(cache_file_path)
                    except OSError as lock_err:
                        logger.warning(
                            f"Could not remove lock file for {cache_file_path} after operation: {lock_err}"
                        )
        logger.info("Cache population finished.")

    def run_segmentation_job(
        self,
        segmentation_model: SegmentationModel,
        seg_mag: int = 10,
        holes_are_tissue: bool = False,
        batch_size: int = 16,
        artifact_remover_model: Optional[SegmentationModel] = None,
        device: str = "cuda:0",
    ) -> str:
        """
        Performs tissue segmentation on the targeted WSIs.

        Uses the provided `segmentation_model` to identify tissue regions.
        Optionally uses an `artifact_remover_model` for refinement. Saves results
        (thumbnails, contours, GeoJSON) in subdirectories under `job_dir`.

        Args:
            segmentation_model: A pre-trained PyTorch model for tissue segmentation.
            seg_mag: Target magnification (e.g., 10 for 10x) for segmentation. Defaults to 10.
            holes_are_tissue: If True, holes within tissue contours are considered tissue.
                              If False, they are excluded. Defaults to False.
            batch_size: Batch size for model inference during segmentation. Defaults to 16.
            artifact_remover_model: Optional second model to refine segmentation, often
                                    used to remove artifacts like pen marks. Defaults to None.
            device: The device for PyTorch computations (e.g., 'cuda:0', 'cpu'). Defaults to 'cuda:0'.

        Returns:
            Absolute path to the directory where GeoJSON contour files are saved.

        Raises:
            RuntimeError: If an error occurs during segmentation and `skip_errors` is False.
        """
        segmentation_job_name = "segmentation_results"
        paths = self._get_job_paths(
            segmentation_job_name, ["contours_geojson", "contours", "thumbnails"]
        )
        geojson_dir = paths["contours_geojson"]
        log_fp = paths["log"]

        # --- Save Configuration ---
        sig = signature(self.run_segmentation_job)
        local_attrs = {k: v for k, v in locals().items() if k in sig.parameters}
        # Add model names if available
        if hasattr(segmentation_model, "model_name"):
            local_attrs["segmentation_model_name"] = segmentation_model.model_name
        if artifact_remover_model and hasattr(artifact_remover_model, "model_name"):
            local_attrs["artifact_remover_model_name"] = (
                artifact_remover_model.model_name
            )
        self.save_config(
            saveto=paths["config"],
            local_attrs=local_attrs,
            ignore=["self", "segmentation_model", "artifact_remover_model"],
        )
        logger.info(f"Starting segmentation job. Results will be in {paths['base']}")

        progress_bar = tqdm(
            self.wsis, desc="Segmenting tissue", total=len(self.wsis), unit="slide"
        )

        for wsi in progress_bar:
            slide_fullname = wsi.name + wsi.ext
            geojson_path = os.path.join(geojson_dir, f"{wsi.name}.geojson")
            progress_bar.set_postfix_str(f"{slide_fullname}")

            # --- Pre-computation Checks ---
            if os.path.exists(geojson_path) and not is_locked(geojson_path):
                update_log(log_fp, slide_fullname, "DONE - Already segmented")
                self.cleanup_wsi_cache(slide_fullname) # Clean up cache if done
                continue
            if is_locked(geojson_path):
                update_log(log_fp, slide_fullname, "SKIP - Locked")
                continue
            
            # Check if original WSI file exists at its load path (which could be cache)
            if not os.path.exists(wsi.slide_path): 
                update_log(
                    log_fp,
                    slide_fullname,
                    f"SKIP - WSI file not found at {wsi.slide_path}",
                )
                continue

            # --- Perform Segmentation ---
            try:
                create_lock(geojson_path)
                update_log(log_fp, slide_fullname, "LOCK - Segmenting")
                wsi._lazy_initialize()  # Ensure WSI is loaded

                generated_geojson_path = wsi.segment_tissue(
                    segmentation_model=segmentation_model,
                    target_mag=seg_mag,
                    holes_are_tissue=holes_are_tissue,
                    job_dir=paths["base"],  # Pass base segmentation dir
                    batch_size=batch_size,
                    device=device,
                    verbose=False,
                )

                if artifact_remover_model is not None:
                    logger.info(f"Applying artifact remover to {slide_fullname}")
                    generated_geojson_path = wsi.segment_tissue(
                        segmentation_model=artifact_remover_model,
                        target_mag=getattr(
                            artifact_remover_model, "target_mag", seg_mag
                        ),
                        holes_are_tissue=False, # Artifact remover usually removes holes (artifacts)
                        job_dir=paths["base"],
                        batch_size=batch_size,
                        device=device,
                        verbose=False,
                    )

                # Verify output
                if not os.path.exists(generated_geojson_path):
                    raise FileNotFoundError(
                        f"Segmentation output {generated_geojson_path} not created."
                    )
                try:
                    gdf = gpd.read_file(generated_geojson_path, rows=1)
                    status = "DONE - Segmented"
                    if gdf.empty:
                        status = "WARN - Empty GeoDataFrame"
                        logger.warning(
                            f"Empty segmentation result for {slide_fullname}"
                        )
                    update_log(log_fp, slide_fullname, status)
                except Exception as gdf_err:
                    update_log(
                        log_fp, slide_fullname, f"ERROR reading GeoJSON: {gdf_err}"
                    )
                    raise ValueError(
                        f"Could not read generated GeoJSON: {gdf_err}"
                    ) from gdf_err

            except Exception as e:
                error_msg = f"ERROR during segmentation: {e}"
                update_log(log_fp, slide_fullname, error_msg)
                logger.error(f"Error segmenting {slide_fullname}: {e}")
                if isinstance(e, KeyboardInterrupt):
                    print("Segmentation interrupted.")
                    raise e
                if not self.skip_errors:
                    raise RuntimeError(f"Error segmenting {slide_fullname}: {e}") from e
                # Continue loop if skipping errors
            finally:
                if os.path.exists(geojson_path + ".lock"):
                    try:
                        remove_lock(geojson_path)
                    except OSError as lock_err:
                        logger.warning(
                            f"Could not remove lock {geojson_path}.lock: {lock_err}"
                        )
                wsi.close()  # Close WSI handle to free resources
                self.cleanup_wsi_cache(slide_fullname)

        logger.info(f"Segmentation job finished. GeoJSONs in: {geojson_dir}")
        return geojson_dir

    def run_patching_job(
        self,
        target_magnification: int,
        patch_size: int,
        overlap: int = 0,
        patch_dir_name: Optional[str] = None,
        visualize: bool = True,
        min_tissue_proportion: float = 0.0,
    ) -> str:
        """Extracts patch coordinates from segmented tissue regions for each WSI."""
        if patch_dir_name is None:
            patch_dir_name = (
                f"patches_{target_magnification}x_{patch_size}px_{overlap}ovlp"
            )

        paths = self._get_job_paths(
            patch_dir_name, ["patches", "visualization"] if visualize else ["patches"]
        )
        coords_h5_dir = paths["patches"]  # HDF5 files go here
        viz_dir = paths.get("visualization")  # Will be None if visualize=False
        log_fp = paths["log"]

        # --- Save Configuration ---
        sig = signature(self.run_patching_job)
        local_attrs = {k: v for k, v in locals().items() if k in sig.parameters}
        self.save_config(
            saveto=paths["config"], local_attrs=local_attrs, ignore=["self"]
        )
        logger.info(f"Starting patching job. Results will be in {paths['base']}")

        progress_bar = tqdm(
            self.wsis,
            desc=f"Extracting patch coordinates ({patch_dir_name})",
            total=len(self.wsis),
            unit="slide",
        )

        for wsi in progress_bar:
            slide_fullname = wsi.name + wsi.ext
            coords_h5_path = os.path.join(coords_h5_dir, f"{wsi.name}_patches.h5")
            progress_bar.set_postfix_str(f"{slide_fullname}")

            # --- Pre-computation Checks ---
            if os.path.exists(coords_h5_path) and not is_locked(coords_h5_path):
                update_log(log_fp, slide_fullname, "DONE - Coords already generated")
                self.cleanup_wsi_cache(slide_fullname) # Clean up cache if done
                continue
            if is_locked(coords_h5_path):
                update_log(log_fp, slide_fullname, "SKIP - Locked")
                continue
            
            # Check if original WSI file exists at its load path (which could be cache)
            if not os.path.exists(wsi.slide_path): 
                update_log(
                    log_fp,
                    slide_fullname,
                    f"SKIP - WSI file not found at {wsi.slide_path}",
                )
                continue

            segmentation_path = wsi.tissue_seg_path  # Should be set if segmentation ran
            if segmentation_path is None or not os.path.exists(segmentation_path):
                update_log(
                    log_fp, slide_fullname, "SKIP - Segmentation GeoJSON not found"
                )
                continue
            try:  # Check if GeoJSON is empty
                gdf = gpd.read_file(segmentation_path, rows=1)
                if gdf.empty:
                    update_log(
                        log_fp,
                        slide_fullname,
                        "SKIP - Empty GeoDataFrame for segmentation",
                    )
                    continue
            except Exception as gdf_err:
                update_log(log_fp, slide_fullname, f"ERROR reading GeoJSON: {gdf_err}")
                if not self.skip_errors:
                    raise RuntimeError(
                        f"Error reading GeoJSON {segmentation_path}"
                    ) from gdf_err
                continue

            # --- Perform Patching ---
            try:
                create_lock(coords_h5_path)
                update_log(log_fp, slide_fullname, "LOCK - Generating coords")
                wsi._lazy_initialize()  # Ensure WSI loaded

                generated_coords_path = wsi.extract_tissue_coords(
                    target_mag=target_magnification,
                    patch_size=patch_size,
                    save_coords=paths["base"],  # Pass base dir for patching job
                    overlap=overlap,
                    min_tissue_proportion=min_tissue_proportion,
                )

                if not os.path.exists(generated_coords_path):
                    raise FileNotFoundError(
                        f"Coordinate file {generated_coords_path} not created."
                    )

                if viz_dir:
                    wsi.visualize_coords(
                        coords_path=generated_coords_path,
                        save_patch_viz=viz_dir,
                    )
                update_log(log_fp, slide_fullname, "DONE - Coords generated")

            except Exception as e:
                error_msg = f"ERROR during patching: {e}"
                update_log(log_fp, slide_fullname, error_msg)
                logger.error(f"Error patching {slide_fullname}: {e}")
                if isinstance(e, KeyboardInterrupt):
                    print("Patching interrupted.")
                    raise e
                if not self.skip_errors:
                    raise RuntimeError(f"Error patching {slide_fullname}: {e}") from e
            finally:
                if os.path.exists(coords_h5_path + ".lock"):
                    try:
                        remove_lock(coords_h5_path)
                    except OSError as lock_err:
                        logger.warning(
                            f"Could not remove lock {coords_h5_path}.lock: {lock_err}"
                        )
                wsi.close() # Close WSI handle to free resources
                self.cleanup_wsi_cache(slide_fullname)

        logger.info(f"Patching job finished. Coordinates in: {coords_h5_dir}")
        return coords_h5_dir  # Return path to HDF5 coordinate files

    # The deprecated decorator for run_feature_extraction_job is handled in trident's Processor,
    # and now can be added here (or just remove the alias). For simplicity, let's just make
    # `run_patch_feature_extraction_job` the primary.

    def run_patch_feature_extraction_job(
        self,
        coords_h5_dir: str,  # Dir containing HDF5 patch coord files
        patch_encoder: BasePatchEncoder, # Use new base class for type hinting
        device: str = "cuda:0",
        saveas: str = 'h5',
        batch_limit: int = 512,
        features_dir_name: Optional[str] = None,
    ) -> str:
        """Extracts patch-level features using a specified patch encoder model."""
        # --- Determine Paths ---
        if not os.path.isdir(coords_h5_dir):
            raise FileNotFoundError(f"Coordinates directory not found: {coords_h5_dir}")
        # Assume coords_h5_dir is like .../job_dir/patch_job_name/patches/
        patching_base_dir = os.path.dirname(coords_h5_dir)

        enc_name = getattr(patch_encoder, "enc_name", "custom_encoder")
        if features_dir_name is None:
            features_dir_name = f"features_{enc_name}"

        # Feature files will live alongside the 'patches' dir
        features_base_dir = os.path.join(patching_base_dir, features_dir_name)
        os.makedirs(features_base_dir, exist_ok=True)

        paths = {
            "base": features_base_dir,
            "config": os.path.join(features_base_dir, "_config_patch_features.json"),
            "log": os.path.join(features_base_dir, "_log_patch_features.txt"),
        }
        log_fp = paths["log"]

        # --- Save Configuration ---
        sig = signature(self.run_patch_feature_extraction_job)
        local_attrs = {k: v for k, v in locals().items() if k in sig.parameters}
        local_attrs["patch_encoder_name"] = enc_name
        local_attrs["patch_encoder_embedding_dim"] = getattr(patch_encoder, 'embedding_dim', 'unknown')

        self.save_config(
            saveto=paths["config"],
            local_attrs=local_attrs,
            ignore=["self", "patch_encoder"],
        )
        logger.info(
            f"Starting patch feature extraction ({features_dir_name}). Results in {paths['base']}"
        )

        progress_bar = tqdm(
            self.wsis,
            desc=f"Extracting patch features ({features_dir_name})",
            total=len(self.wsis),
            unit="slide",
        )

        for wsi in progress_bar:
            slide_fullname = wsi.name + wsi.ext
            coord_h5_path = os.path.join(coords_h5_dir, f"{wsi.name}_patches.h5")
            feature_file_path = os.path.join(features_base_dir, f"{wsi.name}.{saveas}")
            progress_bar.set_postfix_str(f"{slide_fullname}")

            # --- Pre-computation Checks ---
            if os.path.exists(feature_file_path) and not is_locked(feature_file_path):
                update_log(log_fp, slide_fullname, "DONE - Features already extracted")
                self.cleanup_wsi_cache(slide_fullname) # Clean up cache if done
                continue
            if is_locked(feature_file_path):
                update_log(log_fp, slide_fullname, "SKIP - Locked")
                continue
            
            # Check if original WSI file exists at its load path (which could be cache)
            if not os.path.exists(wsi.slide_path): 
                update_log(
                    log_fp,
                    slide_fullname,
                    f"SKIP - WSI file not found at {wsi.slide_path}",
                )
                continue

            if not os.path.exists(coord_h5_path):
                update_log(
                    log_fp,
                    slide_fullname,
                    f"SKIP - Coordinate file not found: {coord_h5_path}",
                )
                continue

            # --- Perform Feature Extraction ---
            try:
                create_lock(feature_file_path)
                update_log(log_fp, slide_fullname, "LOCK - Extracting patch features")
                wsi._lazy_initialize()  # Ensure WSI loaded

                generated_feature_path = wsi.extract_patch_features(
                    patch_encoder=patch_encoder,
                    coords_path=coord_h5_path,
                    save_features=features_base_dir,  # Pass the target directory
                    device=device,
                    saveas=saveas,
                    batch_limit=batch_limit,
                )

                if not os.path.exists(generated_feature_path):
                    raise FileNotFoundError(
                        f"Feature file {generated_feature_path} not created."
                    )
                update_log(log_fp, slide_fullname, "DONE - Features extracted")

            except Exception as e:
                error_msg = f"ERROR during patch feature extraction: {e}"
                update_log(log_fp, slide_fullname, error_msg)
                logger.error(
                    f"Error extracting patch features for {slide_fullname}: {e}"
                )
                if isinstance(e, KeyboardInterrupt):
                    print("Patch feature extraction interrupted.")
                    raise e
                if not self.skip_errors:
                    raise RuntimeError(
                        f"Error extracting patch features for {slide_fullname}: {e}"
                    ) from e
            finally:
                if os.path.exists(feature_file_path + ".lock"):
                    try:
                        remove_lock(feature_file_path)
                    except OSError as lock_err:
                        logger.warning(
                            f"Could not remove lock {feature_file_path}.lock: {lock_err}"
                        )
                wsi.close() # Close WSI handle to free resources
                self.cleanup_wsi_cache(slide_fullname)


        logger.info(
            f"Patch feature extraction finished. Features in: {features_base_dir}"
        )
        return features_base_dir

    def run_slide_feature_extraction_job(
        self,
        slide_encoder: BaseSlideEncoder, # Use new base class for type hinting
        coords_h5_dir: str, # Base directory for the patching job (contains 'patches' and 'features_...')
        device: str = "cuda:0",
        saveas: str = 'h5',
        batch_limit_for_patch_features: int = 512, # Used if auto-generating patch features
        slide_features_dir_name: Optional[str] = None,
    ) -> str:
        """Extracts slide-level features using a specified slide encoder model."""
        # --- Determine Paths and Required Patch Encoder ---
        if not os.path.isdir(coords_h5_dir):
            raise FileNotFoundError(
                f"Coordinates directory (e.g., .../job_dir/patch_job_name) not found: {coords_h5_dir}"
            )
        
        slide_enc_name = getattr(slide_encoder, "enc_name", "custom_slide_encoder")
        required_patch_enc_name = None
        # Infer required patch encoder name from slide encoder name
        if slide_enc_name.startswith("mean-"):
            # For mean-pooled encoders, the patch encoder is the suffix
            required_patch_enc_name = slide_enc_name.split("mean-", 1)[1]
        elif slide_enc_name in slide_to_patch_encoder_name:
            # For specific slide encoders, use the predefined mapping
            required_patch_enc_name = slide_to_patch_encoder_name[slide_enc_name]

        # Construct the expected path for patch features based on the inferred name
        expected_patch_features_dir = os.path.join(coords_h5_dir, f'features_{required_patch_enc_name}')

        # Determine output directory for slide features
        if slide_features_dir_name is None:
            slide_features_dir_name = f"slide_features_{slide_enc_name}"
        slide_features_base_dir = os.path.join(
            coords_h5_dir, slide_features_dir_name # Slide features are children of the same patching job directory
        )
        os.makedirs(slide_features_base_dir, exist_ok=True)

        paths = {
            "base": slide_features_base_dir,
            "config": os.path.join(
                slide_features_base_dir, "_config_slide_features.json"
            ),
            "log": os.path.join(slide_features_base_dir, "_log_slide_features.txt"),
        }
        log_fp = paths["log"]

        # --- Auto-generate Patch Features if Missing ---
        if required_patch_enc_name:
            # Check if all patch feature files for this encoder exist
            all_patch_features_exist = all(
                os.path.exists(os.path.join(expected_patch_features_dir, f"{wsi.name}.h5"))
                for wsi in self.wsis
            )
            
            if not all_patch_features_exist:
                logger.warning(
                    f"Required patch features ('{required_patch_enc_name}') missing in '{expected_patch_features_dir}'. Attempting generation."
                )
                try:
                    patch_encoder = patch_encoder_factory(required_patch_enc_name)
                    # The actual coordinate H5 files are in a 'patches' subfolder
                    patch_coords_h5_dir_for_generation = os.path.join(coords_h5_dir, 'patches')
                    
                    if not os.path.isdir(patch_coords_h5_dir_for_generation):
                        raise FileNotFoundError(
                            f"Coordinate directory '{patch_coords_h5_dir_for_generation}' needed for auto-generation not found."
                        )

                    # Run the patch feature job, ensuring it saves to the correct input dir for *this* job
                    self.run_patch_feature_extraction_job(
                        coords_h5_dir=patch_coords_h5_dir_for_generation, # This is the path to the 'patches' dir
                        patch_encoder=patch_encoder,
                        device=device,
                        saveas="h5",  # Must be h5 for slide feature extraction
                        batch_limit=batch_limit_for_patch_features,
                        features_dir_name=os.path.basename(expected_patch_features_dir), # Ensure it saves to the desired 'features_...' folder
                    )
                    logger.info(f"Auto-generation of patch features ({required_patch_enc_name}) complete.")
                except Exception as patch_gen_e:
                    raise RuntimeError(
                        f"Failed to auto-generate required patch features ('{required_patch_enc_name}'). "
                        f"Generate manually or ensure they exist in '{expected_patch_features_dir}'. Error: {patch_gen_e}"
                    ) from patch_gen_e
        elif not slide_enc_name.startswith("mean-"): # If it's not a mean-pooling model and no specific patch encoder is mapped
            logger.warning(
                f"Cannot auto-generate patch features for '{slide_enc_name}' (unknown requirement). Ensure they exist in '{patch_features_dir}'."
            )

        # --- Save Configuration ---
        sig = signature(self.run_slide_feature_extraction_job)
        local_attrs = {k: v for k, v in locals().items() if k in sig.parameters}
        local_attrs["slide_encoder_name"] = slide_enc_name
        local_attrs["slide_encoder_embedding_dim"] = getattr(slide_encoder, 'embedding_dim', 'unknown')
        if required_patch_enc_name:
            local_attrs["required_patch_encoder"] = required_patch_enc_name
            
        self.save_config(
            saveto=paths["config"],
            local_attrs=local_attrs,
            ignore=["self", "slide_encoder"],
        )
        logger.info(
            f"Starting slide feature extraction ({slide_features_dir_name}). Results in {paths['base']}"
        )

        progress_bar = tqdm(
            self.wsis,
            desc=f"Extracting slide features ({slide_features_dir_name})",
            total=len(self.wsis),
            unit="slide",
        )

        for wsi in progress_bar:
            slide_fullname = wsi.name + wsi.ext
            patch_feature_h5_path = os.path.join(
                expected_patch_features_dir, f"{wsi.name}.h5"
            )  # Assumes h5 input for features
            slide_feature_file_path = os.path.join(
                slide_features_base_dir, f"{wsi.name}.{saveas}"
            )
            progress_bar.set_postfix_str(f"{slide_fullname}")

            # --- Pre-computation Checks ---
            if os.path.exists(slide_feature_file_path) and not is_locked(
                slide_feature_file_path
            ):
                update_log(
                    log_fp, slide_fullname, "DONE - Slide features already extracted"
                )
                self.cleanup_wsi_cache(slide_fullname) # Clean up cache if done
                continue
            if is_locked(slide_feature_file_path):
                update_log(log_fp, slide_fullname, "SKIP - Locked")
                continue
            if not os.path.exists(patch_feature_h5_path):
                update_log(
                    log_fp,
                    slide_fullname,
                    f"SKIP - Required patch feature file not found: {patch_feature_h5_path}",
                )
                continue
            
            # Check if original WSI file exists at its load path (which could be cache)
            if not os.path.exists(wsi.slide_path): 
                logger.debug(
                    f"WSI file {wsi.slide_path} missing, but proceeding with feature file (if not strictly needed)."
                )
                # update_log(log_fp, slide_fullname, f"INFO - WSI file missing at {wsi.slide_path}") # Optional logging

            # --- Perform Slide Feature Extraction ---
            try:
                create_lock(slide_feature_file_path)
                update_log(log_fp, slide_fullname, "LOCK - Extracting slide features")
                # Initialize WSI mainly for metadata access if needed by slide encoder (e.g., GigaPath needs patch_size_level0 from coords.attrs)
                wsi._lazy_initialize() 

                generated_slide_feature_path = wsi.extract_slide_features(
                    patch_features_path=patch_feature_h5_path,
                    slide_encoder=slide_encoder,
                    save_features=slide_features_base_dir,  # Pass directory
                    device=device,
                    # saveas='h5' is implicit in wsi.extract_slide_features currently
                )

                if not os.path.exists(generated_slide_feature_path):
                    raise FileNotFoundError(
                        f"Slide feature file {generated_slide_feature_path} not created."
                    )
                update_log(log_fp, slide_fullname, "DONE - Slide features extracted")

            except Exception as e:
                error_msg = f"ERROR during slide feature extraction: {e}"
                update_log(log_fp, slide_fullname, error_msg)
                logger.error(
                    f"Error extracting slide features for {slide_fullname}: {e}"
                )
                if isinstance(e, KeyboardInterrupt):
                    print("Slide feature extraction interrupted.")
                    raise e
                if not self.skip_errors:
                    raise RuntimeError(
                        f"Error extracting slide features for {slide_fullname}: {e}"
                    ) from e
            finally:
                if os.path.exists(slide_feature_file_path + ".lock"):
                    try:
                        remove_lock(slide_feature_file_path)
                    except OSError as lock_err:
                        logger.warning(
                            f"Could not remove lock {slide_feature_file_path}.lock: {lock_err}"
                        )
                wsi.close()  # Close WSI handle to free resources
                self.cleanup_wsi_cache(slide_fullname)

        logger.info(
            f"Slide feature extraction finished. Features in: {slide_features_base_dir}"
        )
        return slide_features_base_dir

    def cleanup_wsi_cache(self, filename: str) -> None:
        """Removes the specified WSI file and its associated .mrxs directory from the cache if enabled."""
        if self.wsi_cache and self.clear_cache:
            cache_file_path = os.path.join(self.wsi_cache, filename)
            
            # Check for .mrxs subdirectory
            mrxs_dir_path = os.path.splitext(cache_file_path)[0]
            
            if os.path.exists(cache_file_path):
                if not is_locked(cache_file_path): # Only clean if not locked by another process
                    try:
                        os.remove(cache_file_path)
                        logger.debug(f"Cleaned {filename} from cache.")
                        # Check for and remove associated .mrxs directory
                        if os.path.isdir(mrxs_dir_path) and filename.lower().endswith('.mrxs'):
                            shutil.rmtree(mrxs_dir_path)
                            logger.debug(f"Cleaned associated .mrxs directory {os.path.basename(mrxs_dir_path)} from cache.")
                    except OSError as e:
                        logger.warning(f"Failed to remove {cache_file_path} from cache: {e}")
                # else: Logged by previous checks

    def save_config(
        self,
        saveto: PathLike,
        local_attrs: Optional[Dict[str, Any]] = None,
        ignore: Optional[List[str]] = None,
    ) -> None:
        """Saves the processor's configuration and job parameters to a JSON file."""
        if ignore is None:
            ignore = [
                "wsis",
                "loop", # tqdm loop object
                "wsi_source",
                "wsi_cache",
            ]  # Exclude sensitive/large/redundant

        config_to_save = {}

        # Add instance attributes (filter sensitive/large ones)
        for k, v in vars(self).items():
            if k not in ignore and not k.startswith("_"):  # Exclude private attrs
                # Special handling for PathLike objects or objects that might be Path objects
                if isinstance(v, (os.PathLike, Path)):
                    v = str(v)
                try:
                    # Attempt to dump to ensure serializability with custom JSONsaver
                    # This is just a test; the actual dump uses the same logic
                    JSONsaver().encode(v) 
                    config_to_save[k] = v
                except (TypeError, OverflowError):
                    config_to_save[k] = (
                        f"<Object type: {type(v).__name__}>"  # Represent non-serializable
                    )

        # Add/overwrite with local attributes from the specific job method
        if local_attrs:
            for k, v in local_attrs.items():
                if k not in ignore:
                    # Special handling for PathLike objects or objects that might be Path objects
                    if isinstance(v, (os.PathLike, Path)):
                        v = str(v)
                    try:
                        # Attempt to dump to ensure serializability with custom JSONsaver
                        JSONsaver().encode(v) 
                        config_to_save[k] = v
                    except (TypeError, OverflowError):
                        # Special handling for models - just save name if possible
                        if isinstance(v, torch.nn.Module) and hasattr(v, "enc_name"):
                            config_to_save[k] = (
                                f"<Model: {getattr(v, 'enc_name', type(v).__name__)}>"
                            )
                        elif isinstance(v, torch.nn.Module) and hasattr(
                            v, "model_name"
                        ):
                            config_to_save[k] = (
                                f"<Model: {getattr(v, 'model_name', type(v).__name__)}>"
                            )
                        else:
                            config_to_save[k] = f"<Object type: {type(v).__name__}>"

        # Ensure directory exists and save
        try:
            os.makedirs(os.path.dirname(saveto), exist_ok=True)
            with open(saveto, "w") as f:
                # Use JSONsaver for the actual dump
                json.dump(config_to_save, f, indent=4, cls=JSONsaver, ensure_ascii=False)
            logger.debug(f"Configuration saved successfully to {saveto}")
        except Exception as e:
            logger.error(f"Failed to save configuration to {saveto}: {e}")

    def release(self) -> None:
        """
        Release all resources tied to the WSIs held by this Processor instance.
        Frees memory, closes file handles, and clears GPU memory.
        Should be called after processing is complete to avoid memory leaks.
        """
        if hasattr(self, "wsis"):
            for wsi in self.wsis:
                try:
                    wsi.close() # Calls the backend-specific close method
                except Exception:
                    pass
            self.wsis.clear()

        # Also clear loop references (e.g., tqdm)
        if hasattr(self, "loop"):
            self.loop = None

        # Explicit garbage collection and CUDA cache release
        import gc
        import torch
        gc.collect()
        if torch.cuda.is_available():
            torch.cuda.empty_cache()

```

```python
# sauron/feature_extraction/utils/config.py
# Updated file, aligned with trident/utils/config.py

import json
import logging
import os
import warnings
from pathlib import Path
from typing import Any, Dict, List, Optional, TypeAlias

import numpy as np
import torch  # For handling torch types in JSONsaver

# Type Aliases
PathLike: TypeAlias = str | os.PathLike | Path

# Setup logger for this utility module
logger = logging.getLogger(__name__)


class JSONsaver(json.JSONEncoder):
    """
    Custom JSON Encoder to handle non-standard types like NumPy arrays,
    ranges, PyTorch dtypes, and callables for configuration saving.

    Converts unserializable types to strings or lists where appropriate.
    """

    def default(self, obj: Any) -> Any:
        if isinstance(obj, np.floating):
            return float(obj)
        elif isinstance(obj, np.integer):
            return int(obj)
        elif isinstance(obj, np.ndarray):
            # Limit array size representation for sanity, but allow smaller ones to be fully dumped
            if obj.size > 100 and obj.ndim > 0: # Avoid trying to dump huge arrays, but allow small ones
                return f"<NumPy Array shape={obj.shape} dtype={obj.dtype}>"
            return obj.tolist() # Convert smaller arrays to list
        elif isinstance(obj, np.bool_):
            return bool(obj)
        elif isinstance(obj, (range, Path)):  # Add Path handling
            return str(obj)
        elif obj in [
            torch.float16,
            torch.float32,
            torch.bfloat16,
            torch.int32,
            torch.int64,
        ]:  # Add more torch types
            return str(obj)
        elif callable(obj):
            try:
                # Prefer qualified name if possible
                name = getattr(obj, "__qualname__", getattr(obj, "__name__", None))
                if name:
                    module = getattr(obj, "__module__", "")
                    if module:
                        return f"<Callable: {module}.{name}>"
                    return f"<Callable: {name}>"
                # Fallback for objects without clear names (e.g., partials)
                return f"<Callable: {str(obj)}>"
            except Exception:
                return f"<Callable: {str(obj)}>"  # Safeguard
        # Let the base class default method raise the TypeError for other types
        try:
            return super().default(obj)
        except TypeError:
            return f"<Unserializable object type: {type(obj).__name__}>"


def save_json_config(
    config_path: PathLike,
    processor_instance: Optional[Any] = None,  # Can pass the Processor instance
    local_attrs: Optional[Dict[str, Any]] = None,
    ignore: Optional[List[str]] = None,
    custom_config: Optional[Dict[str, Any]] = None,  # Option to pass a pre-made dict
) -> None:
    """
    Saves configuration data to a JSON file.

    Combines attributes from a processor instance (if provided), local attributes,
    and/or a custom config dictionary. Handles non-serializable types gracefully using
    JSONsaver and filters out specified keys.

    Args:
        config_path: The full path (including filename) to save the JSON config.
        processor_instance: Optional instance (e.g., Processor) whose attributes
            should be saved.
        local_attrs: Optional dictionary of additional attributes (e.g., method parameters).
        ignore: A list of attribute names to exclude. Defaults to common Processor
                attributes like 'wsis', 'loop', 'logger', 'paths'.
        custom_config: Optional dictionary containing the configuration to save directly.
                       If provided, processor_instance and local_attrs might be ignored
                       or merged depending on implementation details (here, it merges).

    Raises:
        IOError: If the file cannot be written.
        TypeError: If JSON serialization fails unexpectedly (should be caught by JSONsaver).
    """
    config_path = Path(config_path)
    if ignore is None:
        # Default keys to ignore for the Processor class
        ignore = ["wsis", "loop", "logger", "paths"]

    config_to_save: Dict[str, Any] = {}

    # 1. Add processor instance attributes (if provided)
    if processor_instance:
        for k, v in vars(processor_instance).items():
            if k not in ignore:
                config_to_save[k] = v  # Add raw value, JSONsaver will handle type

    # 2. Add local attributes (potentially overwriting instance attributes)
    if local_attrs:
        for k, v in local_attrs.items():
            if k not in ignore:
                config_to_save[k] = v

    # 3. Add/overwrite with custom config dictionary (if provided)
    if custom_config:
        for k, v in custom_config.items():
            if k not in ignore:
                config_to_save[k] = v

    # Ensure the directory exists
    try:
        config_path.parent.mkdir(parents=True, exist_ok=True)
    except OSError as e:
        logger.error(f"Could not create directory for config file {config_path}: {e}")
        # Depending on severity, you might want to raise an error here
        # For now, we'll attempt to write anyway but log the error.
        # raise IOError(f"Could not create directory for config file {config_path}: {e}") from e

    # Save the combined configuration using JSONsaver
    try:
        with open(config_path, "w") as f:
            json.dump(config_to_save, f, indent=4, cls=JSONsaver, ensure_ascii=False)
        logger.debug(f"Configuration saved successfully to {config_path}")
    except TypeError as e:
        logger.error(
            f"JSON serialization failed for config {config_path}: {e}. Check JSONsaver handling."
        )
        # Re-raise as it indicates a fundamental issue with serialization logic
        raise
    except IOError as e:
        logger.error(f"Failed to write configuration file to {config_path}: {e}")
        # Re-raise as saving the config is often critical for reproducibility
        raise
    except Exception as e:
        logger.error(
            f"An unexpected error occurred while saving config to {config_path}: {e}"
        )
        # Optionally re-raise depending on desired robustness
        raise

```

```python
# sauron/feature_extraction/utils/io.py
# Updated file, integrating trident/IO.py and existing sauron utils/gpd_utils and utils/hdf5_utils.

from __future__ import annotations

import torch
import socket
import os
import json
from typing import List, Optional, Union, Tuple, Any
import h5py
import numpy as np
import cv2
import pandas as pd
from geopandas import gpd
from shapely import Polygon

# from sauron.feature_extraction.models.patch_encoders.utils.constants import IMAGENET_MEAN, IMAGENET_STD # Example, not directly used in this file.
# from sauron.feature_extraction.models.patch_encoders.utils.transform_utils import get_eval_transforms # Example
from pathlib import Path
from concurrent.futures import ThreadPoolExecutor

ENV_SAURON_HOME = "SAURON_HOME"
ENV_XDG_CACHE_HOME = "XDG_CACHE_HOME"
DEFAULT_CACHE_DIR = "~/.cache"
_cache_dir: Optional[str] = None


class CuImageWarning(UserWarning):
    """Warning for missing cucim dependency."""
    pass

def warn_cucim_missing():
    warnings.warn(
        "cuCIM is not installed. Falling back to OpenSlide/PIL. "
        "Install with `pip install cucim cupy-cuda12x` (adjust CUDA version)."
    )

try:
    import cucim
    _CUCIM_AVAILABLE = True
except ImportError:
    _CUCIM_AVAILABLE = False
    import warnings
    warnings.simplefilter('once', CuImageWarning) # Warn only once


# Use the WSI extensions defined in the factory
# These are imported from the factory, so that they are consistent.
# Placeholder imports if WSI objects are not yet defined:
# Assuming these are defined in sauron/feature_extraction/wsi/factory.py
from sauron.feature_extraction.wsi.factory import OPENSLIDE_EXTENSIONS, CUCIM_EXTENSIONS, PIL_EXTENSIONS

def collect_valid_slides(
    wsi_dir: str,
    custom_list_path: Optional[str] = None,
    wsi_ext: Optional[List[str]] = None,
    search_nested: bool = False,
    max_workers: int = 8,
    return_relative_paths: bool = False,
    return_mpp_from_csv: bool = False,
) -> Union[List[str], Tuple[List[str], List[str]], Tuple[List[str], List[str], Optional[List[float]]]]:
    """
    Retrieve all valid WSI file paths from a directory, optionally filtered by a custom list.

    Args:
        wsi_dir (str): Path to the directory containing WSIs.
        custom_list_path (Optional[str]): Path to a CSV file with 'wsi' column of relative slide paths.
        wsi_ext (Optional[List[str]]): Allowed file extensions.
        search_nested (bool): Whether to search subdirectories.
        max_workers (int): Threads to use when checking file existence.
        return_relative_paths (bool): Whether to also return relative paths.
        return_mpp_from_csv (bool): Whether to also return MPP values from the CSV.

    Returns:
        List[str]: Full paths to valid WSIs.
        OR
        Tuple[List[str], List[str]]: (full paths, relative paths) if `return_relative_paths` is True.
        OR
        Tuple[List[str], List[str], Optional[List[float]]]: (full paths, relative paths, mpp_values) if `return_mpp_from_csv` is True.
    
    Raises:
        ValueError: If custom CSV is invalid or files not found.
    """
    valid_rel_paths: List[str] = []
    mpp_values: Optional[List[float]] = None

    if wsi_ext is None:
        # Default extensions if not provided
        all_extensions = set(OPENSLIDE_EXTENSIONS) | set(CUCIM_EXTENSIONS) | set(PIL_EXTENSIONS)
        wsi_ext = list(all_extensions)
    wsi_ext = [ext.lower() for ext in wsi_ext]

    if custom_list_path is not None:
        try:
            wsi_df = pd.read_csv(custom_list_path)
        except FileNotFoundError as e:
            raise FileNotFoundError(f"Custom list CSV not found at {custom_list_path}") from e

        if 'wsi' not in wsi_df.columns:
            raise ValueError("Custom list CSV must contain a column named 'wsi'.")
        
        # Ensure 'wsi' column is string and drop NaNs
        wsi_df = wsi_df.dropna(subset=['wsi']).astype({'wsi': str})

        # Pre-filter by extension if custom list contains many non-WSI files
        wsi_df = wsi_df[wsi_df['wsi'].apply(lambda x: any(x.lower().endswith(ext) for ext in wsi_ext))]

        if wsi_df.empty:
            raise ValueError(f"No valid slides found in the custom list at {custom_list_path} after filtering by extension.")

        rel_paths_from_csv = wsi_df['wsi'].tolist()

        # Check existence in parallel
        def exists_fn(rel_path: str) -> bool:
            return os.path.exists(os.path.join(wsi_dir, rel_path))

        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            existence_results = list(executor.map(exists_fn, rel_paths_from_csv))
        
        # Filter rel_paths_from_csv and corresponding MPP values
        final_rel_paths = []
        final_mpp_values = [] if return_mpp_from_csv and 'mpp' in wsi_df.columns else None

        for idx, (rel_path, exists) in enumerate(zip(rel_paths_from_csv, existence_results)):
            if not exists:
                if search_nested: # If searching nested, rel_path should be correct already. If not, it could be a top-level file expected in a subdir.
                    # Warning for missing file for nested search, means rel_path is wrong or file is truly absent.
                    print(f"Warning: WSI '{rel_path}' listed in CSV but not found under '{wsi_dir}' (nested search was {search_nested}). Skipping.")
                else: # If not nested, maybe user provided 'subdir/file.svs' but expects direct lookup
                    # For non-nested search, a rel_path like 'subdir/file.svs' might fail if wsi_dir is 'base_dir' and 'subdir' is not part of 'base_dir'.
                    # We only match top-level files unless search_nested is true.
                    # For custom_list_path, rel_paths are taken as-is, assuming user knows their structure.
                    print(f"Warning: WSI '{rel_path}' listed in CSV but not found in '{wsi_dir}'. Skipping.")
                continue
            
            final_rel_paths.append(rel_path)
            if final_mpp_values is not None:
                mpp_val = wsi_df.loc[wsi_df['wsi'] == rel_path, 'mpp'].iloc[0] # Retrieve MPP for this specific slide
                final_mpp_values.append(float(mpp_val) if pd.notna(mpp_val) else None)

        valid_rel_paths = final_rel_paths
        mpp_values = final_mpp_values

        if not valid_rel_paths:
            raise ValueError(f"No valid slides found in the custom list at {custom_list_path} that exist in '{wsi_dir}'.")

    else: # No custom list provided, scan directory
        def matches_ext(filename: str) -> bool:
            return any(filename.lower().endswith(ext) for ext in wsi_ext)

        if search_nested:
            for root, _, files in os.walk(wsi_dir):
                for f in files:
                    if matches_ext(f):
                        rel_path = os.path.relpath(os.path.join(root, f), wsi_dir)
                        valid_rel_paths.append(rel_path)
        else:
            valid_rel_paths = [
                f for f in os.listdir(wsi_dir)
                if matches_ext(f)
            ]

        valid_rel_paths.sort() # Ensure consistent order

    full_paths = [os.path.join(wsi_dir, rel) for rel in valid_rel_paths]

    if return_mpp_from_csv:
        return full_paths, valid_rel_paths, mpp_values
    elif return_relative_paths:
        return full_paths, valid_rel_paths
    else:
        return full_paths


def get_dir() -> str:
    r"""
    Get Sauron cache directory used for storing downloaded models & weights.
    If :func:`~sauron.feature_extraction.utils.io.set_dir` is not called, default path is ``$SAURON_HOME`` where
    environment variable ``$SAURON_HOME`` defaults to ``$XDG_CACHE_HOME/sauron``.
    ``$XDG_CACHE_HOME`` follows the X Design Group specification of the Linux
    filesystem layout, with a default value ``~/.cache`` if the environment
    variable is not set.
    """

    if _cache_dir is not None:
        return _cache_dir
    return _get_sauron_home()


def set_dir(d: Union[str, os.PathLike]) -> None:
    r"""
    Optionally set the Sauron cache directory used to save downloaded models & weights.
    Args:
        d (str): path to a local folder to save downloaded models & weights.
    """
    global _cache_dir
    _cache_dir = os.path.expanduser(d)


def _get_sauron_home():
    sauron_home = os.path.expanduser(
        os.getenv(
            ENV_SAURON_HOME,
            os.path.join(os.getenv(ENV_XDG_CACHE_HOME, DEFAULT_CACHE_DIR), "sauron"),
        )
    )
    return sauron_home


def has_internet_connection(timeout=3.0) -> bool:
    endpoint = os.environ.get("HF_ENDPOINT", "huggingface.co")
    
    if endpoint.startswith(("http://", "https://")):
        from urllib.parse import urlparse
        endpoint = urlparse(endpoint).netloc
    
    try:
        # Fast socket-level check
        socket.create_connection((endpoint, 443), timeout=timeout)
        return True
    except OSError:
        pass

    try:
        # Fallback HTTP-level check (if requests is available)
        import requests
        url = f"https://{endpoint}" if not endpoint.startswith(("http://", "https://")) else endpoint
        r = requests.head(url, timeout=timeout)
        return r.status_code < 500
    except Exception:
        return False


def get_weights_path(model_type, encoder_name):
    """
    Retrieve the path to the weights file for a given model name.
    This function looks up the path to the weights file in a local checkpoint
    registry (checkpoints.json). If the path in the registry is absolute, it
    returns that path. If the path is relative, it joins the relative path with
    the provided weights_root directory (which is typically a sub-module path).
    Args:
        model_type (str): 'patch', 'slide', or 'seg'
        encoder_name (str): The name of the model whose weights path is to be retrieved.
    Returns:
        str: The absolute path to the weights file or directory.
    """

    assert model_type in ['patch', 'slide', 'seg'], f"Model type must be 'patch', 'slide', or 'seg', not '{model_type}'"

    if model_type == 'patch':
        root = os.path.join(os.path.dirname(__file__), "..", "models", "patch_encoders")
    elif model_type == 'slide':
        root = os.path.join(os.path.dirname(__file__), "..", "models", "slide_encoders")
    else: # model_type == 'seg'
        root = os.path.join(os.path.dirname(__file__), "..", "models", "segmentation")

    registry_path = os.path.join(root, "checkpoints.json")
    
    if not os.path.exists(registry_path):
        raise FileNotFoundError(f"Model checkpoint registry not found at {registry_path}")

    with open(registry_path, "r") as f:
        registry = json.load(f)

    path = registry.get(encoder_name)    
    if path:
        # If path is relative, assume it's relative to the 'zoo' folder or model_type_encoders folder
        # For patch/slide, it's often in 'zoo' or directly under model_type_encoders.
        # For segmentation, it's often directly under segmentation_models.
        
        # Priority:
        # 1. Absolute path as given in JSON
        # 2. Relative to `root` (e.g. sauron/feature_extraction/models/patch_encoders)
        # 3. Relative to `root/zoo`
        
        if os.path.isabs(path):
            if not os.path.exists(path): # If absolute path doesn't exist, return empty string for auto-download
                return "" 
            return path
        
        # Try relative to `root`
        abs_path_candidate = os.path.abspath(os.path.join(root, path))
        if os.path.exists(abs_path_candidate):
            return abs_path_candidate
        
        # Try relative to `root/zoo` (common for many patch/slide models)
        abs_path_candidate_zoo = os.path.abspath(os.path.join(root, 'zoo', path))
        if os.path.exists(abs_path_candidate_zoo):
            return abs_path_candidate_zoo

        # If it's a relative path in the JSON but no corresponding file is found, assume it should be auto-downloaded
        return "" 

    return "" # No path found in registry


def create_lock(path: str, suffix: Optional[str] = None):
    """
    The `create_lock` function creates a lock file to signal that a particular file or process 
    is currently being worked on. This is especially useful in multiprocessing or distributed 
    systems to avoid conflicts or multiple processes working on the same resource.

    Parameters:
    -----------
    path : str
        The path to the file or resource being locked.
    suffix : str, optional
        An additional suffix to append to the lock file name. This allows for creating distinct 
        lock files for similar resources. Defaults to None.

    Returns:
    --------
    None
        The function creates a `.lock` file in the specified path and does not return anything.
    """
    if suffix is not None:
        path = f"{path}_{suffix}"
    lock_file = f"{path}.lock"
    # Create parent directories if they don't exist
    os.makedirs(os.path.dirname(lock_file), exist_ok=True)
    with open(lock_file, 'w') as f:
        f.write("")

def remove_lock(path: str, suffix: Optional[str] = None):
    """
    The `remove_lock` function removes a lock file, signaling that the file or process 
    is no longer in use and is available for other operations.

    Parameters:
    -----------
    path : str
        The path to the file or resource whose lock needs to be removed.
    suffix : str, optional
        An additional suffix to identify the lock file. Defaults to None.

    Returns:
    --------
    None
        The function deletes the `.lock` file associated with the resource.
    """
    if suffix is not None:
        path = f"{path}_{suffix}"
    lock_file = f"{path}.lock"
    if os.path.exists(lock_file):
        os.remove(lock_file)

def is_locked(path: str, suffix: Optional[str] = None):
    """
    The `is_locked` function checks if a resource is currently locked by verifying 
    the existence of a `.lock` file.

    Parameters:
    -----------
    path : str
        The path to the file or resource to check for a lock.
    suffix : str, optional
        An additional suffix to identify the lock file. Defaults to None.

    Returns:
    --------
    bool
        True if the `.lock` file exists, indicating the resource is locked. False otherwise.
    """
    if suffix is not None:
        path = f"{path}_{suffix}"
    return os.path.exists(f"{path}.lock")


def update_log(path_to_log: str, key: str, message: str):
    """
    The `update_log` function appends or updates a message in a log file. It is useful for tracking 
    progress or recording errors during a long-running process.

    Parameters:
    -----------
    path_to_log : str
        The path to the log file where messages will be written.
    key : str
        A unique identifier for the log entry, such as a slide name or file ID.
    message : str
        The message to log, such as a status update or error message.

    Returns:
    --------
    None
        The function writes to the log file in-place.
    """    
    # Ensure log directory exists
    os.makedirs(os.path.dirname(path_to_log), exist_ok=True)

    # Read all lines to check for existing key
    lines = []
    if os.path.exists(path_to_log):
        with open(path_to_log, 'r') as f:
            lines = f.readlines()
    
    # Rewrite log, excluding old entry for key and adding new one
    with open(path_to_log, 'w') as f:
        found = False
        for line in lines:
            if line.split(':', 1)[0] == key: # Only update if key matches EXACTLY
                f.write(f'{key}: {message}\n')
                found = True
            else:
                f.write(line)
        if not found: # If key was not found, append it
            f.write(f'{key}: {message}\n')


def save_h5(save_path: str, assets: Dict[str, Any], attributes: Optional[Dict[str, Any]] = None, mode: str = 'w'):
    """
    The `save_h5` function saves a dictionary of assets to an HDF5 file. This is commonly used to store 
    large datasets or hierarchical data structures in a compact and organized format.

    Parameters:
    -----------
    save_path : str
        The path where the HDF5 file will be saved.
    assets : dict
        A dictionary containing the data to save. Keys represent dataset names, and values are NumPy arrays.
    attributes : dict, optional
        A dictionary mapping dataset names to additional metadata (attributes) to save alongside the data. Defaults to None.
    mode : str, optional
        The file mode for opening the HDF5 file. Options include 'w' (write) and 'a' (append). Defaults to 'w'.

    Returns:
    --------
    None
        The function writes data and attributes to the specified HDF5 file.
    """
    # Create parent directories if they don't exist
    os.makedirs(os.path.dirname(save_path), exist_ok=True)

    with h5py.File(save_path, mode) as file:
        for key, val in assets.items():
            if not isinstance(val, (np.ndarray, torch.Tensor)):
                raise TypeError(f"Asset '{key}' must be a NumPy array or PyTorch tensor, but got {type(val)}")
            
            if isinstance(val, torch.Tensor):
                val = val.cpu().numpy() # Ensure it's a NumPy array

            data_shape = val.shape
            if key not in file:
                data_type = val.dtype
                chunk_shape = (1, ) + data_shape[1:] if data_shape else (1,) # Handle scalar/empty arrays
                maxshape = (None, ) + data_shape[1:] if data_shape else (None,)
                
                dset = file.create_dataset(key, shape=data_shape, maxshape=maxshape, chunks=chunk_shape, dtype=data_type)
                dset[:] = val
                if attributes is not None:
                    if key in attributes.keys():
                        for attr_key, attr_val in attributes[key].items():
                            try:
                                # Serialize if the attribute value is a dictionary
                                if isinstance(attr_val, dict):
                                    attr_val = json.dumps(attr_val)
                                # Serialize Nones
                                elif attr_val is None:
                                    attr_val = 'None'
                                dset.attrs[attr_key] = attr_val
                            except Exception as e:
                                print(f'WARNING: Could not save attribute {attr_key} with value {attr_val} for asset {key}: {e}')
                                
            else:
                dset = file[key]
                # Check if current data can be appended (same dimensionality and compatible shape)

```python
# sauron/feature_extraction/utils/io.py
# Updated file, integrating trident/IO.py and existing sauron utils/gpd_utils and utils/hdf5_utils.

from __future__ import annotations

import torch
import socket
import os
import json
from typing import List, Optional, Union, Tuple, Any
import h5py
import numpy as np
import cv2
import pandas as pd
from geopandas import gpd
from shapely import Polygon
from shapely.validation import make_valid as shapely_make_valid # To avoid name collision if a local function is also named make_valid

from pathlib import Path
from concurrent.futures import ThreadPoolExecutor

# Placeholder imports for WSI extensions. These will be defined in 
# sauron.feature_extraction.wsi.factory, but needed here for `collect_valid_slides`
# To prevent circular imports, we don't import the factory directly, but define a fallback.
try:
    from sauron.feature_extraction.wsi.factory import OPENSLIDE_EXTENSIONS, CUCIM_EXTENSIONS, PIL_EXTENSIONS
except ImportError:
    # Fallback if wsi.factory is not yet available or for standalone testing
    OPENSLIDE_EXTENSIONS = {'.svs', '.tif', '.tiff', '.ndpi', '.vms', '.vmu', '.scn', '.mrxs', '.bif', '.czi'}
    CUCIM_EXTENSIONS = {'.svs', '.tif', '.tiff', '.czi', '.ndpi'} # CuCIM supports more than just svs/tif
    PIL_EXTENSIONS = {'.png', '.jpg', '.jpeg', '.bmp'}


ENV_SAURON_HOME = "SAURON_HOME"
ENV_XDG_CACHE_HOME = "XDG_CACHE_HOME"
DEFAULT_CACHE_DIR = "~/.cache"
_cache_dir: Optional[str] = None


class CuImageWarning(UserWarning):
    """Warning for missing cucim dependency."""
    pass

# No explicit warn_cucim_missing here to avoid multiple warnings.
# The warning is handled by the `cucim` WSI object's lazy_initialize method.

def collect_valid_slides(
    wsi_dir: str,
    custom_list_path: Optional[str] = None,
    wsi_ext: Optional[List[str]] = None,
    search_nested: bool = False,
    max_workers: int = 8,
    return_relative_paths: bool = False,
    return_mpp_from_csv: bool = False,
) -> Union[List[str], Tuple[List[str], List[str]], Tuple[List[str], List[str], Optional[List[float]]]]:
    """
    Retrieve all valid WSI file paths from a directory, optionally filtered by a custom list.

    Args:
        wsi_dir (str): Path to the directory containing WSIs.
        custom_list_path (Optional[str]): Path to a CSV file with 'wsi' column of relative slide paths.
        wsi_ext (Optional[List[str]]): Allowed file extensions.
        search_nested (bool): Whether to search subdirectories.
        max_workers (int): Threads to use when checking file existence.
        return_relative_paths (bool): Whether to also return relative paths.
        return_mpp_from_csv (bool): Whether to also return MPP values from the CSV.

    Returns:
        List[str]: Full paths to valid WSIs.
        OR
        Tuple[List[str], List[str]]: (full paths, relative paths) if `return_relative_paths` is True.
        OR
        Tuple[List[str], List[str], Optional[List[float]]]: (full paths, relative paths, mpp_values) if `return_mpp_from_csv` is True.
    
    Raises:
        ValueError: If custom CSV is invalid or files not found.
    """
    valid_rel_paths: List[str] = []
    mpp_values: Optional[List[float]] = None

    if wsi_ext is None:
        # Default extensions if not provided
        all_extensions = set(OPENSLIDE_EXTENSIONS) | set(CUCIM_EXTENSIONS) | set(PIL_EXTENSIONS)
        wsi_ext = list(all_extensions)
    wsi_ext = [ext.lower() for ext in wsi_ext]

    if custom_list_path is not None:
        try:
            wsi_df = pd.read_csv(custom_list_path)
        except FileNotFoundError as e:
            raise FileNotFoundError(f"Custom list CSV not found at {custom_list_path}") from e

        if 'wsi' not in wsi_df.columns:
            raise ValueError("Custom list CSV must contain a column named 'wsi'.")
        
        # Ensure 'wsi' column is string and drop NaNs
        wsi_df = wsi_df.dropna(subset=['wsi']).astype({'wsi': str})

        # Pre-filter by extension if custom list contains many non-WSI files
        wsi_df = wsi_df[wsi_df['wsi'].apply(lambda x: any(x.lower().endswith(ext) for ext in wsi_ext))]

        if wsi_df.empty:
            raise ValueError(f"No valid slides found in the custom list at {custom_list_path} after filtering by extension.")

        # If custom_list_path is provided, rel_paths are exactly what's in the 'wsi' column
        rel_paths_from_csv = wsi_df['wsi'].tolist()

        # Check existence in parallel
        def exists_fn(rel_path: str) -> bool:
            return os.path.exists(os.path.join(wsi_dir, rel_path))

        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            existence_results = list(executor.map(exists_fn, rel_paths_from_csv))
        
        # Filter rel_paths_from_csv and corresponding MPP values
        final_rel_paths = []
        # Initialize final_mpp_values only if 'mpp' column exists and is requested
        final_mpp_values = [] if return_mpp_from_csv and 'mpp' in wsi_df.columns else None

        for idx, (rel_path, exists) in enumerate(zip(rel_paths_from_csv, existence_results)):
            if not exists:
                # Warning for missing file for nested search, means rel_path is wrong or file is truly absent.
                print(f"Warning: WSI '{rel_path}' listed in CSV but not found under '{wsi_dir}'. Skipping.")
                continue
            
            final_rel_paths.append(rel_path)
            if final_mpp_values is not None:
                # It's important to use .loc and check for potential NaNs for the MPP value
                mpp_val = wsi_df.loc[wsi_df['wsi'] == rel_path, 'mpp'].iloc[0] # Retrieve MPP for this specific slide
                final_mpp_values.append(float(mpp_val) if pd.notna(mpp_val) else None)

        valid_rel_paths = final_rel_paths
        mpp_values = final_mpp_values

        if not valid_rel_paths:
            raise ValueError(f"No valid slides found in the custom list at {custom_list_path} that exist in '{wsi_dir}'.")

    else: # No custom list provided, scan directory
        def matches_ext(filename: str) -> bool:
            return any(filename.lower().endswith(ext) for ext in wsi_ext)

        if search_nested:
            for root, _, files in os.walk(wsi_dir):
                for f in files:
                    if matches_ext(f):
                        rel_path = os.path.relpath(os.path.join(root, f), wsi_dir)
                        valid_rel_paths.append(rel_path)
        else:
            valid_rel_paths = [
                f for f in os.listdir(wsi_dir)
                if matches_ext(f) and os.path.isfile(os.path.join(wsi_dir, f)) # Ensure it's a file
            ]

        valid_rel_paths.sort() # Ensure consistent order

    full_paths = [os.path.join(wsi_dir, rel) for rel in valid_rel_paths]

    if return_mpp_from_csv:
        return full_paths, valid_rel_paths, mpp_values
    elif return_relative_paths:
        return full_paths, valid_rel_paths
    else:
        return full_paths


def get_dir() -> str:
    r"""
    Get Sauron cache directory used for storing downloaded models & weights.
    If :func:`~sauron.feature_extraction.utils.io.set_dir` is not called, default path is ``$SAURON_HOME`` where
    environment variable ``$SAURON_HOME`` defaults to ``$XDG_CACHE_HOME/sauron``.
    ``$XDG_CACHE_HOME`` follows the X Design Group specification of the Linux
    filesystem layout, with a default value ``~/.cache`` if the environment
    variable is not set.
    """

    if _cache_dir is not None:
        return _cache_dir
    return _get_sauron_home()


def set_dir(d: Union[str, os.PathLike]) -> None:
    r"""
    Optionally set the Sauron cache directory used to save downloaded models & weights.
    Args:
        d (str): path to a local folder to save downloaded models & weights.
    """
    global _cache_dir
    _cache_dir = os.path.expanduser(d)


def _get_sauron_home():
    sauron_home = os.path.expanduser(
        os.getenv(
            ENV_SAURON_HOME,
            os.path.join(os.getenv(ENV_XDG_CACHE_HOME, DEFAULT_CACHE_DIR), "sauron"),
        )
    )
    return sauron_home


def has_internet_connection(timeout=3.0) -> bool:
    endpoint = os.environ.get("HF_ENDPOINT", "huggingface.co")
    
    if endpoint.startswith(("http://", "https://")):
        from urllib.parse import urlparse
        endpoint = urlparse(endpoint).netloc
    
    try:
        # Fast socket-level check
        socket.create_connection((endpoint, 443), timeout=timeout)
        return True
    except OSError:
        pass

    try:
        # Fallback HTTP-level check (if requests is available)
        import requests
        url = f"https://{endpoint}" if not endpoint.startswith(("http://", "https://")) else endpoint
        r = requests.head(url, timeout=timeout)
        return r.status_code < 500
    except Exception:
        return False


def get_weights_path(model_type: str, encoder_name: str) -> str:
    """
    Retrieve the path to the weights file for a given model name.
    This function looks up the path to the weights file in a local checkpoint
    registry (checkpoints.json). If the path in the registry is absolute, it
    returns that path. If the path is relative, it joins the relative path with
    the provided weights_root directory (which is typically a sub-module path).
    Args:
        model_type (str): 'patch', 'slide', or 'seg'
        encoder_name (str): The name of the model whose weights path is to be retrieved.
    Returns:
        str: The absolute path to the weights file or directory. Returns empty string if not found locally.
    """

    assert model_type in ['patch', 'slide', 'seg'], f"Model type must be 'patch', 'slide', or 'seg', not '{model_type}'"

    if model_type == 'patch':
        root = os.path.join(os.path.dirname(__file__), "..", "models", "patch_encoders")
    elif model_type == 'slide':
        root = os.path.join(os.path.dirname(__file__), "..", "models", "slide_encoders")
    else: # model_type == 'seg'
        root = os.path.join(os.path.dirname(__file__), "..", "models", "segmentation")

    registry_path = os.path.join(root, "checkpoints.json")
    
    if not os.path.exists(registry_path):
        # This is a critical error for configured models, so raise explicitly
        raise FileNotFoundError(f"Model checkpoint registry not found at {registry_path}")

    with open(registry_path, "r") as f:
        registry = json.load(f)

    path = registry.get(encoder_name)    
    if path:
        # Priority:
        # 1. Absolute path as given in JSON
        # 2. Relative to `root` (e.g. sauron/feature_extraction/models/patch_encoders)
        # 3. Relative to `root/zoo` (common for many patch/slide models with complex structures)
        
        if os.path.isabs(path):
            if not os.path.exists(path): # If absolute path doesn't exist, return empty string for auto-download
                return "" 
            return path
        
        # Try relative to `root`
        abs_path_candidate = os.path.abspath(os.path.join(root, path))
        if os.path.exists(abs_path_candidate):
            return abs_path_candidate
        
        # Try relative to `root/zoo` (common for many patch/slide models)
        abs_path_candidate_zoo = os.path.abspath(os.path.join(root, 'zoo', path))
        if os.path.exists(abs_path_candidate_zoo):
            return abs_path_candidate_zoo

        # If it's a relative path in the JSON but no corresponding file is found, assume it should be auto-downloaded
        return "" 

    return "" # No path found in registry


def create_lock(path: str, suffix: Optional[str] = None):
    """
    The `create_lock` function creates a lock file to signal that a particular file or process 
    is currently being worked on. This is especially useful in multiprocessing or distributed 
    systems to avoid conflicts or multiple processes working on the same resource.

    Parameters:
    -----------
    path : str
        The path to the file or resource being locked.
    suffix : str, optional
        An additional suffix to append to the lock file name. This allows for creating distinct 
        lock files for similar resources. Defaults to None.

    Returns:
    --------
    None
        The function creates a `.lock` file in the specified path and does not return anything.
    """
    if suffix is not None:
        path = f"{path}_{suffix}"
    lock_file = f"{path}.lock"
    # Create parent directories if they don't exist
    os.makedirs(os.path.dirname(lock_file), exist_ok=True)
    with open(lock_file, 'w') as f:
        f.write("")

def remove_lock(path: str, suffix: Optional[str] = None):
    """
    The `remove_lock` function removes a lock file, signaling that the file or process 
    is no longer in use and is available for other operations.

    Parameters:
    -----------
    path : str
        The path to the file or resource whose lock needs to be removed.
    suffix : str, optional
        An additional suffix to identify the lock file. Defaults to None.

    Returns:
    --------
    None
        The function deletes the `.lock` file associated with the resource.
    """
    if suffix is not None:
        path = f"{path}_{suffix}"
    lock_file = f"{path}.lock"
    if os.path.exists(lock_file):
        os.remove(lock_file)

def is_locked(path: str, suffix: Optional[str] = None):
    """
    The `is_locked` function checks if a resource is currently locked by verifying 
    the existence of a `.lock` file.

    Parameters:
    -----------
    path : str
        The path to the file or resource to check for a lock.
    suffix : str, optional
        An additional suffix to identify the lock file. Defaults to None.

    Returns:
    --------
    bool
        True if the `.lock` file exists, indicating the resource is locked. False otherwise.
    """
    if suffix is not None:
        path = f"{path}_{suffix}"
    return os.path.exists(f"{path}.lock")


def update_log(path_to_log: str, key: str, message: str):
    """
    The `update_log` function appends or updates a message in a log file. It is useful for tracking 
    progress or recording errors during a long-running process.

    Parameters:
    -----------
    path_to_log : str
        The path to the log file where messages will be written.
    key : str
        A unique identifier for the log entry, such as a slide name or file ID.
    message : str
        The message to log, such as a status update or error message.

    Returns:
    --------
    None
        The function writes to the log file in-place.
    """    
    # Ensure log directory exists
    os.makedirs(os.path.dirname(path_to_log), exist_ok=True)

    # Read all lines to check for existing key
    lines = []
    if os.path.exists(path_to_log):
        with open(path_to_log, 'r') as f:
            lines = f.readlines()
    
    # Rewrite log, excluding old entry for key and adding new one
    with open(path_to_log, 'w') as f:
        found = False
        for line in lines:
            # Check if line starts with key + ':' to avoid partial matches
            if line.strip().startswith(f'{key}:'): 
                f.write(f'{key}: {message}\n')
                found = True
            else:
                f.write(line)
        if not found: # If key was not found, append it
            f.write(f'{key}: {message}\n')


def save_h5(save_path: str, assets: Dict[str, Any], attributes: Optional[Dict[str, Any]] = None, mode: str = 'w'):
    """
    The `save_h5` function saves a dictionary of assets to an HDF5 file. This is commonly used to store 
    large datasets or hierarchical data structures in a compact and organized format.

    Parameters:
    -----------
    save_path : str
        The path where the HDF5 file will be saved.
    assets : dict
        A dictionary containing the data to save. Keys represent dataset names, and values are NumPy arrays.
    attributes : dict, optional
        A dictionary mapping dataset names to additional metadata (attributes) to save alongside the data. Defaults to None.
    mode : str, optional
        The file mode for opening the HDF5 file. Options include 'w' (write) and 'a' (append). Defaults to 'w'.

    Returns:
    --------
    None
        The function writes data and attributes to the specified HDF5 file.
    """
    # Create parent directories if they don't exist
    os.makedirs(os.path.dirname(save_path), exist_ok=True)

    with h5py.File(save_path, mode) as file:
        for key, val in assets.items():
            if not isinstance(val, (np.ndarray, torch.Tensor)):
                raise TypeError(f"Asset '{key}' must be a NumPy array or PyTorch tensor, but got {type(val)}")
            
            if isinstance(val, torch.Tensor):
                val = val.cpu().numpy() # Ensure it's a NumPy array

            data_shape = val.shape
            if key not in file:
                data_type = val.dtype
                # Handle empty array/scalar chunking
                chunk_shape = (1, ) + data_shape[1:] if data_shape else (1,) 
                maxshape = (None, ) + data_shape[1:] if data_shape else (None,)
                
                # Special handling for string data type if needed, but for numerical/bool h5py handles automatically
                # if data_type == np.object_: # E.g., for strings, use h5py.string_dtype
                #    data_type = h5py.string_dtype(encoding='utf-8')

                dset = file.create_dataset(key, shape=data_shape, maxshape=maxshape, chunks=chunk_shape, dtype=data_type)
                dset[:] = val
                if attributes is not None:
                    if key in attributes.keys():
                        for attr_key, attr_val in attributes[key].items():
                            try:
                                # Serialize if the attribute value is a dictionary
                                if isinstance(attr_val, dict):
                                    attr_val = json.dumps(attr_val)
                                # Serialize Nones to string 'None'
                                elif attr_val is None:
                                    attr_val = 'None'
                                dset.attrs[attr_key] = attr_val
                            except Exception as e:
                                print(f'WARNING: Could not save attribute {attr_key} with value {attr_val} for asset {key}: {e}')
                                
            else: # Dataset already exists, append data
                dset = file[key]
                # Ensure compatibility before resizing and appending
                if dset.ndim != val.ndim:
                    raise ValueError(f"Cannot append to dataset '{key}'. Dimensionality mismatch: existing {dset.ndim} vs new {val.ndim}.")
                if dset.ndim > 1 and dset.shape[1:] != val.shape[1:]:
                    raise ValueError(f"Cannot append to dataset '{key}'. Shape mismatch: existing {dset.shape[1:]} vs new {val.shape[1:]}.")

                dset.resize(len(dset) + data_shape[0], axis=0)
                if dset.dtype != val.dtype:
                    # Attempt type conversion if possible, otherwise raise error
                    try:
                        val_converted = val.astype(dset.dtype)
                        dset[-data_shape[0]:] = val_converted
                    except TypeError:
                        raise TypeError(f"Cannot append to dataset '{key}'. Data type mismatch and automatic conversion failed: existing {dset.dtype} vs new {val.dtype}.")
                else:
                    dset[-data_shape[0]:] = val

def read_coords(coords_path: str) -> Tuple[Dict[str, Any], np.ndarray]:
    """
    The `read_coords` function reads patch coordinates from an HDF5 file, along with any user-defined 
    attributes stored during the patching process. This function is essential for workflows that rely 
    on spatial metadata, such as patch-based analysis in computational pathology.

    Parameters:
    -----------
    coords_path : str
        The path to the HDF5 file containing patch coordinates and attributes.

    Returns:
    --------
    attrs : dict
        A dictionary of user-defined attributes stored during patching.
    coords : np.array
        An array of patch coordinates at level 0.
    """
    if not os.path.exists(coords_path):
        raise FileNotFoundError(f"Coordinate file not found: {coords_path}")
    
    with h5py.File(coords_path, 'r') as f:
        if 'coords' not in f:
            raise ValueError(f"Dataset 'coords' not found in HDF5 file: {coords_path}")
        
        attrs = dict(f['coords'].attrs)
        coords = f['coords'][:]
    return attrs, coords


def read_coords_legacy(coords_path: str) -> Tuple[int, int, int, np.ndarray]:
    """
    The `read_coords_legacy` function reads legacy patch coordinates from an HDF5 file. This function 
    is designed for compatibility with older patching tools such as CLAM or Fishing-Rod, which used 
    a different structure for storing patching metadata.

    Parameters:
    -----------
    coords_path : str
        The path to the HDF5 file containing legacy patch coordinates and metadata.

    Returns:
    --------
    patch_size : int
        The target patch size at the desired magnification.
    patch_level : int
        The patch level used when reading the slide.
    custom_downsample : int
        Any additional downsampling applied to the patches.
    coords : np.array
        An array of patch coordinates.
    """
    if not os.path.exists(coords_path):
        raise FileNotFoundError(f"Legacy coordinate file not found: {coords_path}")

    with h5py.File(coords_path, 'r') as f:
        if 'coords' not in f:
            raise ValueError(f"Dataset 'coords' not found in legacy HDF5 file: {coords_path}")
            
        patch_size = f['coords'].attrs['patch_size']
        patch_level = f['coords'].attrs['patch_level']
        # Default to 1 if 'custom_downsample' attribute is missing
        custom_downsample = f['coords'].attrs.get('custom_downsample', 1) 
        coords = f['coords'][:]
    return patch_size, patch_level, custom_downsample, coords


def mask_to_gdf(
    mask: np.ndarray,
    keep_ids: List[int] = [],
    exclude_ids: List[int] = [],
    max_nb_holes: int = 0,
    min_contour_area: float = 1000,
    pixel_size: float = 1,
    contour_scale: float = 1.0
) -> gpd.GeoDataFrame:
    """
    Convert a binary mask into a GeoDataFrame of polygons representing detected regions.

    This function processes a binary mask to identify contours, filter them based on specified parameters,
    and scale them to the desired dimensions. The output is a GeoDataFrame where each row corresponds 
    to a detected region, with polygons representing the tissue contours and their associated holes.

    Args:
        mask (np.ndarray): The binary mask to process, where non-zero regions represent areas of interest.
        keep_ids (List[int], optional): A list of contour indices to keep. Defaults to an empty list (keep all).
        exclude_ids (List[int], optional): A list of contour indices to exclude. Defaults to an empty list.
        max_nb_holes (int, optional): The maximum number of holes to retain for each contour. 
            Use 0 to retain no holes. Defaults to 0.
        min_contour_area (float, optional): Minimum area (in pixels) for a contour to be retained. Defaults to 1000.
        pixel_size (float, optional): Pixel size of level 0. Defaults to 1.
        contour_scale (float, optional): Scaling factor for the output polygons. Defaults to 1.0.

    Returns:
        gpd.GeoDataFrame: A GeoDataFrame containing polygons for the detected regions. The GeoDataFrame
        includes a `tissue_id` column (integer ID for each region) and a `geometry` column (polygons).

    Raises:
        Exception: If no valid contours are detected in the mask.

    Notes:
        - The function internally downsamples the input mask for efficiency before finding contours.
        - The resulting polygons are scaled back to the original resolution using the `contour_scale` parameter.
        - Holes in contours are also detected and included in the resulting polygons.
    """

    TARGET_EDGE_SIZE = 2000
    scale = TARGET_EDGE_SIZE / mask.shape[0] if mask.shape[0] > 0 else 1.0 # Avoid division by zero for empty masks

    downscaled_mask = cv2.resize(mask, (round(mask.shape[1] * scale), round(mask.shape[0] * scale)))

    # Find and filter contours
    # RETR_TREE: retrieves all contours and reconstructs a full hierarchy of nested contours.
    # RETR_CCOMP: retrieves all of the contours and organizes them into a two-level hierarchy.
    #             At the first level are the external boundaries of the components.
    #             At the second level are the boundaries of the holes inside those components.
    mode = cv2.RETR_TREE if max_nb_holes == 0 else cv2.RETR_CCOMP
    contours, hierarchy = cv2.findContours(downscaled_mask, mode, cv2.CHAIN_APPROX_NONE)

    if hierarchy is None: # No contours found
        hierarchy = np.array([])
    else:
        hierarchy = np.squeeze(hierarchy, axis=(0,))[:, 2:] # Remove batch dim and keep parent/child indices

    filter_params = {
        'filter_color_mode': 'none', # Not used in this function's logic
        'max_n_holes': max_nb_holes,
        'a_t': min_contour_area * pixel_size ** 2, # Minimum area in original pixels^2
        'min_hole_area': 4000 * pixel_size ** 2 # Minimum hole area in original pixels^2
    }

    foreground_contours, hole_contours = filter_contours(contours, hierarchy, filter_params)

    if len(foreground_contours) == 0:
        print(f"[Warning] No contour were detected. Contour GeoJSON will be empty.")
        return gpd.GeoDataFrame(columns=['tissue_id', 'geometry'])
    else:
        # Scale contours back to level 0 (original resolution)
        # The scale applied here is `(original_pixels / target_pixels_in_mask) * (output_geojson_scale / original_mpp_scale)`
        # `contour_scale` is typically 1.0. `scale` is target_edge_size / mask_height.
        # So overall factor is `1 / scale = original_height / target_edge_size`
        final_scale_factor = contour_scale / scale # Example: 1.0 / (2000 / original_mask_height)

        contours_tissue = scale_contours(foreground_contours, final_scale_factor, is_nested=False)
        contours_holes = scale_contours(hole_contours, final_scale_factor, is_nested=True)

    if len(keep_ids) > 0:
        contour_ids = set(keep_ids) - set(exclude_ids)
    else:
        # Default to all found contours if no specific IDs are kept/excluded
        contour_ids = set(np.arange(len(contours_tissue))) - set(exclude_ids)

    tissue_ids = [i for i in sorted(list(contour_ids))] # Ensure consistent order
    polygons = []
    for i in tissue_ids:
        # Ensure contour is valid before creating polygon
        if i >= len(contours_tissue): # Skip if contour_id is out of bounds
            continue

        holes = [contours_holes[i][j].squeeze(1) for j in range(len(contours_holes[i]))] if len(contours_holes[i]) > 0 else None
        
        # Ensure exterior coords are at least 3 points for a valid polygon
        exterior_coords = contours_tissue[i].squeeze(1)
        if len(exterior_coords) < 3:
            continue # Skip invalid contour

        polygon = Polygon(exterior_coords, holes=holes)
        if not polygon.is_valid:
            polygon = fix_invalid_polygon(polygon) # Use the shapely fix_invalid_polygon
        
        if polygon.is_empty: # After fixing, it might become empty
            continue

        polygons.append(polygon)
    
    # Create GeoDataFrame with tissue_id and geometry
    gdf_contours = gpd.GeoDataFrame(pd.DataFrame(tissue_ids[:len(polygons)], columns=['tissue_id']), geometry=polygons)
    
    # Set a Coordinate Reference System (CRS) - Web Mercator is common for geospatial data visualization
    gdf_contours.set_crs("EPSG:3857", inplace=True, allow_override=True)
    
    return gdf_contours


def filter_contours(contours: List[np.ndarray], hierarchy: np.ndarray, filter_params: Dict[str, float]) -> Tuple[List[np.ndarray], List[List[np.ndarray]]]:
    """
    The `filter_contours` function processes a list of contours and their hierarchy, filtering 
    them based on specified criteria such as minimum area and hole limits. This function is 
    typically used in digital pathology workflows to isolate meaningful tissue regions.

    Original implementation from: https://github.com/mahmoodlab/CLAM/blob/f1e93945d5f5ac6ed077cb020ed01cf984780a77/wsi_core/WholeSlideImage.py#L97

    Parameters:
    -----------
    contours : list
        A list of contours representing detected regions.
    hierarchy : np.ndarray
        The hierarchy of the contours, used to identify relationships (e.g., parent-child).
    filter_params : dict
        A dictionary containing filtering criteria. Expected keys include:
        - `max_n_holes`: Maximum number of holes to retain.
        - `a_t`: Minimum area threshold for contours (in pixel_size units squared).
        - `min_hole_area`: Minimum area threshold for holes (in pixel_size units squared).

    Returns:
    --------
    tuple:
        A tuple containing:
        - Filtered foreground contours (list)
        - Corresponding hole contours (list)
    """
    if not hierarchy.size:
        return [], []

    # Find indices of foreground contours (parent == -1)
    foreground_indices = np.flatnonzero(hierarchy[:, 1] == -1)
    filtered_foregrounds = []
    filtered_holes = []

    # Loop through each foreground contour
    for cont_idx in foreground_indices:
        contour = contours[cont_idx]
        hole_indices = np.flatnonzero(hierarchy[:, 1] == cont_idx)

        # Calculate area of the contour (foreground area minus holes)
        contour_area = cv2.contourArea(contour)
        hole_areas = [cv2.contourArea(contours[hole_idx]) for hole_idx in hole_indices]
        
        # Area is already scaled by pixel_size^2 in mask_to_gdf, so just use raw contour_area
        net_area = (contour_area - sum(hole_areas)) # Already in target_edge_size pixels

        # Skip contours with negligible area (already converted by pixel_size in mask_to_gdf)
        if net_area <= 0 or net_area <= filter_params['a_t']:
            continue

        # Append valid contours
        filtered_foregrounds.append(contour)

        # Filter and limit the number of holes
        valid_holes = [
            contours[hole_idx]
            for hole_idx in hole_indices
            if cv2.contourArea(contours[hole_idx]) > filter_params['min_hole_area'] # Area check against mask_pixels
        ]
        valid_holes = sorted(valid_holes, key=cv2.contourArea, reverse=True)[:filter_params['max_n_holes']]
        filtered_holes.append(valid_holes)

    return filtered_foregrounds, filtered_holes


def fix_invalid_polygon(polygon: Polygon) -> Polygon:
    """
    The `fix_invalid_polygon` function attempts to fix invalid polygons by applying small buffer operations. 
    This is particularly useful in cases where geometric operations result in self-intersecting 
    or malformed polygons.

    Parameters:
    -----------
    polygon : shapely.geometry.Polygon
        The input polygon that may be invalid.

    Returns:
    --------
    shapely.geometry.Polygon
        A valid polygon object.

    Raises:
    -------
    ValueError:
        If the function fails to create a valid polygon after several attempts.
    """
    
    # Try using shapely's built-in make_valid first, which is often sufficient
    if not polygon.is_valid:
        new_polygon = shapely_make_valid(polygon)
        if isinstance(new_polygon, Polygon) and new_polygon.is_valid:
            return new_polygon

    # If shapely_make_valid fails or returns a MultiPolygon/GeometryCollection, try buffering
    # (buffering with 0 can sometimes fix geometry issues)
    # The original trident code also tried +/- 0.1, 0.2
    for i in [0, 0.1, -0.1, 0.2]:
        new_polygon = polygon.buffer(i)
        if isinstance(new_polygon, Polygon) and new_polygon.is_valid:
            return new_polygon
    
    # Fallback to a warning and returning potentially invalid or empty polygon if all attempts fail
    # Or, raise an error if strict validity is required.
    warnings.warn(f"Failed to make a valid polygon after multiple attempts. Original polygon might be too complex or malformed.")
    return polygon # Return the last attempt or original polygon


def scale_contours(contours: Union[List[np.ndarray], List[List[np.ndarray]]], scale: float, is_nested: bool = False) -> Union[List[np.ndarray], List[List[np.ndarray]]]:
    """
    The `scale_contours` function scales the dimensions of contours or nested contours (e.g., holes) 
    by a specified factor. This is useful for resizing detected regions in masks or GeoDataFrames.

    Parameters:
    -----------
    contours : list
        A list of contours (or nested lists for holes) to be scaled.
    scale : float
        The scaling factor to apply.
    is_nested : bool, optional
        Indicates whether the input is a nested list of contours (e.g., for holes). Defaults to False.

    Returns:
    --------
    list:
        A list of scaled contours or nested contours.
    """
    if is_nested:
        return [[np.array(hole * scale, dtype=np.int32) for hole in holes] for holes in contours]
    return [np.array(cont * scale, dtype=np.int32) for cont in contours]


def overlay_gdf_on_thumbnail(
    gdf_contours: gpd.GeoDataFrame, thumbnail: np.ndarray, contours_saveto: str, scale: float, tissue_color: Tuple[int, int, int] = (0, 255, 0), hole_color: Tuple[int, int, int] = (255, 0, 0)
):
    """
    The `overlay_gdf_on_thumbnail` function overlays polygons from a GeoDataFrame onto a scaled 
    thumbnail image using OpenCV. This is particularly useful for visualizing tissue regions and 
    their boundaries on smaller representations of whole-slide images.

    Parameters:
    -----------
    gdf_contours : gpd.GeoDataFrame
        A GeoDataFrame containing the polygons to overlay, with a `geometry` column.
    thumbnail : np.ndarray
        The thumbnail image as a NumPy array (RGB or BGR).
    contours_saveto : str
        The file path to save the annotated thumbnail.
    scale : float
        The scaling factor between the GeoDataFrame coordinates and the thumbnail resolution.
    tissue_color : tuple, optional
        The color (BGR format) for tissue polygons. Defaults to green `(0, 255, 0)`.
    hole_color : tuple, optional
        The color (BGR format) for hole polygons. Defaults to red `(255, 0, 0)`.

    Returns:
    --------
    None
        The function saves the annotated image to the specified file path.
    """
    # Ensure thumbnail is mutable
    annotated_thumbnail = np.copy(thumbnail)
    
    # Convert to BGR if it's RGB (OpenCV uses BGR by default for drawing)
    if annotated_thumbnail.shape[2] == 3 and annotated_thumbnail[0,0,0] > annotated_thumbnail[0,0,2]: # Simple check for RGB vs BGR
        annotated_thumbnail = cv2.cvtColor(annotated_thumbnail, cv2.COLOR_RGB2BGR)

    for poly in gdf_contours.geometry:
        if poly.is_empty:
            continue

        # Draw tissue boundary
        if poly.exterior:
            exterior_coords = (np.array(poly.exterior.coords) * scale).astype(np.int32)
            # Reshape to (N, 1, 2) for cv2.polylines
            cv2.polylines(annotated_thumbnail, [exterior_coords.reshape(-1, 1, 2)], isClosed=True, color=tissue_color, thickness=2)

        # Draw holes (if any) in a different color
        if poly.interiors:
            for interior in poly.interiors:
                interior_coords = (np.array(interior.coords) * scale).astype(np.int32)
                cv2.polylines(annotated_thumbnail, [interior_coords.reshape(-1, 1, 2)], isClosed=True, color=hole_color, thickness=2)

    # Crop black borders of the annotated image (assuming black background if it was filled)
    # This might be too aggressive if the thumbnail itself has black regions
    gray_thumbnail = cv2.cvtColor(annotated_thumbnail, cv2.COLOR_BGR2GRAY)
    nz = np.nonzero(gray_thumbnail) # Non-zero pixel locations
    if nz[0].size > 0: # Check if there are any non-zero pixels
        ymin, ymax = np.min(nz[0]), np.max(nz[0])
        xmin, xmax = np.min(nz[1]), np.max(nz[1])
        cropped_annotated = annotated_thumbnail[ymin:ymax, xmin:xmax]
    else:
        cropped_annotated = annotated_thumbnail # No non-zero pixels, keep as is
 
    # Save the annotated image
    os.makedirs(os.path.dirname(contours_saveto), exist_ok=True)
    # Convert back to RGB for PIL-compatible saving if it was BGR for drawing
    cropped_annotated = cv2.cvtColor(cropped_annotated, cv2.COLOR_BGR2RGB)
    Image.fromarray(cropped_annotated).save(contours_saveto)


def get_num_workers(batch_size: int, 
                    factor: float = 0.75, 
                    fallback: int = 16, 
                    max_workers: int | None = None) -> int:
    """
    The `get_num_workers` function calculates the optimal number of workers for a PyTorch DataLoader, 
    balancing system resources and workload. This ensures efficient data loading while avoiding 
    resource overutilization.

    Parameters:
    -----------
    batch_size : int
        The batch size for the DataLoader. This is used to limit the number of workers.
    factor : float, optional
        The fraction of available CPU cores to use. Defaults to 0.75 (75% of available cores).
    fallback : int, optional
        The default number of workers to use if the system's CPU core count cannot be determined. Defaults to 16.
    max_workers : int or None, optional
        The maximum number of workers allowed. Defaults to `2 * batch_size` if not provided.

    Returns:
    --------
    int
        The calculated number of workers for the DataLoader.

    Example:
    --------
    >>> num_workers = get_num_workers(batch_size=64, factor=0.5)
    >>> print(num_workers)
    8

    Notes:
    ------
    - The number of workers is clipped to a minimum of 1 to ensure multiprocessing is not disabled.
    - The maximum number of workers defaults to `2 * batch_size` unless explicitly specified.
    - The function ensures compatibility with systems where `os.cpu_count()` may return `None`.
    - On Windows systems, the number of workers is always set to 0 to ensure compatibility with PyTorch datasets whose attributes may not be serializable.
    """

    # Disable pytorch multiprocessing on Windows
    if os.name == 'nt':
        return 0
    
    num_cores = os.cpu_count() or fallback
    num_workers_calculated = int(factor * num_cores)  # Use a fraction of available cores
    
    # Default max_workers to 2 * batch_size if not specified
    max_workers_effective = max_workers if max_workers is not None else (2 * batch_size) 
    
    num_workers = np.clip(num_workers_calculated, 1, max_workers_effective)
    return int(num_workers)

```

```python
# sauron/feature_extraction/utils/misc.py
# New file, adapted from trident/Maintenance.py

import functools
import warnings

def deprecated(func):
    """This is a decorator which can be used to mark functions
    as deprecated. It will result in a warning being emitted
    when the function is used."""
    @functools.wraps(func)
    def new_func(*args, **kwargs):
        warnings.simplefilter('always', DeprecationWarning)  # turn off filter
        warnings.warn("Call to deprecated function {}.".format(func.__name__),
                      category=DeprecationWarning,
                      stacklevel=2)
        warnings.simplefilter('default', DeprecationWarning)  # reset filter
        return func(*args, **kwargs)
    return new_func

```

```python
# sauron/feature_extraction/wsi/base.py
# New file, adapted from trident/wsi_objects/WSI.py

from __future__ import annotations
import numpy as np
from PIL import Image
import os 
import warnings
import torch 
from typing import List, Tuple, Optional, Literal, Dict, Any
from torch.utils.data import DataLoader
from tqdm import tqdm

from sauron.feature_extraction.wsi.patching import WSIPatcher # Use WSIPatcher for type hinting
from sauron.feature_extraction.wsi.dataset import WSIPatcherDataset # Use WSIPatcherDataset for dataloader
from sauron.feature_extraction.utils.io import (
    save_h5, read_coords, read_coords_legacy,
    mask_to_gdf, overlay_gdf_on_thumbnail, get_num_workers
)
# Import segmentation models and patch/slide encoders for type hinting and internal use
from sauron.feature_extraction.models.segmentation.factory import SegmentationModel # For type hinting
from sauron.feature_extraction.models.patch_encoders.factory import BasePatchEncoder # For type hinting
from sauron.feature_extraction.models.slide_encoders.factory import BaseSlideEncoder # For type hinting


ReadMode = Literal['pil', 'numpy']


class WSI:
    """
    The `WSI` class provides an interface to work with Whole Slide Images (WSIs). 
    It supports lazy initialization, metadata extraction, tissue segmentation,
    patching, and feature extraction. The class handles various WSI file formats and 
    offers utilities for integration with AI models.

    Attributes
    ----------
    slide_path : str
        Path to the WSI file (the path from which the WSI object is currently loaded).
    original_path : str
        The original path to the WSI file, typically in the source directory. Useful for caching logic.
    name : str
        Name of the WSI (inferred from the file path if not provided).
    custom_mpp_keys : List[str]
        Custom keys for extracting microns per pixel (MPP) and magnification metadata.
    lazy_init : bool
        Indicates whether lazy initialization is used.
    tissue_seg_path : str
        Path to a tissue segmentation mask (if available).
    width : int
        Width of the WSI in pixels (set during lazy initialization).
    height : int
        Height of the WSI in pixels (set during lazy initialization).
    dimensions : Tuple[int, int]
        (width, height) tuple of the WSI (set during lazy initialization).
    mpp : float
        Microns per pixel (set during lazy initialization or inferred).
    mag : float
        Estimated magnification level (set during lazy initialization or inferred).
    level_count : int
        Number of resolution levels in the WSI (set during lazy initialization).
    level_downsamples : List[float]
        Downsampling factors for each pyramid level (set during lazy initialization).
    level_dimensions : List[Tuple[int, int]]
        Dimensions of the WSI at each pyramid level (set during lazy initialization).
    properties : dict
        Metadata properties extracted from the image backend (set during lazy initialization).
    img : Any
        Backend-specific image object used for reading regions (set during lazy initialization).
    gdf_contours : geopandas.GeoDataFrame
        Tissue segmentation mask as a GeoDataFrame, if available (set during lazy initialization).
    """

    def __init__(
        self,
        slide_path: str, # Path where the WSI is *currently* located (could be cache)
        original_path: Optional[str] = None, # Original path (source directory)
        name: Optional[str] = None,
        tissue_seg_path: Optional[str] = None,
        custom_mpp_keys: Optional[List[str]] = None,
        lazy_init: bool = True,
        mpp: Optional[float] = None,
        max_workers: Optional[int] = None,
    ):
        """
        Initialize the `WSI` object for working with a Whole Slide Image (WSI).

        Args:
        -----
        slide_path : str
            Path to the WSI file (where it is currently accessible, e.g., in cache).
        original_path : str, optional
            The original full path to the WSI file in its source directory. Used for tracking
            and potentially for caching logic. If None, defaults to `slide_path`.
        name : str, optional
            Optional name for the WSI. Defaults to the filename (without extension).
        tissue_seg_path : str, optional
            Path to the tissue segmentation mask file. Defaults to None.
        custom_mpp_keys : Optional[List[str]]
            Custom keys for extracting MPP and magnification metadata. Defaults to None.
        lazy_init : bool, optional
            If True, defer loading the WSI until required. Defaults to True.
        mpp: float, optional
            If not None, will be the reference micron per pixel (mpp). Handy when mpp is not provided in the WSI.
        max_workers (Optional[int]): Maximum number of workers for data loading

        """
        self.slide_path = slide_path
        self.original_path = original_path if original_path is not None else slide_path
        if name is None:
            self.name, self.ext = os.path.splitext(os.path.basename(slide_path)) 
        else:
            self.name, self.ext = os.path.splitext(name)
        self.tissue_seg_path = tissue_seg_path
        self.custom_mpp_keys = custom_mpp_keys

        self.width, self.height = None, None  # Placeholder dimensions
        self.mpp = mpp  # Placeholder microns per pixel. Defaults will be None unless specified in constructor. 
        self.mag = None  # Placeholder magnification
        self._is_initialized = False # Internal flag to track if _lazy_initialize has run
        self.max_workers = max_workers

        if not lazy_init: # User explicitly requested immediate initialization
            self._lazy_initialize()
        else: # Lazy init is True, so only attempt to load existing contours for quick check
            if self.tissue_seg_path is not None and os.path.exists(self.tissue_seg_path):
                try:
                    self.gdf_contours = gpd.read_file(self.tissue_seg_path)
                except Exception as e:
                    warnings.warn(f"Failed to load existing GeoJSON for {self.name}: {e}. Will re-segment if needed.")
                    self.gdf_contours = None
                    self.tissue_seg_path = None # Clear path if loading failed.
            else:
                self.gdf_contours = None
                self.tissue_seg_path = None # Ensure it's None if file doesn't exist

    def __repr__(self) -> str:
        if self._is_initialized:
            return f"<WSI: name={self.name}, width={self.width}, height={self.height}, backend={self.__class__.__name__}, mpp={self.mpp}, mag={self.mag}>"
        else:
            return f"<WSI: name={self.name}, path={self.slide_path}, (not initialized)>"
    
    def _lazy_initialize(self) -> None:
        """
        Perform lazy initialization of internal attributes for the WSI interface.

        This method is intended to be called by subclasses of `WSI`, and should not be used directly.
        It sets default values for key image attributes and optionally loads a tissue segmentation mask
        if a path is provided. Subclasses must override this method to implement backend-specific behavior.

        Raises
        ------
        FileNotFoundError
            If the tissue segmentation mask file is provided but cannot be found.

        Notes
        -----
        This method sets the following attributes:
        - `img`, `dimensions`, `width`, `height`: placeholder image properties (set to None).
        - `level_count`, `level_downsamples`, `level_dimensions`: multiresolution placeholders (None).
        - `properties`, `mag`: metadata and magnification (None).
        - `gdf_contours`: loaded from `tissue_seg_path` if available.
        """
        if self._is_initialized:
            return # Already initialized

        # Common initialization for all WSI types
        self.img = None # Backend-specific image object
        self.dimensions = None
        self.width, self.height = None, None
        self.level_count = None
        self.level_downsamples = None
        self.level_dimensions = None
        self.properties = None
        # self.mpp and self.mag are set by subclasses via _fetch_mpp and _fetch_magnification
        # only if they are not explicitly provided in the constructor.

        # Load existing GeoJSON contours if path is valid and not already loaded
        if self.tissue_seg_path is not None and os.path.exists(self.tissue_seg_path) and self.gdf_contours is None:
            try:
                self.gdf_contours = gpd.read_file(self.tissue_seg_path)
            except Exception as e:
                warnings.warn(f"Failed to load GeoJSON from {self.tissue_seg_path}: {e}. Segmenting will proceed without pre-loaded contours.")
                self.gdf_contours = None
                self.tissue_seg_path = None # Clear path if loading failed.
        elif self.tissue_seg_path is None or not os.path.exists(self.tissue_seg_path):
            self.gdf_contours = None # Ensure explicit None if path invalid/missing

        self._is_initialized = True # Mark as initialized

    def create_patcher(
        self, 
        patch_size: int, 
        src_pixel_size: Optional[float] = None, 
        dst_pixel_size: Optional[float] = None, 
        src_mag: Optional[int] = None, 
        dst_mag: Optional[int] = None, 
        overlap: int = 0, 
        mask: Optional[gpd.GeoDataFrame] = None,
        coords_only: bool = False, 
        custom_coords:  Optional[np.ndarray] = None,
        min_tissue_proportion: float = 0., # Changed from 'threshold'
        pil: bool = False,
    ) -> WSIPatcher:
        """
        Create a patcher object for extracting patches from the WSI.

        Args:
        -----
        patch_size : int
            Size of each patch in pixels for the output (at `dst_mag` or `dst_pixel_size`).
        src_pixel_size : float, optional
            Pixel size in um/px of the source WSI. If None, `self.mpp` is used.
        dst_pixel_size : float, optional
            Desired pixel size in um/px for the output patches. If None, `dst_mag` must be provided.
        src_mag : int, optional
            Magnification of the source WSI. If None, `self.mag` is used.
        dst_mag : int, optional
            Desired magnification for the output patches. If None, `dst_pixel_size` must be provided.
        overlap (int, optional): Overlap between patches in pixels (at the output resolution). Defaults to 0. 
        mask (gpd.GeoDataFrame, optional): geopandas dataframe of Polygons to filter patches. Defaults to None.
        coords_only (bool, optional): If True, the patcher yields only (x, y) coordinates instead of (patch, x, y). Default to False.
        custom_coords (np.ndarray, optional): Pre-defined (N, 2) array of (x, y) coordinates (at level 0) to extract. If provided, `mask`, grid generation, and `overlap` are ignored. Defaults to None.
        min_tissue_proportion: float, optional 
            Minimum proportion of the patch area (at `dst_mag`/`dst_pixel_size`) that must be tissue to be kept.
            Applied only if `mask` is provided. Between 0.0 and 1.0. Defaults to 0.0 (any tissue presence).
        pil (bool, optional): If True, patches are returned as `PIL.Image` objects. Otherwise, as NumPy arrays. Defaults to False.

        Returns:
        --------
        WSIPatcher:
            An object for extracting patches.
        """
        # Ensure WSI is initialized to access metadata like self.mpp, self.mag
        self._lazy_initialize()

        # Resolve src_pixel_size and src_mag if not provided
        if src_pixel_size is None:
            if self.mpp is None:
                raise ValueError("src_pixel_size or self.mpp must be available to create patcher.")
            src_pixel_size = self.mpp
        
        if src_mag is None:
            if self.mag is None:
                raise ValueError("src_mag or self.mag must be available to create patcher.")
            src_mag = self.mag

        return WSIPatcher(
            wsi=self, # Pass self (WSI instance)
            patch_size=patch_size,
            src_pixel_size=src_pixel_size,
            dst_pixel_size=dst_pixel_size,
            src_mag=src_mag,
            dst_mag=dst_mag,
            overlap=overlap,
            mask=mask,
            coords_only=coords_only,
            custom_coords=custom_coords,
            min_tissue_proportion=min_tissue_proportion,
            pil=pil,
        )
    
    def _fetch_magnification(self, custom_mpp_keys: Optional[List[str]] = None) -> Optional[int]:
        """
        The `_fetch_magnification` function of the class `WSI` calculates the magnification level 
        of the WSI based on the microns per pixel (MPP) value or other metadata. The magnification levels are 
        approximated to commonly used values such as 80x, 40x, 20x, etc. If the MPP is unavailable or insufficient 
        for calculation, it attempts to fallback to metadata-based values.

        Args:
        -----
        custom_mpp_keys : Optional[List[str]], optional
            Custom keys to search for MPP values in the WSI properties. Defaults to None.

        Returns:
        --------
        Optional[int]]:
            The approximated magnification level, or None if the magnification could not be determined.

        Raises:
        -------
        ValueError:
            If the identified MPP is too low for valid magnification values.
        """
        if self.mpp is None: # Attempt to get MPP from backend if not set by constructor
            try:
                mpp_x = self._fetch_mpp(custom_mpp_keys)
            except ValueError: # If backend cannot provide MPP, then cannot infer magnification from MPP
                mpp_x = None
        else: # MPP already provided in constructor
            mpp_x = self.mpp

        if mpp_x is not None:
            # Approximate standard magnifications based on common MPP values (e.g., 40x is ~0.25 MPP)
            # These are inverse relationships: higher mag -> lower mpp
            if mpp_x < 0.16: return 80
            elif mpp_x < 0.2: return 60
            elif mpp_x < 0.3: return 40
            elif mpp_x < 0.6: return 20
            elif mpp_x < 1.2: return 10
            elif mpp_x < 2.4: return 5
            else:
                warnings.warn(f"Identified mpp ({mpp_x}) is unusually high. It may indicate a very low magnification or an error in MPP metadata.")
                return 1 # Fallback to 1x or lowest common mag
        
        return None # Could not infer magnification from MPP

    @torch.inference_mode()
    @torch.autocast(device_type="cuda", dtype=torch.float16) # Enable autocast for faster inference
    def segment_tissue(
        self,
        segmentation_model: SegmentationModel,
        target_mag: int = 10,
        holes_are_tissue: bool = True,
        job_dir: Optional[str] = None,
        batch_size: int = 16,
        device: str = 'cuda:0',
        verbose: bool = False
    ) -> str:
        """
        The `segment_tissue` function segments tissue regions in the WSI using 
        a specified segmentation model. It processes the WSI at a target magnification level, optionally 
        treating holes in the mask as tissue. The segmented regions are saved as thumbnails and GeoJSON contours.

        Args:
        -----
        segmentation_model : SegmentationModel (torch.nn.Module)
            The model used for tissue segmentation. Must be an instance of `SegmentationModel`
            (from `sauron.feature_extraction.models.segmentation.factory`).
        target_mag : int, optional
            Target magnification level for segmentation. Defaults to 10.
        holes_are_tissue : bool, optional
            Specifies whether to treat holes within tissue regions as part of the tissue. Defaults to True.
        job_dir :  Optional[str], optional
            Directory to save the segmentation results. Required.
        batch_size : int, optional
            Batch size for processing patches. Defaults to 16.
        device (str): 
            The computation device to use (e.g., 'cuda:0' for GPU or 'cpu' for CPU).
        verbose: bool, optional:
            Whether to print segmentation progress bar. Defaults to False.

        Returns:
        --------
        str:
            The absolute path to where the segmentation as GeoJSON is saved. 

        Raises:
        -------
        ValueError: If `job_dir` is None.
        """

        if job_dir is None:
            raise ValueError("`job_dir` must be provided to save segmentation results.")

        self._lazy_initialize() # Ensure WSI object is fully loaded

        # Move model to device and set to eval mode
        segmentation_model.to(device)
        segmentation_model.eval()

        # Determine thumbnail size for visualization (max 1000 pixels on longest side)
        max_dimension = 1000
        if self.width > self.height:
            thumbnail_width = max_dimension
            thumbnail_height = int(thumbnail_width * self.height / self.width)
        else:
            thumbnail_height = max_dimension
            thumbnail_width = int(thumbnail_height * self.width / self.height)
        thumbnail_pil = self.get_thumbnail((thumbnail_width, thumbnail_height))
        thumbnail_np = np.array(thumbnail_pil) # Convert to numpy for OpenCV

        # Get patch iterator for the segmentation model's target input resolution
        # Assumes segmentation_model.input_size and segmentation_model.target_mag are available
        destination_mpp = 10 / segmentation_model.target_mag # MPP at model's target magnification
        
        # Ensure self.mpp is available
        if self.mpp is None:
            raise ValueError(f"WSI {self.name} does not have MPP information. Cannot segment without it.")

        # Create patcher for segmentation
        # Patches are read at segmentation_model.target_mag, resized to segmentation_model.input_size
        patcher = self.create_patcher(
            patch_size = segmentation_model.input_size,
            src_pixel_size = self.mpp, # WSI's base MPP
            dst_pixel_size = destination_mpp, # Target MPP for input patches to model
            mask=None, # No mask applied during segmentation (segment whole image)
            min_tissue_proportion=0., # All patches are considered for segmentation
            pil=True # Model expects PIL images which are converted to tensor by transforms
        )
        
        precision = segmentation_model.precision
        eval_transforms = segmentation_model.eval_transforms
        dataset = WSIPatcherDataset(patcher, eval_transforms)
        # Use get_num_workers from utils.io
        dataloader = DataLoader(dataset, batch_size=batch_size, num_workers=get_num_workers(batch_size, max_workers=self.max_workers), pin_memory=True)

        # Calculate mask dimensions at segmentation target magnification
        mpp_reduction_factor = self.mpp / destination_mpp
        mask_width_at_target_mpp, mask_height_at_target_mpp = int(round(self.width * mpp_reduction_factor)), int(round(self.height * mpp_reduction_factor))
        
        # Initialize an empty mask to stitch predictions
        stitched_mask = np.zeros((mask_height_at_target_mpp, mask_width_at_target_mpp), dtype=np.uint8)

        dataloader = tqdm(dataloader, desc=f"Segmenting {self.name}", disable=not verbose)

        for imgs, (xcoords_level0, ycoords_level0) in dataloader:
            imgs = imgs.to(device, dtype=precision)  # Move to device and match dtype
            
            with torch.autocast(device_type=device.split(":")[0], dtype=precision, enabled=(precision != torch.float32)):
                preds = segmentation_model(imgs).cpu().numpy() # Raw predictions from model

            # Convert level 0 coords to coords at segmentation target MPP
            x_starts_target_mpp = np.clip(np.round(xcoords_level0.numpy() * mpp_reduction_factor).astype(int), 0, mask_width_at_target_mpp - 1)
            y_starts_target_mpp = np.clip(np.round(ycoords_level0.numpy() * mpp_reduction_factor).astype(int), 0, mask_height_at_target_mpp - 1)
            
            patch_input_size = segmentation_model.input_size # Size of input patches to the model (e.g. 512)

            for i in range(len(preds)):
                x_start, y_start = x_starts_target_mpp[i], y_starts_target_mpp[i]
                
                # Calculate end coordinates, clipping to mask boundaries
                x_end = min(x_start + patch_input_size, mask_width_at_target_mpp)
                y_end = min(y_start + patch_input_size, mask_height_at_target_mpp)
                
                if x_start >= x_end or y_start >= y_end: # Invalid patch after clipping, skip
                    continue

                # Ensure patch_pred matches the actual clipped size for slicing
                patch_pred = preds[i][:y_end - y_start, :x_end - x_start]
                stitched_mask[y_start:y_end, x_start:x_end] += patch_pred
        
        # Post-process the stitched mask: binarize (anything > 0 is tissue) and convert to 255 for contours
        binary_mask_for_contours = (stitched_mask > 0).astype(np.uint8) * 255

        # Define save paths
        thumbnail_saveto = os.path.join(job_dir, 'thumbnails', f'{self.name}.jpg')
        os.makedirs(os.path.dirname(thumbnail_saveto), exist_ok=True)
        thumbnail_pil.save(thumbnail_saveto) # Save the original thumbnail

        gdf_saveto = os.path.join(job_dir, 'contours_geojson', f'{self.name}.geojson')
        os.makedirs(os.path.dirname(gdf_saveto), exist_ok=True)

        # Convert the binary mask (at `target_mag` resolution) to GeoDataFrame contours
        # `pixel_size` passed to `mask_to_gdf` is the original WSI's MPP for area calculations.
        # `contour_scale` is used to scale the contours back to level 0 coordinates if the mask was generated at a lower resolution than level 0.
        # Here, `contour_scale` is 1 / mpp_reduction_factor, converting coordinates from the `target_mag` resolution back to level 0 pixels.
        gdf_contours = mask_to_gdf(
            mask=binary_mask_for_contours,
            max_nb_holes=0 if holes_are_tissue else segmentation_model.max_holes_to_fill, # Default: 20 in trident. Max_nb_holes (int)
            min_contour_area=1000, # Default: 1000. Area in pixels at the mask generation resolution
            pixel_size=self.mpp, # Use original WSI MPP for consistency of area units (um^2)
            contour_scale=1/mpp_reduction_factor # Scale GeoJSON coords back to level 0 (full resolution)
        )
        gdf_contours.to_file(gdf_saveto, driver="GeoJSON")
        self.gdf_contours = gdf_contours # Update WSI object's contours
        self.tissue_seg_path = gdf_saveto # Update WSI object's tissue_seg_path

        # Draw the contours on the thumbnail image
        contours_saveto = os.path.join(job_dir, 'contours', f'{self.name}.jpg')
        # Use the already loaded thumbnail_np (which is RGB)
        # Scale for overlay_gdf_on_thumbnail is from GeoJSON Level0 coords to thumbnail pixels.
        # So scale = thumbnail_width / self.width (or thumbnail_height / self.height)
        overlay_gdf_on_thumbnail(gdf_contours, thumbnail_np, contours_saveto, thumbnail_width / self.width)

        return gdf_saveto

    # These methods are abstract in the base class and implemented by subclasses (OpenSlideWSI, ImageWSI, CuCIMWSI)
    @abstractmethod
    def _fetch_mpp(self, custom_mpp_keys: Optional[List[str]] = None) -> float:
        """Abstract method to fetch MPP from backend-specific properties."""
        pass

    @abstractmethod
    def get_dimensions(self) -> Tuple[int, int]:
        """Abstract method to get dimensions from backend."""
        pass

    @abstractmethod
    def get_thumbnail(self, size: Tuple[int, int]) -> Image.Image:
        """Abstract method to get a thumbnail from backend."""
        pass

    @abstractmethod
    def read_region(
        self,
        location: Tuple[int, int],
        level: int,
        size: Tuple[int, int],
        read_as: ReadMode = 'pil',
    ) -> Union[Image.Image, np.ndarray]:
        """Abstract method to read a region from backend."""
        pass

    @abstractmethod
    def level_dimensions(self) -> List[Tuple[int, int]]:
        """Abstract method to get all level dimensions from backend."""
        pass

    @abstractmethod
    def level_downsamples(self) -> List[float]:
        """Abstract method to get all level downsamples from backend."""
        pass

    @abstractmethod
    def get_best_level_and_custom_downsample(
        self,
        downsample: float,
        tolerance: float = 0.01
    ) -> Tuple[int, float]:
        """Abstract method to determine the best level and custom downsample."""
        pass
    
    @abstractmethod
    def close(self):
        """Abstract method to close the WSI object and release resources."""
        pass


    def extract_tissue_coords(
        self,
        target_mag: int,
        patch_size: int,
        save_coords: str,
        overlap: int = 0,
        min_tissue_proportion: float  = 0.,
    ) -> str:
        """
        The `extract_tissue_coords` function extracts patch coordinates 
        from tissue regions in the WSI. It generates coordinates of patches at the specified 
        magnification and saves the results in an HDF5 file.

        Args:
        -----
        target_mag : int
            Target magnification level for the patches.
        patch_size : int
            Size of each patch at the target magnification.
        save_coords : str
            Directory path to save the extracted coordinates.
        overlap : int, optional
            Overlap between patches in pixels. Defaults to 0.
        min_tissue_proportion: float, optional 
            Minimum proportion of the patch under tissue to be kept. Defaults to 0. 

        Returns:
        --------
        str:
            The absolute file path to the saved HDF5 file containing the patch coordinates.
        """

        self._lazy_initialize() # Ensure WSI object is fully loaded

        # Ensure self.mpp and self.mag are available
        if self.mpp is None or self.mag is None:
            raise ValueError(f"WSI {self.name} does not have MPP or magnification information. Cannot extract tissue coordinates.")

        patcher = self.create_patcher(
            patch_size=patch_size,
            src_mag=self.mag, # Use WSI's inferred or provided base magnification
            dst_mag=target_mag,
            mask=self.gdf_contours, # Use the loaded or newly segmented contours
            coords_only=True,
            overlap=overlap,
            min_tissue_proportion=min_tissue_proportion,
        )

        coords_to_keep = np.array([(x, y) for x, y in patcher])
        
        if len(coords_to_keep) == 0:
            warnings.warn(f"No patches found for {self.name} after filtering. Coordinates file will be empty.")
            # For empty patches, return an empty array with appropriate shape
            coords_to_keep = np.empty((0, 2), dtype=np.int32)


        # Prepare assets for saving
        # patch_size_level0 is the size of the patch in pixels at Level 0 of the WSI
        patch_size_level0 = int(patch_size * self.mag / target_mag) 

        assets = {'coords' : coords_to_keep}
        attributes = {
            'patch_size': patch_size, # Size of patch at target_mag
            'patch_size_level0': patch_size_level0, # Size of patch at level 0
            'level0_magnification': self.mag,
            'target_magnification': target_mag,
            'overlap': overlap,
            'name': self.name,
            'savetodir': save_coords # This attribute is for context, not a path
        }

        # Save the assets and attributes to an hdf5 file
        out_fname = os.path.join(save_coords, 'patches', str(self.name) + '_patches.h5')
        save_h5(out_fname,
                assets = assets,
                attributes = {'coords': attributes},
                mode='w')
        
        return out_fname

    def visualize_coords(self, coords_path: str, save_patch_viz: str) -> str:
        """
        The `visualize_coords` function overlays patch coordinates computed by the WSIPatcher
        onto a scaled thumbnail of the WSI. It creates a visualization of the extracted patches 
        and saves it as an image file.

        Args:
        -----
        coords_path : str
            Path to the file containing the patch coordinates.
        save_patch_viz : str
            Directory path to save the visualization image.

        Returns:
        --------
        str:
            The file path to the saved visualization image.
        """

        self._lazy_initialize() # Ensure WSI object is fully loaded

        try:
            coords_attrs, coords = read_coords(coords_path)  # Coords are ALWAYS wrt. level 0 of the slide.
            patch_size = coords_attrs.get('patch_size')
            level0_magnification = coords_attrs.get('level0_magnification')
            target_magnification = coords_attrs.get('target_magnification')
            
            if any(val is None for val in [patch_size, level0_magnification, target_magnification]):
                raise KeyError('Missing essential attributes in coords_attrs (patch_size, level0_magnification, target_magnification).')
        except (KeyError, FileNotFoundError, ValueError) as e:
            warnings.warn(f"Cannot read using new Sauron coords format ({str(e)}). Trying with legacy CLAM/Fishing-Rod format.")
            # Fallback to legacy format
            try:
                patch_size, patch_level, custom_downsample, coords = read_coords_legacy(coords_path)
                level0_magnification = self.mag
                if self.mag is None or not self.level_downsamples:
                    raise ValueError("WSI object's magnification or level_downsamples not initialized for legacy coords.")
                target_magnification = int(self.mag / (self.level_downsamples[patch_level] * custom_downsample))
            except Exception as legacy_e:
                raise RuntimeError(f"Failed to read coordinates from {coords_path} in both new and legacy formats: {legacy_e}") from legacy_e


        patcher = self.create_patcher(
            patch_size=patch_size, # This is the size at target_magnification
            src_mag=level0_magnification, # This is the base magnification of the coordinates
            dst_mag=target_magnification, # This is the magnification at which patch_size is defined
            custom_coords=coords, # Use the loaded coordinates
            coords_only=True, # For visualization, we just need coordinates, not image data
            mask=self.gdf_contours # Pass existing contours for drawing visualization
        )

        img_pil =  patcher.visualize() # This method in WSIPatcher returns PIL Image

        # Save visualization
        os.makedirs(save_patch_viz, exist_ok=True)
        viz_coords_path = os.path.join(save_patch_viz, f'{self.name}.jpg')
        img_pil.save(viz_coords_path)
        return viz_coords_path

    @torch.inference_mode()
    def extract_patch_features(
        self,
        patch_encoder: BasePatchEncoder, # Use the new base class for type hinting
        coords_path: str,
        save_features: str,
        device: str = 'cuda:0',
        saveas: str = 'h5',
        batch_limit: int = 512
    ) -> str:
        """
        The `extract_patch_features` function extracts feature embeddings 
        from the WSI using a specified patch encoder. It processes the patches as specified 
        in the coordinates file and saves the features in the desired format.

        Args:
        -----
        patch_encoder : BasePatchEncoder (torch.nn.Module)
            The model used for feature extraction. Must be an instance of `BasePatchEncoder`
            (from `sauron.feature_extraction.models.patch_encoders.factory`).
        coords_path : str
            Path to the file containing patch coordinates.
        save_features : str
            Directory path to save the extracted features.
        device : str, optional
            Device to run feature extraction on (e.g., 'cuda:0'). Defaults to 'cuda:0'.
        saveas : str, optional
            Format to save the features ('h5' or 'pt'). Defaults to 'h5'.
        batch_limit : int, optional
            Maximum batch size for feature extraction. Defaults to 512.

        Returns:
        --------
        str:
            The absolute file path to the saved feature file in the specified format.
        """

        self._lazy_initialize() # Ensure WSI object is fully loaded
        
        # Ensure patch_encoder is on correct device and in eval mode
        patch_encoder.to(device)
        patch_encoder.eval()
        
        precision = getattr(patch_encoder, 'precision', torch.float32) # Get precision from encoder, default to float32
        patch_transforms = patch_encoder.eval_transforms

        # Read coordinates and their attributes
        try:
            coords_attrs, coords = read_coords(coords_path)
            patch_size = coords_attrs.get('patch_size')
            level0_magnification = coords_attrs.get('level0_magnification')
            target_magnification = coords_attrs.get('target_magnification')            
            if any(val is None for val in [patch_size, level0_magnification, target_magnification]):
                raise KeyError('Missing essential attributes in coords_attrs (patch_size, level0_magnification, target_magnification).')
        except (KeyError, FileNotFoundError, ValueError) as e:
            warnings.warn(f"Cannot read using new Sauron coords format ({str(e)}). Trying with legacy CLAM/Fishing-Rod format.")
            # Fallback to legacy format
            try:
                patch_size, patch_level, custom_downsample, coords = read_coords_legacy(coords_path)
                level0_magnification = self.mag # Use WSI's native mag for level0_magnification
                if self.mag is None or not self.level_downsamples:
                    raise ValueError("WSI object's magnification or level_downsamples not initialized for legacy coords.")
                target_magnification = int(self.mag / (self.level_downsamples[patch_level] * custom_downsample))
            except Exception as legacy_e:
                raise RuntimeError(f"Failed to read coordinates from {coords_path} in both new and legacy formats: {legacy_e}") from legacy_e

        # Create patcher for feature extraction
        patcher = self.create_patcher(
            patch_size=patch_size, # This is the size at target_magnification
            src_mag=level0_magnification, # Base magnification of the coordinates
            dst_mag=target_magnification, # Magnification at which patches are effectively extracted/resized
            custom_coords=coords, # Use the loaded coordinates
            coords_only=False, # We need the actual image data
            pil=True, # Patches as PIL Image to be processed by torchvision transforms
            mask=self.gdf_contours # For consistent behavior and potential visualization (though not used for filtering here)
        )
        
        dataset = WSIPatcherDataset(patcher, patch_transforms)
        dataloader = DataLoader(dataset, batch_size=batch_limit, num_workers=get_num_workers(batch_limit, max_workers=self.max_workers), pin_memory=True)

        features = []
        for imgs, _ in dataloader:
            imgs = imgs.to(device)
            with torch.autocast(device_type=device.split(":")[0], dtype=precision, enabled=(precision != torch.float32)):
                batch_features = patch_encoder(imgs)  
            features.append(batch_features.cpu().numpy())

        # Concatenate features
        if not features:
            # Handle case where no patches were processed (e.g., empty slide after filtering)
            warnings.warn(f"No features extracted for {self.name}. Returning empty feature array.")
            feature_dim = getattr(patch_encoder, 'embedding_dim', None)
            if feature_dim is None:
                # Attempt to get feature_dim from a dummy forward pass if not in attribute
                # This can be risky if model expects specific input shapes
                try:
                    dummy_input = patch_transforms(Image.new('RGB', (patch_size, patch_size))).unsqueeze(0).to(device)
                    feature_dim = patch_encoder(dummy_input).shape[-1]
                    warnings.warn(f"Inferred empty feature dim as {feature_dim} for {self.name}")
                except Exception:
                    feature_dim = 0 # Cannot infer, return 0
            features_np = np.empty((0, feature_dim), dtype=np.float32)
        else:
            features_np = np.concatenate(features, axis=0)

        # Save the features to disk
        os.makedirs(save_features, exist_ok=True)
        out_fp = os.path.join(save_features, f'{self.name}.{saveas}')

        if saveas == 'h5':
            save_h5(out_fp,
                    assets = {
                        'features' : features_np,
                        'coords': coords, # Save original level 0 coordinates
                    },
                    attributes = {
                        'features': {'name': self.name, 'savetodir': save_features},
                        'coords': coords_attrs # Save coordinate attributes
                    },
                    mode='w')
        elif saveas == 'pt':
            torch.save({'features': torch.from_numpy(features_np), 'coords': torch.from_numpy(coords), 'coords_attrs': coords_attrs}, out_fp)
        else:
            raise ValueError(f'Invalid save_features_as: {saveas}. Only "h5" and "pt" are supported.')

        return out_fp

    @torch.inference_mode()
    def extract_slide_features(
        self,
        patch_features_path: str, # Path to the H5/PT file containing patch features and coords
        slide_encoder: BaseSlideEncoder, # Use the new base class for type hinting
        save_features: str, # Directory to save the final slide-level features
        device: str = 'cuda',
    ) -> str:
        """
        Extract slide-level features by encoding patch-level features using a pretrained slide encoder.

        This function processes patch-level features extracted from a whole-slide image (WSI) and
        generates a single feature vector representing the entire slide. The extracted features are
        saved to a specified directory in HDF5 format.

        Args:
            patch_features_path (str): Path to the H5/PT file containing patch-level features and coordinates.
            slide_encoder (BaseSlideEncoder): Pretrained slide encoder model for generating slide-level features.
            save_features (str): Directory where the extracted slide features will be saved.
            device (str, optional): Device to run computations on (e.g., 'cuda', 'cpu'). Defaults to 'cuda'.

        Returns:
            str: The absolute path to the slide-level features.

        Workflow:
            1. Load the pretrained slide encoder model and set it to evaluation mode.
            2. Load patch-level features and corresponding coordinates from the provided HDF5 file.
            3. Convert loaded data into tensors and move to the specified device.
            4. Generate slide-level features using the slide encoder.
            5. Save the slide-level features and associated metadata (e.g., original patch coordinates) in an HDF5 file.
            6. Return the path to the saved slide features.

        Notes:
            - The `patch_features_path` must point to a valid H5 or PT file containing datasets named `features` and `coords`.
            - The saved HDF5 file includes both the slide-level features and metadata such as patch coordinates.
            - Automatic mixed precision is enabled if the slide encoder supports precision lower than `torch.float32`.
        """
        import h5py

        # Set the slide encoder model to device and eval
        slide_encoder.to(device)
        slide_encoder.eval()
        
        # Determine file type and load accordingly
        file_extension = os.path.splitext(patch_features_path)[1].lower()
        if file_extension == '.h5':
            with h5py.File(patch_features_path, 'r') as f:
                coords = f['coords'][:]
                patch_features = f['features'][:]
                coords_attrs = dict(f['coords'].attrs)
        elif file_extension == '.pt':
            loaded_data = torch.load(patch_features_path, map_location='cpu')
            patch_features = loaded_data['features'].numpy()
            coords = loaded_data['coords'].numpy()
            coords_attrs = loaded_data.get('coords_attrs', {}) # Get attributes if saved
        else:
            raise ValueError(f"Unsupported patch feature file format: {file_extension}. Only .h5 and .pt are supported.")

        # Convert to tensors for model input. Add batch dimension (B=1 for single slide)
        patch_features_tensor = torch.from_numpy(patch_features).float().unsqueeze(0).to(device)
        coords_tensor = torch.from_numpy(coords).to(device)
        if coords_tensor.dtype == torch.float64: # Ensure coords are not float64 if not needed
             coords_tensor = coords_tensor.float()
        coords_tensor = coords_tensor.unsqueeze(0) # Add batch dimension

        # Prepare input batch dictionary
        batch = {
            'features': patch_features_tensor,
            'coords': coords_tensor,
            'attributes': coords_attrs # Pass coordinate attributes, as some slide encoders need them (e.g., GigaPath, Titan)
        }

        # Generate slide-level features with autocast
        with torch.autocast(device_type=device.split(":")[0], dtype=slide_encoder.precision, enabled=(slide_encoder.precision != torch.float32)):
            features = slide_encoder(batch, device) # Call the slide encoder's forward method
        
        # Squeeze batch dim and convert to numpy
        features_np = features.float().cpu().numpy().squeeze() # Ensure float32 for numpy and remove batch dim if present

        # Save slide-level features
        os.makedirs(save_features, exist_ok=True)
        save_path = os.path.join(save_features, f'{self.name}.h5') # Always save slide features as h5

        save_h5(save_path,
                assets = {
                    'features' : features_np,
                    'coords': coords, # Keep original patch coords for traceability
                },
                attributes = {
                    'features': {'name': self.name, 'savetodir': save_features, 'encoder_name': slide_encoder.enc_name, 'embedding_dim': slide_encoder.embedding_dim},
                    'coords': coords_attrs # Save original coordinate attributes
                },
                mode='w')

        return save_path

    def release(self) -> None:
        """
        Release internal data (CPU/GPU/memory) and clear heavy references in the WSI instance.
        Call this method after you're done processing to avoid memory/GPU leaks.
        """
        # Clear backend image object

        if hasattr(self, "close"):
            self.close() # Calls the backend-specific close method

        # Clear generic image object reference if exists
        if hasattr(self, "img"):
            self.img = None

        # Clear segmentation results and coordinates
        for attr in ["gdf_contours", "tissue_seg_path"]:
            if hasattr(self, attr):
                setattr(self, attr, None)

        import gc
        import torch
        gc.collect()
        if torch.cuda.is_available():
            torch.cuda.empty_cache()

```

```python
# sauron/feature_extraction/wsi/openslide.py
# New file, adapted from trident/wsi_objects/OpenSlideWSI.py

from __future__ import annotations
import numpy as np
import openslide
from PIL import Image
from typing import List, Tuple, Union, Optional

from sauron.feature_extraction.wsi.base import WSI, ReadMode
from sauron.feature_extraction.wsi.patching import OpenSlideWSIPatcher # For type hinting


class OpenSlideWSI(WSI):

    def __init__(self, image_source: openslide.OpenSlide, **kwargs) -> None:
        """
        Initialize an OpenSlideWSI instance.

        Parameters
        ----------
        image_source : openslide.OpenSlide
            An already opened OpenSlide object.
        **kwargs : dict
            Keyword arguments forwarded to the base `WSI` class. Most important key is:
            - slide_path (str): Path to the WSI.
            - lazy_init (bool, default=True): Whether to defer loading WSI and metadata.

        Please refer to WSI constructor for all parameters. 
        """
        self.img = image_source # Store the OpenSlide object directly
        super().__init__(**kwargs) # Call base WSI init, will trigger _lazy_initialize if lazy_init=False

    def _lazy_initialize(self) -> None:
        """
        Lazily initialize the WSI using OpenSlide.

        This method opens a whole-slide image using the OpenSlide backend, extracting
        key metadata including dimensions, magnification, and multiresolution pyramid
        information. If a tissue segmentation mask is provided, it is also loaded.

        Raises
        ------
        FileNotFoundError
            If the WSI file or the tissue segmentation mask cannot be found.
        Exception
            If an unexpected error occurs during WSI initialization.

        Notes
        -----
        After initialization, the following attributes are set:
        - `width` and `height`: spatial dimensions of the base level.
        - `dimensions`: (width, height) tuple from the highest resolution.
        - `level_count`: number of resolution levels in the image pyramid.
        - `level_downsamples`: downsampling factors for each level.
        - `level_dimensions`: image dimensions at each level.
        - `properties`: metadata dictionary from OpenSlide.
        - `mpp`: microns per pixel, inferred if not manually specified.
        - `mag`: estimated magnification level.
        - `gdf_contours`: loaded from `tissue_seg_path` if provided.
        """

        super()._lazy_initialize() # Call base class lazy init

        if not self._is_initialized: # Only proceed if base init didn't already mark as initialized
            try:
                # self.img should already be set from __init__ for OpenSlideWSI
                # If not provided, it means this WSI object was created via `load_wsi(path)`
                # which would provide the OpenSlide object to __init__.
                if self.img is None:
                    self.img = openslide.OpenSlide(self.slide_path)

                # set openslide attrs as self
                self.dimensions = self.get_dimensions()
                self.width, self.height = self.dimensions
                self.level_count = self.img.level_count
                self.level_downsamples = self.img.level_downsamples
                self.level_dimensions = self.img.level_dimensions
                self.properties = self.img.properties

                # Fetch MPP and Magnification only if not already provided in constructor
                if self.mpp is None:
                    self.mpp = self._fetch_mpp(self.custom_mpp_keys)
                if self.mag is None:
                    self.mag = self._fetch_magnification(self.custom_mpp_keys)
                
                self._is_initialized = True # Mark as fully initialized
            except openslide.OpenSlideError as ose:
                raise RuntimeError(f"Failed to open WSI with OpenSlide: {self.slide_path}. Error: {ose}") from ose
            except Exception as e:
                raise RuntimeError(f"Failed to initialize WSI with OpenSlide: {self.slide_path}. Error: {e}") from e

    def _fetch_mpp(self, custom_mpp_keys: Optional[List[str]] = None) -> float:
        """
        Retrieve microns per pixel (MPP) from OpenSlide metadata.

        Parameters
        ----------
        custom_mpp_keys : list of str, optional
            Additional metadata keys to check for MPP.

        Returns
        -------
        float
            MPP value in microns per pixel.

        Raises
        ------
        ValueError
            If MPP cannot be determined from metadata.
        """
        self._lazy_initialize() # Ensure img and properties are loaded

        mpp_keys = [
            openslide.PROPERTY_NAME_MPP_X, # Standard OpenSlide MPP key for X (also for Y)
            'openslide.mirax.MPP', # Mirax specific
            'aperio.MPP', # Aperio specific
            'hamamatsu.XResolution', # Hamamatsu specific, needs conversion
            'openslide.comment', # Sometimes embedded in comments
        ]

        if custom_mpp_keys:
            mpp_keys.extend(custom_mpp_keys)

        for key in mpp_keys:
            if key in self.properties:
                try:
                    mpp_x = float(self.properties[key])
                    if mpp_x > 0: # Ensure MPP is positive
                        return round(mpp_x, 4)
                except ValueError:
                    continue # Try next key

        # Fallback for TIFF resolution properties if MPP keys don't work
        x_resolution = self.properties.get('tiff.XResolution')
        unit = self.properties.get('tiff.ResolutionUnit')

        if x_resolution and unit:
            try:
                x_res_val = float(x_resolution)
                if x_res_val > 0:
                    if unit.lower() == 'centimeter':
                        return round(10000 / x_res_val, 4) # 10000 um/cm
                    elif unit.lower() == 'inch':
                        return round(25400 / x_res_val, 4) # 25400 um/inch
            except ValueError:
                pass

        raise ValueError(
            f"Unable to extract MPP from slide metadata for: '{self.slide_path}'.\n"
            "Suggestions:\n"
            "- Provide `custom_mpp_keys` to specify metadata keys to look for.\n"
            "- Set the MPP explicitly via the class constructor (`mpp` argument).\n"
        )

    def _fetch_magnification(self, custom_mpp_keys: Optional[List[str]] = None) -> int:
        """
        Retrieve estimated magnification from metadata or calculated from MPP.

        Parameters
        ----------
        custom_mpp_keys : list of str, optional
            Keys to aid in computing magnification from MPP (passed to _fetch_mpp).

        Returns
        -------
        int
            Estimated magnification.

        Raises
        ------
        ValueError
            If magnification cannot be determined.
        """
        self._lazy_initialize() # Ensure img and properties are loaded

        # First try to get from OpenSlide's OBJECTIVE_POWER property
        metadata_mag = self.properties.get(openslide.PROPERTY_NAME_OBJECTIVE_POWER)
        if metadata_mag is not None:
            try:
                mag_val = int(float(metadata_mag)) # Cast to float first, then int, for robustness
                if mag_val > 0:
                    return mag_val
            except ValueError:
                pass # If it's not a valid number, continue to next method

        # Then try to infer from MPP using base WSI's helper, if MPP is available
        inferred_mag_from_mpp = super()._fetch_magnification(custom_mpp_keys)
        if inferred_mag_from_mpp is not None:
            return inferred_mag_from_mpp

        raise ValueError(f"Unable to determine magnification from metadata for: {self.slide_path}")

    def read_region(
        self,
        location: Tuple[int, int],
        level: int,
        size: Tuple[int, int],
        read_as: ReadMode = 'pil',
    ) -> Union[Image.Image, np.ndarray]:
        """
        Extract a specific region from the whole-slide image (WSI).

        Parameters
        ----------
        location : Tuple[int, int]
            (x, y) coordinates of the top-left corner of the region to extract at level 0.
        level : int
            Pyramid level to read from.
        size : Tuple[int, int]
            (width, height) of the region to extract at the specified level.
        read_as : {'pil', 'numpy'}, optional
            Output format for the region:
            - 'pil': returns a PIL Image (default)
            - 'numpy': returns a NumPy array (H, W, 3)

        Returns
        -------
        Union[PIL.Image.Image, np.ndarray]
            Extracted image region in the specified format.

        Raises
        ------
        ValueError
            If `read_as` is not one of 'pil' or 'numpy'.
        """
        self._lazy_initialize() # Ensure img is loaded

        # openslide.OpenSlide.read_region returns PIL.Image
        region_pil = self.img.read_region(location, level, size).convert('RGB')

        if read_as == 'pil':
            return region_pil
        elif read_as == 'numpy':
            return np.array(region_pil)
        else:
            raise ValueError(f"Invalid `read_as` value: {read_as}. Must be 'pil' or 'numpy'.")

    def get_dimensions(self) -> Tuple[int, int]:
        """
        Return the dimensions (width, height) of the WSI at level 0.

        Returns
        -------
        tuple of int
            (width, height) in pixels.
        """
        self._lazy_initialize() # Ensure img is loaded
        return self.img.dimensions

    def get_thumbnail(self, size: tuple[int, int]) -> Image.Image:
        """
        Generate a thumbnail of the WSI.

        Parameters
        ----------
        size : tuple of int
            Desired (width, height) of the thumbnail.

        Returns
        -------
        PIL.Image.Image
            RGB thumbnail as a PIL Image.
        """
        self._lazy_initialize() # Ensure img is loaded
        return self.img.get_thumbnail(size).convert('RGB')

    def level_dimensions(self) -> List[Tuple[int, int]]:
        """
        Gets the dimensions of each level in the WSI.

        Returns
        -------
        List[Tuple[int, int]]: A list of (width, height) tuples for each level.
        """
        self._lazy_initialize() # Ensure img is loaded
        return self.img.level_dimensions

    def level_downsamples(self) -> List[float]:
        """
        Gets the downsample factor for each level in the WSI.

        Returns
        -------
        List[float]: A list of downsample factors for each level relative to level 0.
        """
        self._lazy_initialize() # Ensure img is loaded
        return self.img.level_downsamples

    def get_best_level_and_custom_downsample(
        self,
        downsample: float,
        tolerance: float = 0.01
    ) -> Tuple[int, float]:
        """
        Determines the best OpenSlide level and custom downsample factor to approximate a desired downsample value.

        Args:
            downsample (float): The desired total downsample factor relative to level 0.
            tolerance (float, optional): Tolerance for matching existing downsample levels. Defaults to 0.01.

        Returns:
            Tuple[int, float]: A tuple containing the best level index and the additional
                               custom downsample factor needed at that level.
        Raises:
            ValueError: If no suitable level can be found (should not happen for valid OpenSlide).
        """
        self._lazy_initialize() # Ensure img is loaded

        # OpenSlide's get_best_level_for_downsample returns the level with the highest resolution
        # that is greater than or equal to the desired downsample factor.
        best_level_index = self.img.get_best_level_for_downsample(downsample)
        
        # The actual downsample at that level
        actual_level_downsample = self.img.level_downsamples[best_level_index]

        # Calculate the additional custom_downsample factor needed
        # If actual_level_downsample is exactly `downsample`, custom_downsample will be 1.0.
        # If `downsample` is higher (more zoomed in) than `actual_level_downsample`,
        # then we need to upsample, so `custom_downsample` will be > 1.0.
        # If `downsample` is lower (more zoomed out) than `actual_level_downsample`,
        # then we need to downsample further, so `custom_downsample` will be < 1.0.
        
        if actual_level_downsample > 0: # Avoid division by zero
            custom_downsample = downsample / actual_level_downsample
        else:
            raise ValueError(f"Invalid level downsample factor {actual_level_downsample} at level {best_level_index} for slide {self.name}.")

        return best_level_index, custom_downsample

    def close(self):
        """
        Close the OpenSlide object and release its resources.
        """
        if self.img is not None:
            self.img.close()
            self.img = None
            self._is_initialized = False # Mark as uninitialized

```

```python
# sauron/feature_extraction/wsi/image.py
# New file, adapted from trident/wsi_objects/ImageWSI.py

from __future__ import annotations
import numpy as np
from PIL import Image
from typing import Tuple, Union, Optional

from sauron.feature_extraction.wsi.base import WSI, ReadMode
from sauron.feature_extraction.wsi.patching import NumpyWSIPatcher # For type hinting


class ImageWSI(WSI):

    def __init__(self, image_source: Image.Image, **kwargs) -> None:
        """
        Initialize a WSI object from a standard PIL Image file (e.g., PNG, JPEG, etc.).

        Parameters
        ----------
        image_source : PIL.Image.Image
            An already opened PIL Image object.
        mpp : float
            Microns per pixel. Required since standard image formats do not store this metadata.
        **kwargs: other arguments to pass to base WSI constructor.
            - name : str, optional (inherited)
            - lazy_init : bool, default=True (inherited)

        Raises
        ------
        ValueError
            If the required 'mpp' argument is not provided.
        """
        mpp = kwargs.get("mpp")
        if mpp is None:
            raise ValueError(
                "Missing required argument `mpp`. Standard image formats (PNG, JPEG etc.) do not contain microns-per-pixel "
                "information, so you must specify it manually via the `ImageWSI` constructor's `mpp` argument (or in CSV for Processor)."
            )
        
        # Enable loading large images if MAX_IMAGE_PIXELS is set for PIL
        # This is usually set globally, but ensuring it here.
        Image.MAX_IMAGE_PIXELS = None # Disables decompression bomb limit if not set globally

        self.img = image_source # Store the PIL Image object directly
        super().__init__(**kwargs) # Call base WSI init, will trigger _lazy_initialize if lazy_init=False

    def _lazy_initialize(self) -> None:
        """
        Lazily initialize the WSI using a standard PIL Image object.

        This method loads the image using PIL and extracts relevant metadata such as
        dimensions and magnification. It assumes a single-resolution image (no pyramid).
        If a tissue segmentation mask is available, it is also loaded.

        Raises
        ------
        FileNotFoundError
            If the WSI file (if re-opened) or tissue segmentation mask is not found.
        Exception
            If an unexpected error occurs during initialization.

        Notes
        -----
        After initialization, the following attributes are set:
        - `width` and `height`: dimensions of the image.
        - `dimensions`: (width, height) tuple of the image.
        - `level_downsamples`: set to `[1]` (single resolution).
        - `level_dimensions`: set to a list containing the image dimensions.
        - `level_count`: set to `1`.
        - `mag`: estimated magnification level.
        - `gdf_contours`: loaded from `tissue_seg_path`, if available.
        """

        super()._lazy_initialize() # Call base class lazy init

        if not self._is_initialized: # Only proceed if base init didn't already mark as initialized
            try:
                # self.img should already be set from __init__ for ImageWSI
                # If not provided, it means this WSI object was created via `load_wsi(path)`
                # which would provide the PIL Image object to __init__.
                if self.img is None:
                    # If somehow self.img is None, try to re-open the file.
                    # This might happen if `ImageWSI` was initialized lazily and then later closed.
                    self.img = Image.open(self.slide_path).convert("RGB")
                
                self.level_downsamples = [1.0] # Single level image
                self.level_dimensions = [(self.img.width, self.img.height)] # Store actual dimensions
                self.level_count = 1
                
                self.dimensions = self.get_dimensions() # Get actual dimensions
                self.width, self.height = self.dimensions[0], self.dimensions[1]

                # Fetch MPP and Magnification only if not already provided in constructor
                # For ImageWSI, MPP *must* be provided in constructor, it cannot be fetched
                if self.mpp is None:
                    raise ValueError(f"MPP for {self.name} is not available. ImageWSI requires MPP to be passed during initialization.")
                # Magnification is inferred from the provided MPP
                if self.mag is None:
                    self.mag = self._fetch_magnification(self.custom_mpp_keys)
                
                self._is_initialized = True # Mark as fully initialized
            except Exception as e:
                raise RuntimeError(f"Error initializing WSI with PIL.Image backend: {self.slide_path}. Error: {e}") from e

    def _fetch_mpp(self, custom_mpp_keys: Optional[List[str]] = None) -> float:
        """
        For ImageWSI, MPP *must* be provided in the constructor. This method acts as a check.
        """
        if self.mpp is not None:
            return self.mpp
        else:
            # This should have been caught in __init__, but as a safeguard.
            raise ValueError(f"MPP not available for ImageWSI '{self.name}'. It must be provided during initialization.")

    def _fetch_magnification(self, custom_mpp_keys: Optional[List[str]] = None) -> int:
        """
        Retrieve estimated magnification from `self.mpp` (which must be set).
        """
        self._lazy_initialize() # Ensure MPP is available
        inferred_mag_from_mpp = super()._fetch_magnification(custom_mpp_keys)
        if inferred_mag_from_mpp is not None:
            return inferred_mag_from_mpp
        else:
            raise ValueError(f"Unable to determine magnification for ImageWSI '{self.name}'. MPP is available but could not infer common magnification.")


    def get_dimensions(self) -> Tuple[int, int]:
        """
        Return the dimensions (width, height) of the PIL Image.
        """
        self._lazy_initialize() # Ensure img is loaded
        return self.img.size

    def get_thumbnail(self, size: Tuple[int, int]) -> Image.Image:
        """
        Generate a thumbnail of the PIL Image.

        Parameters
        ----------
        size : tuple of int
            Desired thumbnail size (width, height).

        Returns
        -------
        PIL.Image.Image
            RGB thumbnail image.
        """
        self._lazy_initialize() # Ensure img is loaded
        img_copy = self.img.copy() # Work on a copy to avoid modifying original in place
        img_copy.thumbnail(size, Image.Resampling.LANCZOS) # Use LANCZOS for high-quality downsampling
        return img_copy.convert('RGB')

    def read_region(
        self,
        location: Tuple[int, int],
        level: int,
        size: Tuple[int, int],
        read_as: ReadMode = 'pil',
    ) -> Union[Image.Image, np.ndarray]:
        """
        Extract a specific region from a single-resolution image (e.g., JPEG, PNG, TIFF).

        Parameters
        ----------
        location : Tuple[int, int]
            (x, y) coordinates of the top-left corner of the region to extract (at level 0).
        level : int
            Pyramid level to read from. Only level 0 is supported for non-pyramidal images.
        size : Tuple[int, int]
            (width, height) of the region to extract.
        read_as : {'pil', 'numpy'}, optional
            Output format for the region:
            - 'pil': returns a PIL Image (default)
            - 'numpy': returns a NumPy array (H, W, 3)

        Returns
        -------
        Union[PIL.Image.Image, np.ndarray]
            Extracted image region in the specified format.

        Raises
        ------
        ValueError
            If `level` is not 0 or if `read_as` is not one of the supported options.
        """
        if level != 0:
            raise ValueError("ImageWSI only supports reading at level=0 (no pyramid levels).")

        self._lazy_initialize() # Ensure img is loaded
        
        # Ensure the region is within bounds to prevent PIL errors for out-of-bounds crops
        x, y = location
        w, h = size
        
        x_end = min(x + w, self.width)
        y_end = min(y + h, self.height)
        
        # Adjust width and height to actual cropped size if bounds were hit
        actual_w = x_end - x
        actual_h = y_end - y

        if actual_w <= 0 or actual_h <= 0:
            # If the crop region is invalid (e.g., completely outside), return a black image
            if read_as == 'pil':
                return Image.new('RGB', size, (0,0,0))
            else:
                return np.zeros((size[1], size[0], 3), dtype=np.uint8)


        region = self.img.crop((x, y, x_end, y_end)).convert('RGB')
        
        # If the extracted region is smaller than requested (due to image boundary), pad it.
        if region.width != w or region.height != h:
            padded_region = Image.new('RGB', (w, h), (0,0,0)) # Black padding
            padded_region.paste(region, (0,0))
            region = padded_region

        if read_as == 'pil':
            return region
        elif read_as == 'numpy':
            return np.array(region)
        else:
            raise ValueError(f"Invalid `read_as` value: {read_as}. Must be 'pil' or 'numpy'.")

    def level_dimensions(self) -> List[Tuple[int, int]]:
        """
        Gets the dimensions of each level in the WSI. For ImageWSI, it's always just level 0.
        """
        self._lazy_initialize()
        return self.level_dimensions

    def level_downsamples(self) -> List[float]:
        """
        Gets the downsample factor for each level in the WSI. For ImageWSI, it's always just level 0 with 1.0.
        """
        self._lazy_initialize()
        return self.level_downsamples

    def get_best_level_and_custom_downsample(
        self,
        downsample: float,
        tolerance: float = 0.01 # Not strictly used, but kept for signature consistency
    ) -> Tuple[int, float]:
        """
        Determines the best level and custom downsample for an ImageWSI.
        Since ImageWSI has only one level (level 0), it always returns level 0.
        The custom_downsample is the desired `downsample` factor itself.
        """
        self._lazy_initialize()
        return 0, downsample # Always level 0, custom downsample is the requested factor

    def create_patcher(
        self,
        patch_size: int, 
        src_pixel_size: Optional[float] = None, 
        dst_pixel_size: Optional[float] = None, 
        src_mag: Optional[int] = None, 
        dst_mag: Optional[int] = None, 
        overlap: int = 0, 
        mask: Optional[gpd.GeoDataFrame] = None,
        coords_only: bool = False, 
        custom_coords:  Optional[np.ndarray] = None,
        min_tissue_proportion: float = 0.,
        pil: bool = False,
    ) -> NumpyWSIPatcher: # ImageWSI internally treats images as numpy-like
        """
        Creates a patcher for the PIL Image WSI.
        """
        # Call the base WSI's create_patcher. It will then call the appropriate WSIPatcher subclass.
        # Since ImageWSI doesn't have OpenSlide or CuCIM specific features, it routes to NumpyWSIPatcher.
        return NumpyWSIPatcher(
            wsi=self, # Pass self (ImageWSI instance)
            patch_size=patch_size,
            src_pixel_size=src_pixel_size,
            dst_pixel_size=dst_pixel_size,
            src_mag=src_mag,
            dst_mag=dst_mag,
            overlap=overlap,
            mask=mask,
            coords_only=coords_only,
            custom_coords=custom_coords,
            min_tissue_proportion=min_tissue_proportion,
            pil=pil,
        )

    def close(self):
        """
        Close the internal PIL Image object to free memory.
        """
        if self.img is not None:
            self.img.close()
            self.img = None
            self._is_initialized = False # Mark as uninitialized

```

```python
# sauron/feature_extraction/wsi/cucim.py
# New file, adapted from trident/wsi_objects/CuCIMWSI.py

from __future__ import annotations
import numpy as np
from PIL import Image
from typing import Tuple, Optional, Union, List

from sauron.feature_extraction.wsi.base import WSI, ReadMode
from sauron.feature_extraction.wsi.patching import CuImageWSIPatcher # For type hinting
from sauron.feature_extraction.utils.io import CuImageWarning # For warning


class CuCIMWSI(WSI):

    def __init__(self, image_source: Any, **kwargs) -> None: # Use Any for cucim.CuImage due to import issues
        # Check if cucim is actually available before proceeding, to provide clearer errors
        try:
            from cucim import CuImage
        except ImportError:
            # Re-raise if cucim is chosen but not installed
            raise ImportError(
                "cuCIM is not installed. Cannot use CuCIMWSI backend. "
                "Install with `pip install cucim cupy-cuda12x` (adjust CUDA version)."
            )

        if not isinstance(image_source, CuImage):
            # If image_source is a path, try to open it with CuImage
            if isinstance(image_source, str):
                try:
                    self.img = CuImage(image_source)
                except Exception as e:
                    raise RuntimeError(f"Failed to open WSI with CuImage from path '{image_source}': {e}")
            else:
                raise ValueError(f"CuCIMWSI expects a cucim.CuImage object or a string path, but got {type(image_source)}.")
        else:
            self.img = image_source # Store the CuImage object directly

        super().__init__(**kwargs) # Call base WSI init, will trigger _lazy_initialize if lazy_init=False

    def _lazy_initialize(self) -> None:
        """
        Lazily load the whole-slide image (WSI) and its metadata using CuCIM.

        This method performs deferred initialization by reading the WSI file
        only when needed. It also retrieves key metadata such as dimensions,
        magnification, and microns-per-pixel (MPP). If a tissue segmentation
        mask is available, it is also loaded.

        Raises
        ------
        FileNotFoundError
            If the WSI file or required segmentation mask is missing.
        Exception
            For any other errors that occur while initializing the WSI.

        Notes
        -----
        After initialization, the following attributes are set:
        - `width` and `height`: spatial dimensions of the WSI.
        - `mpp`: microns per pixel, inferred if not already set.
        - `mag`: estimated magnification level of the image.
        - `level_count`, `level_downsamples`, and `level_dimensions`: multiresolution pyramid metadata.
        - `properties`: raw metadata from the image.
        - `gdf_contours`: tissue mask contours, if applicable.
        """

        super()._lazy_initialize() # Call base class lazy init

        if not self._is_initialized: # Only proceed if base init didn't already mark as initialized
            try:
                # self.img should already be set from __init__ for CuCIMWSI
                # If not provided, it means this WSI object was created via `load_wsi(path)`
                # which would provide the CuImage object to __init__.
                if self.img is None:
                    from cucim import CuImage
                    self.img = CuImage(self.slide_path)

                # CuImage's size() returns (height, width), unlike OpenSlide's dimensions (width, height)
                self.dimensions = (self.img.size()[1], self.img.size()[0])  # width, height
                self.width, self.height = self.dimensions
                
                # CuImage's resolutions dictionary
                self.level_count = self.img.resolutions['level_count']
                self.level_downsamples = self.img.resolutions['level_downsamples']
                self.level_dimensions = self.img.resolutions['level_dimensions']
                self.properties = self.img.metadata

                # Fetch MPP and Magnification only if not already provided in constructor
                if self.mpp is None:
                    self.mpp = self._fetch_mpp(self.custom_mpp_keys)
                if self.mag is None:
                    self.mag = self._fetch_magnification(self.custom_mpp_keys)
                
                self._is_initialized = True # Mark as fully initialized

            except Exception as e:
                # Issue CuImageWarning if cucim is technically installed but fails to load a specific file
                warnings.warn(f"Failed to initialize WSI using CuCIM for '{self.slide_path}'. Falling back may not be possible. Error: {e}", CuImageWarning)
                raise RuntimeError(f"Failed to initialize WSI using CuCIM: {self.slide_path}. Error: {e}") from e

    def _fetch_mpp(self, custom_keys: Optional[List[str]] = None) -> float:
        """
        Fetch the microns per pixel (MPP) from CuImage metadata.

        Parameters
        ----------
        custom_keys : list of str, optional
            Optional list of custom metadata keys (e.g., 'mpp_x', 'mpp_y') to check first.

        Returns
        -------
        float
            MPP value in microns per pixel.

        Raises
        ------
        ValueError
            If MPP cannot be determined from metadata.
        """
        self._lazy_initialize() # Ensure img and properties are loaded

        import json
        import warnings

        def try_parse_float(val: Any) -> Optional[float]:
            try:
                return float(val)
            except (ValueError, TypeError):
                return None

        # CuCIM metadata can be a JSON string or a dictionary
        metadata = self.img.metadata
        if isinstance(metadata, str):
            try:
                metadata = json.loads(metadata)
            except json.JSONDecodeError:
                warnings.warn(f"CuCIM metadata for {self.name} is a malformed JSON string. Will proceed with direct key access.")
                metadata = {} # Treat as empty dict if malformed

        # Flatten nested CuCIM metadata for easier access
        flat_meta = {}
        def flatten_dict(d: dict, parent_key: str = ''):
            for k, v in d.items():
                key = f"{parent_key}.{k}" if parent_key else k
                if isinstance(v, dict):
                    flatten_dict(v, key)
                else:
                    flat_meta[key.lower()] = v
        
        # Only flatten if metadata is a dict; if it's a direct string, it's not nested dict.
        if isinstance(metadata, dict):
            flatten_dict(metadata)
        elif isinstance(metadata, str): # Handle cases where metadata might be a direct string value for MPP
            if try_parse_float(metadata) is not None:
                return try_parse_float(metadata) # If the whole metadata string IS the MPP

        mpp_x = None
        mpp_y = None

        # Prioritize custom keys if provided
        if custom_keys:
            # Assume custom_keys contains string keys like 'mpp_x', 'mpp_y' directly
            # It's better if custom_keys maps to the exact metadata path e.g. {'mpp_x': 'some.nested.mpp_x'}
            # For simplicity, if custom_keys provided, treat them as direct lookup keys in flat_meta
            for k in custom_keys:
                lower_k = k.lower()
                if mpp_x is None and lower_k in flat_meta:
                    mpp_x = try_parse_float(flat_meta[lower_k])
                if mpp_y is None and lower_k in flat_meta:
                    mpp_y = try_parse_float(flat_meta[lower_k])
                if mpp_x is not None and mpp_y is not None:
                    break # Found both

        # Standard fallback keys used in SVS, NDPI, MRXS, etc. (often present in CuCIM metadata)
        # Note: CuCIM sometimes uses 'spacing' or 'resolution' directly for MPP
        fallback_keys = [
            'openslide.mpp-x', 'openslide.mpp-y',
            'tiff.resolution-x', 'tiff.resolution-y',
            'mpp', 'spacing', 'microns_per_pixel',
            'aperio.mpp', 'hamamatsu.mpp',
            'metadata.resolutions.level[0].spacing',
            'metadata.resolutions.level[0].physical_size.0', # Common for some OME-TIFF metadata
            'resolution', # Simple key, sometimes used.
        ]

        for key in fallback_keys:
            lower_key = key.lower()
            if mpp_x is None and lower_key in flat_meta:
                mpp_x = try_parse_float(flat_meta[lower_key])
            if mpp_y is None and lower_key in flat_meta: # Check for Y explicitly
                mpp_y = try_parse_float(flat_meta[lower_key])
            if mpp_x is not None and mpp_y is not None:
                break

        # Use same value for both axes if only one was found
        if mpp_x is not None and mpp_y is None:
            mpp_y = mpp_x
        if mpp_y is not None and mpp_x is None:
            mpp_x = mpp_y
        
        # Ensure positive and non-zero MPP
        if mpp_x is not None and mpp_y is not None and mpp_x > 0 and mpp_y > 0:
            return float((mpp_x + mpp_y) / 2)
        
        raise ValueError(
            f"Unable to extract MPP from CuCIM metadata for: '{self.slide_path}'.\n"
            "Suggestions:\n"
            "- Provide `custom_mpp_keys` with metadata key mappings for 'mpp_x' and 'mpp_y'.\n"
            "- Set the MPP manually when constructing the CuCIMWSI object (e.g., `mpp=0.25`)."
        )

    def _fetch_magnification(self, custom_mpp_keys: Optional[List[str]] = None) -> int:
        """
        Retrieve estimated magnification from CuCIM metadata or calculated from MPP.

        Parameters
        ----------
        custom_mpp_keys : list of str, optional
            Keys to aid in computing magnification from MPP.

        Returns
        -------
        int
            Estimated magnification.

        Raises
        ------
        ValueError
            If magnification cannot be determined.
        """
        self._lazy_initialize() # Ensure img and properties are loaded

        # First try to infer from MPP using base WSI's helper, if MPP is available
        inferred_mag_from_mpp = super()._fetch_magnification(custom_mpp_keys)
        if inferred_mag_from_mpp is not None:
            return inferred_mag_from_mpp

        # Then try to get from CuCIM properties (less common to have a direct "magnification" property)
        # Assuming properties could be a dict
        if isinstance(self.properties, dict):
            metadata_mag = self.properties.get('objective_power') # Common key
            if metadata_mag is not None:
                try:
                    mag_val = int(float(metadata_mag)) # Robust conversion
                    if mag_val > 0: return mag_val
                except ValueError:
                    pass
            # Also check in flattened metadata if available
            flat_meta = {}
            if isinstance(self.properties, dict):
                def flatten_dict_recursive(d, parent_key=''):
                    for k, v in d.items():
                        new_key = f"{parent_key}.{k}" if parent_key else k
                        if isinstance(v, dict):
                            flatten_dict_recursive(v, new_key)
                        else:
                            flat_meta[new_key.lower()] = v
                flatten_dict_recursive(self.properties)
                if 'objective_power' in flat_meta:
                    try:
                        mag_val = int(float(flat_meta['objective_power']))
                        if mag_val > 0: return mag_val
                    except ValueError:
                        pass


        raise ValueError(f"Unable to determine magnification from metadata for: {self.slide_path}")

    def read_region(
        self,
        location: Tuple[int, int],
        level: int,
        size: Tuple[int, int],
        read_as: ReadMode = 'pil',
    ) -> Union[Image.Image, np.ndarray]:
        """
        Extract a specific region from the whole-slide image (WSI) using CuCIM.

        Parameters
        ----------
        location : Tuple[int, int]
            (x, y) coordinates of the top-left corner of the region to extract at level 0.
        level : int
            Pyramid level to read from.
        size : Tuple[int, int]
            (width, height) of the region to extract at the specified level.
        read_as : {'pil', 'numpy'}, optional
            Output format for the region:
            - 'pil': returns a PIL Image (default)
            - 'numpy': returns a NumPy array (H, W, 3)

        Returns
        -------
        Union[PIL.Image.Image, np.ndarray]
            The extracted region in the specified format.

        Raises
        ------
        ValueError
            If `read_as` is not one of the supported options.
        """
        self._lazy_initialize() # Ensure img is loaded

        import cupy as cp

        # CuCIM's read_region returns a CuPy array. Convert to NumPy.
        # It's always (C, H, W) by default unless specified, but CuCIM also supports (H, W, C)
        # Default behavior: (C, H, W) is typical for `read_region` of images.
        # However, `read_region` for images usually outputs (H, W, C).
        # To be safe, specify `to_device='cpu'` which often returns NumPy.
        region_cp_or_np = self.img.read_region(location=location, level=level, size=size, to_device='cpu')
        
        # Ensure it's a NumPy array and has the correct channel order (H, W, C)
        if isinstance(region_cp_or_np, cp.ndarray):
            region_np = cp.asnumpy(region_cp_or_np)
        else: # Assumed to be numpy array already if to_device='cpu' worked
            region_np = region_cp_or_np

        # If image is (C, H, W), transpose to (H, W, C)
        if region_np.ndim == 3 and region_np.shape[0] < region_np.shape[1] and region_np.shape[0] < region_np.shape[2]:
            region_np = np.transpose(region_np, (1, 2, 0)) # C H W -> H W C
        
        # Ensure 3 channels, handle RGBA by dropping alpha
        if region_np.shape[-1] == 4:
            region_np = region_np[:, :, :3]
        elif region_np.shape[-1] == 1: # Convert grayscale to RGB
            region_np = np.stack([region_np.squeeze()] * 3, axis=-1)

        if read_as == 'numpy':
            return region_np
        elif read_as == 'pil':
            return Image.fromarray(region_np).convert("RGB")
        else:
            raise ValueError(f"Invalid `read_as` value: {read_as}. Must be 'pil' or 'numpy'.")
        
    def get_dimensions(self) -> Tuple[int, int]:
        """
        Return the (width, height) dimensions of the CuCIM-managed WSI at level 0.
        """
        self._lazy_initialize() # Ensure img is loaded
        # CuImage's size() returns (height, width)
        return (self.img.size()[1], self.img.size()[0])

    def get_thumbnail(self, size: tuple[int, int]) -> Image.Image:
        """
        Generate a thumbnail image of the WSI using CuCIM.

        Args:
        -----
        size : tuple[int, int]
            A tuple specifying the desired width and height of the thumbnail.

        Returns:
        --------
        Image.Image:
            The thumbnail as a PIL Image in RGB format.
        """
        self._lazy_initialize() # Ensure img is loaded

        target_width, target_height = size

        # Compute desired downsample factor from original dimensions
        downsample_x = self.width / target_width
        downsample_y = self.height / target_height
        desired_downsample_factor_relative_to_level0 = max(downsample_x, downsample_y)

        # Get the best level index and the additional custom downsample needed from that level
        level, custom_downsample = self.get_best_level_and_custom_downsample(desired_downsample_factor_relative_to_level0)

        # Read the region at the chosen level. Size needs to be computed correctly for that level.
        level_actual_dims = self.level_dimensions()[level]
        # Read the full content of this level, then resize in CPU
        region_np = self.read_region(
            location=(0, 0), # Read from top-left of the level
            level=level,
            size=level_actual_dims, # Read the whole level
            read_as='numpy' # Get as numpy to resize with OpenCV (or PIL)
        )
        
        # Resize to target size. CuCIM read_region sometimes has issues with direct target size.
        # Using PIL resize from numpy.
        thumbnail_pil = Image.fromarray(region_np).convert("RGB")
        thumbnail_pil = thumbnail_pil.resize(size, resample=Image.Resampling.LANCZOS)

        return thumbnail_pil


    def level_dimensions(self) -> List[Tuple[int, int]]:
        """
        Gets the dimensions of each level in the WSI from CuCIM.
        """
        self._lazy_initialize()
        return self.img.resolutions['level_dimensions']

    def level_downsamples(self) -> List[float]:
        """
        Gets the downsample factor for each level in the WSI from CuCIM.
        """
        self._lazy_initialize()
        return self.img.resolutions['level_downsamples']

    def get_best_level_and_custom_downsample(
        self,
        downsample: float, # desired downsample factor relative to level 0 (e.g. 2.0 for 2x downsample from level 0)
        tolerance: float = 0.01 # Not strictly used, but for consistency
    ) -> Tuple[int, float]:
        """
        Determines the best CuCIM level and custom downsample factor to approximate a desired downsample value.

        Args:
            downsample (float): The desired total downsample factor relative to level 0.
            tolerance (float, optional): Tolerance for matching existing downsample levels. Defaults to 0.01.

        Returns:
            Tuple[int, float]: A tuple containing the best level index and the additional
                               custom downsample factor needed at that level.
        """
        self._lazy_initialize() # Ensure img is loaded

        level_downsamples = self.img.resolutions["level_downsamples"]
        
        # Find the smallest level_downsample factor that is >= desired `downsample`
        # This gives the level that is *just* high enough resolution.
        best_level_index = 0
        for i, level_ds in enumerate(level_downsamples):
            if level_ds >= downsample:
                best_level_index = i
                break
        
        actual_level_downsample = level_downsamples[best_level_index]

        # Calculate the custom downsample needed from this `best_level_index`
        # If actual_level_downsample is 4.0, and desired is 2.0, then custom_downsample = 2.0 / 4.0 = 0.5 (need to downsample by 0.5 at that level)
        # If actual_level_downsample is 2.0, and desired is 4.0, then custom_downsample = 4.0 / 2.0 = 2.0 (need to upsample by 2.0 at that level)
        if actual_level_downsample > 0: # Avoid division by zero
            custom_downsample = downsample / actual_level_downsample
        else:
            raise ValueError(f"Invalid level downsample factor {actual_level_downsample} at level {best_level_index} for slide {self.name}.")

        return best_level_index, custom_downsample

    def create_patcher(
        self,
        patch_size: int, 
        src_pixel_size: Optional[float] = None, 
        dst_pixel_size: Optional[float] = None, 
        src_mag: Optional[int] = None, 
        dst_mag: Optional[int] = None, 
        overlap: int = 0, 
        mask: Optional[gpd.GeoDataFrame] = None,
        coords_only: bool = False, 
        custom_coords:  Optional[np.ndarray] = None,
        min_tissue_proportion: float = 0.,
        pil: bool = False,
    ) -> CuImageWSIPatcher:
        """
        Creates a patcher for the cuCIM WSI.
        """
        # Call the base WSI's create_patcher. It will then call the appropriate WSIPatcher subclass.
        return CuImageWSIPatcher(
            wsi=self, # Pass self (CuCIMWSI instance)
            patch_size=patch_size,
            src_pixel_size=src_pixel_size,
            dst_pixel_size=dst_pixel_size,
            src_mag=src_mag,
            dst_mag=dst_mag,
            overlap=overlap,
            mask=mask,
            coords_only=coords_only,
            custom_coords=custom_coords,
            min_tissue_proportion=min_tissue_proportion,
            pil=pil,
        )

    def close(self):
        """
        Close the CuCIM object and release its resources.
        """
        if self.img is not None:
            self.img.close()
            self.img = None
            self._is_initialized = False # Mark as uninitialized

```

```python
# sauron/feature_extraction/wsi/factory.py
# New file, adapted from trident/wsi_objects/WSIFactory.py

import os
from typing import Optional, Literal, Union, Any

# Import base WSI class for type hinting
from sauron.feature_extraction.wsi.base import WSI 
from sauron.feature_extraction.wsi.openslide import OpenSlideWSI
from sauron.feature_extraction.wsi.image import ImageWSI
# Conditional import for CuCIM to handle optional dependency
try:
    from sauron.feature_extraction.wsi.cucim import CuCIMWSI
    _CUCIM_AVAILABLE = True
except ImportError:
    _CUCIM_AVAILABLE = False
    import warnings
    from sauron.feature_extraction.utils.io import CuImageWarning
    warnings.simplefilter('once', CuImageWarning) # Warn only once


# Define common WSI extensions and their typical readers
WSIReaderType = Literal['openslide', 'image', 'cucim']
OPENSLIDE_EXTENSIONS = {'.svs', '.tif', '.tiff', '.ndpi', '.vms', '.vmu', '.scn', '.mrxs', '.bif', '.czi'} # .czi added as OpenSlide supports it from 4.0.0
CUCIM_EXTENSIONS = {'.svs', '.tif', '.tiff', '.czi', '.ndpi'} # CuCIM supports these
PIL_EXTENSIONS = {'.png', '.jpg', '.jpeg', '.bmp', '.gif'} # PIL supports these


def load_wsi(
    slide_path: str,
    reader_type: Optional[WSIReaderType] = None,
    **kwargs: Any # Capture additional args for WSI constructor
) -> WSI:
    """
    Load a whole-slide image (WSI) using the appropriate backend.

    By default, uses OpenSlideWSI for OpenSlide-supported file extensions,
    and ImageWSI for others. Users may override this behavior by explicitly
    specifying a reader using the `reader_type` argument.

    Parameters
    ----------
    slide_path : str
        Path to the whole-slide image.
    reader_type : {'openslide', 'image', 'cucim'}, optional
        Manually specify the WSI reader to use. If None (default), selection
        is automatic based on file extension.
    **kwargs : Any
        Additional keyword arguments passed to the WSI reader constructor.
        Common kwargs: `original_path`, `name`, `tissue_seg_path`, `custom_mpp_keys`, `lazy_init`, `mpp`, `max_workers`.

    Returns
    -------
    WSI
        An instance of the appropriate WSI reader subclass.

    Raises
    ------
    ValueError
        If `reader_type` is 'cucim' but the cucim package is not installed.
        Or if an unknown reader type is specified.
    FileNotFoundError
        If the slide_path does not exist.
    """
    if not os.path.exists(slide_path):
        raise FileNotFoundError(f"WSI file not found at: {slide_path}")

    ext = os.path.splitext(slide_path)[1].lower()

    # Try to open image source object first, as some WSI classes accept already opened objects
    # This avoids redundant file opening by downstream WSI constructors
    image_source_obj: Any = None
    if reader_type == 'openslide' or (reader_type is None and ext in OPENSLIDE_EXTENSIONS):
        try:
            image_source_obj = openslide.OpenSlide(slide_path)
            # Ensure it's not a broken or empty OpenSlide handle
            if image_source_obj.dimensions == (0,0):
                raise openslide.OpenSlideError(f"OpenSlide returned 0x0 dimensions for {slide_path}. Likely corrupted or unsupported format.")
        except openslide.OpenSlideError as e:
            # If OpenSlide fails, print warning and try next reader if reader_type was None
            print(f"Warning: OpenSlide failed to open {slide_path}: {e}. Trying other readers if reader_type is auto.")
            if reader_type == 'openslide': # If explicitly requested OpenSlide, re-raise
                raise e
            reader_type = None # Reset to auto-select

    if _CUCIM_AVAILABLE and (reader_type == 'cucim' or (reader_type is None and ext in CUCIM_EXTENSIONS and image_source_obj is None)):
        try:
            from cucim import CuImage
            image_source_obj = CuImage(slide_path)
        except Exception as e:
            print(f"Warning: CuCIM failed to open {slide_path}: {e}. Trying other readers if reader_type is auto.")
            if reader_type == 'cucim': # If explicitly requested CuCIM, re-raise
                raise e
            reader_type = None # Reset to auto-select

    if reader_type == 'image' or (reader_type is None and image_source_obj is None):
        # If no other reader succeeded or explicitly image reader
        try:
            image_source_obj = Image.open(slide_path).convert("RGB")
        except Exception as e:
            raise RuntimeError(f"Failed to open {slide_path} with PIL.Image: {e}. No suitable reader found.")


    # Now instantiate the WSI class with the opened image_source_obj
    if reader_type == 'openslide':
        if not isinstance(image_source_obj, openslide.OpenSlide):
            raise TypeError(f"Expected openslide.OpenSlide object, but got {type(image_source_obj)} after opening {slide_path}. Check file format compatibility.")
        return OpenSlideWSI(image_source=image_source_obj, slide_path=slide_path, **kwargs)

    elif reader_type == 'image':
        if not isinstance(image_source_obj, Image.Image):
            raise TypeError(f"Expected PIL.Image.Image object, but got {type(image_source_obj)} after opening {slide_path}. Check file format compatibility.")
        return ImageWSI(image_source=image_source_obj, slide_path=slide_path, **kwargs)

    elif reader_type == 'cucim':
        if not _CUCIM_AVAILABLE:
            raise ValueError(
                f"Unsupported file format '{ext}' for CuCIM. Or CuCIM is not installed. "
                f"Supported formats: {', '.join(CUCIM_EXTENSIONS)}."
            )
        # image_source_obj should be CuImage if we reach here via CuCIM path
        return CuCIMWSI(image_source=image_source_obj, slide_path=slide_path, **kwargs)

    elif reader_type is None: # Auto-detection based on successful opening
        if isinstance(image_source_obj, openslide.OpenSlide):
            return OpenSlideWSI(image_source=image_source_obj, slide_path=slide_path, **kwargs)
        elif _CUCIM_AVAILABLE and isinstance(image_source_obj, CuImage):
            return CuCIMWSI(image_source=image_source_obj, slide_path=slide_path, **kwargs)
        elif isinstance(image_source_obj, Image.Image):
            return ImageWSI(image_source=image_source_obj, slide_path=slide_path, **kwargs)
        else:
            raise RuntimeError(f"Failed to auto-determine WSI reader for {slide_path}. No suitable backend could open the file.")

    else:
        raise ValueError(f"Unknown reader_type: {reader_type}. Choose from 'openslide', 'image', or 'cucim'.")
```

```python
# sauron/feature_extraction/wsi/patching.py
# New file, adapted from trident/wsi_objects/WSIPatcher.py

from __future__ import annotations

from typing import Tuple, Optional, Union, TYPE_CHECKING
import warnings
import cv2
import numpy as np
import geopandas as gpd
from shapely import Polygon

# Prevent circular import if Base is not fully defined yet, for type hinting
if TYPE_CHECKING:
    from sauron.feature_extraction.wsi.base import WSI


class WSIPatcher:
    """ Iterator class to handle patching, patch scaling and tissue mask intersection """
    
    def __init__(
        self, 
        wsi: 'WSI', 
        patch_size: int, # Desired output patch size in pixels (e.g., 256 for 256x256)
        src_pixel_size: float, # MPP of the WSI at level 0 (e.g., 0.25)
        dst_pixel_size: Optional[float] = None, # Desired MPP for output patches (e.g., 0.5 for 20x)
        src_mag: Optional[int] = None, # Base magnification of the WSI (e.g., 40)
        dst_mag: Optional[int] = None, # Desired output magnification (e.g., 20)
        overlap: int = 0, # Overlap in pixels at the output patch size (dst_pixel_size/dst_mag)
        mask: Optional[gpd.GeoDataFrame] = None,
        coords_only: bool = False,
        custom_coords: Optional[np.ndarray] = None, # (N, 2) array of (x_level0, y_level0)
        min_tissue_proportion: float = 0., # Min proportion of patch that must be tissue
        pil: bool = False, # Return PIL Image or NumPy array
    ):
        """ Initialize patcher, compute number of (masked) rows, columns.

        Args:
            wsi (WSI): WSI object to patch.
            patch_size (int): Patch width/height in pixels for the *output* patches (e.g., 256).
            src_pixel_size (float): MPP of the WSI at level 0.
            dst_pixel_size (float, optional): Desired MPP of the output patches. If None, `dst_mag` must be provided.
            src_mag (int, optional): Magnification of the WSI at level 0. If None, `self.wsi.mag` is used.
            dst_mag (int, optional): Target magnification for the output patches. If None, `dst_pixel_size` must be provided.
            overlap (int, optional): Overlap between patches in pixels (at `patch_size` resolution). Defaults to 0. 
            mask (gpd.GeoDataFrame, optional): GeoDataFrame of Polygons for tissue regions. Patches are filtered by this mask. Defaults to None (no filtering).
            coords_only (bool, optional): If True, iterator yields only (x, y) coordinates. Otherwise, yields (image, x, y). Default to False.
            custom_coords (np.ndarray, optional): Pre-defined (N, 2) array of (x, y) coordinates at Level 0 to extract. If provided, grid generation and mask filtering are bypassed. Defaults to None.
            min_tissue_proportion (float, optional): Minimum proportion of the *output* patch (at `patch_size`) that must be tissue to be kept. Only applies if `mask` is provided. Between 0. and 1.0. Defaults to 0. (any tissue).
            pil (bool, optional): If True, `get_patch_at` returns PIL.Image. Otherwise, NumPy array. Defaults to False.
        """
        self.wsi = wsi
        self.overlap = overlap
        self.width, self.height = self.wsi.get_dimensions() # WSI level 0 dimensions
        self.patch_size_output = patch_size # The final size of the patch returned by iterator

        self.mask = mask
        self.current_index = 0
        self.coords_only = coords_only
        self.pil = pil
        self.min_tissue_proportion = min_tissue_proportion

        # --- Determine scaling from src to output resolution (pixel size or magnification) ---
        # Ensure either dst_pixel_size or dst_mag is provided if scaling is implied
        if dst_pixel_size is None and dst_mag is None:
            warnings.warn("Neither dst_pixel_size nor dst_mag provided. Assuming no scaling is desired (dst_pixel_size = src_pixel_size, dst_mag = src_mag).")
            self.dst_pixel_size = src_pixel_size
            self.dst_mag = src_mag
        elif dst_pixel_size is not None and dst_mag is not None:
            # If both are provided, check for consistency. Prefer pixel size for more exact control.
            if abs(dst_pixel_size - (10 / dst_mag)) > 0.01: # Check for a small tolerance
                warnings.warn(f"Inconsistent dst_pixel_size ({dst_pixel_size}) and dst_mag ({dst_mag}). Using dst_pixel_size.")
            self.dst_pixel_size = dst_pixel_size
            self.dst_mag = dst_mag
        elif dst_pixel_size is not None:
            self.dst_pixel_size = dst_pixel_size
            self.dst_mag = 10 / dst_pixel_size if dst_pixel_size > 0 else None
        elif dst_mag is not None:
            self.dst_mag = dst_mag
            self.dst_pixel_size = 10 / dst_mag if dst_mag > 0 else None
        
        if self.dst_pixel_size is None: # Should not happen with above logic, but for safety
            raise ValueError("Could not determine destination pixel size.")

        # Calculate the overall downsample factor from WSI's level 0 MPP to the desired output MPP
        # Example: src_pixel_size=0.25 (40x), dst_pixel_size=0.5 (20x) -> overall_downsample_factor = 0.5 / 0.25 = 2.0
        self.overall_downsample_factor = self.dst_pixel_size / src_pixel_size
        
        # Calculate the size of the patch to read from Level 0 of the WSI
        # This is `patch_size_output` scaled back to Level 0
        # Example: patch_size_output=256, overall_downsample_factor=2.0 -> patch_size_src = 256 * 2.0 = 512
        self.patch_size_src = round(self.patch_size_output * self.overall_downsample_factor)
        
        # Calculate overlap in Level 0 pixels
        self.overlap_src = round(self.overlap * self.overall_downsample_factor)

        # --- Prepare backend-specific parameters (level, patch_size_at_level, overlap_at_level) ---
        # This needs to be implemented by subclasses to handle different WSI backends
        # `level` is the best pyramid level to read from.
        # `patch_size_level` is the size in pixels to read at `level`.
        # `overlap_level` is the overlap in pixels at `level`.
        self.level, self.patch_size_level, self.overlap_level = self._prepare_backend_patching_params()
        
        # --- Generate or use provided coordinates ---
        if custom_coords is None: 
            self.cols, self.rows = self._calculate_grid_dimensions()
            # Generate all possible (x, y) top-left coordinates at Level 0
            all_grid_coords = np.array([
                self._grid_index_to_level0_coords(col, row) 
                for col in range(self.cols) 
                for row in range(self.rows)
            ])
            self.all_coords_level0 = all_grid_coords
        else:
            if not isinstance(custom_coords, np.ndarray) or custom_coords.ndim != 2 or custom_coords.shape[1] != 2:
                raise ValueError("custom_coords must be an (N, 2) NumPy array.")
            # Ensure custom_coords are integers (pixel coordinates)
            if not np.issubdtype(custom_coords.dtype, np.integer):
                warnings.warn("custom_coords are not integer type. Converting to int.")
                custom_coords = custom_coords.astype(np.int32)
            self.all_coords_level0 = custom_coords

        # --- Filter coordinates by mask if provided ---
        if self.mask is not None and not self.mask.empty:
            # Filter the level 0 coordinates based on the tissue mask and min_tissue_proportion
            self.valid_coords_level0 = self._filter_coords_by_mask(self.all_coords_level0, self.min_tissue_proportion)
        else:
            self.valid_coords_level0 = self.all_coords_level0 # All generated/custom coords are valid

    @abstractmethod
    def _prepare_backend_patching_params(self) -> Tuple[int, int, int]:
        """
        Abstract method to calculate patching parameters specific to the WSI backend.
        This includes the optimal pyramid `level` to read from, and the `patch_size`
        and `overlap` values at that specific `level`.

        Must be implemented by subclasses.

        Returns:
            Tuple[int, int, int]: (level, patch_size_at_level, overlap_at_level).
        """
        pass

    def _grid_index_to_level0_coords(self, col: int, row: int) -> Tuple[int, int]:
        """
        Converts grid indices (col, row) to their top-left pixel coordinates (x, y) at Level 0 of the WSI.
        Takes into account `patch_size_src` (size of patch to read from level 0) and `overlap_src` (overlap at level 0).
        """
        x = col * (self.patch_size_src - self.overlap_src)
        y = row * (self.patch_size_src - self.overlap_src)
        return x, y
            
    def _level0_coords_to_grid_index(self, x: int, y: int) -> Tuple[int, int]:
        """
        Converts Level 0 pixel coordinates (x, y) to grid indices (col, row).
        This is the inverse of `_grid_index_to_level0_coords`.
        """
        if (self.patch_size_src - self.overlap_src) == 0:
            raise ValueError("Patch size minus overlap cannot be zero for grid index calculation.")
        col = x // (self.patch_size_src - self.overlap_src)
        row = y // (self.patch_size_src - self.overlap_src)
        return col, row

    def _calculate_grid_dimensions(self) -> Tuple[int, int]:
        """
        Calculates the number of columns and rows required to cover the WSI at Level 0,
        considering `patch_size_src` and `overlap_src`.
        """
        step_size_src = self.patch_size_src - self.overlap_src
        if step_size_src <= 0:
            raise ValueError("Step size must be positive (patch_size_src > overlap_src).")

        # Number of full steps + 1 if there's a remainder (to cover the last partial patch)
        cols = (self.width - self.overlap_src + step_size_src - 1) // step_size_src if self.width > self.overlap_src else 1
        rows = (self.height - self.overlap_src + step_size_src - 1) // step_size_src if self.height > self.overlap_src else 1

        # Adjust for edge cases if the slide is smaller than a single patch
        if self.width <= self.patch_size_src and self.width > 0: cols = 1
        if self.height <= self.patch_size_src and self.height > 0: rows = 1
        
        # If image dimension is 0, then 0 columns/rows
        if self.width == 0: cols = 0
        if self.height == 0: rows = 0

        return cols, rows 
    
    def _filter_coords_by_mask(self, coords_level0: np.ndarray, min_proportion: float) -> np.ndarray:
        """
        Filters the given Level 0 coordinates based on the initialized tissue `mask`
        and `min_proportion` of tissue required within each patch.
        
        Args:
            coords_level0 (np.ndarray): An (N, 2) array of Level 0 (x, y) coordinates.
            min_proportion (float): Minimum proportion of the patch that must be tissue.
        
        Returns:
            np.ndarray: Filtered (M, 2) array of Level 0 (x, y) coordinates.
        """
        if self.mask is None or self.mask.empty:
            return coords_level0 # No mask, all patches are valid

        # Convert the mask to a single shapely geometry for faster intersection checks
        # Use unary_union which is generally more efficient than repeated intersections with GeoDataFrame
        # The CRS should already be set in mask_to_gdf, but ensure it's compatible if not.
        try:
            mask_geometry = self.mask.geometry.unary_union
        except Exception as e:
            warnings.warn(f"Failed to create unary_union from mask geometry: {e}. Falling back to individual intersections.")
            mask_geometry = self.mask.geometry

        valid_indices = []
        for i, (x, y) in enumerate(coords_level0):
            # Define the patch polygon at Level 0
            patch_polygon = Polygon([
                (x, y),
                (x + self.patch_size_src, y),
                (x + self.patch_size_src, y + self.patch_size_src),
                (x, y + self.patch_size_src)
            ])
            
            if not patch_polygon.is_valid: # Should ideally not happen for simple squares
                patch_polygon = shapely_make_valid(patch_polygon) # Use shapely's make_valid

            if not patch_polygon.is_empty:
                intersection = patch_polygon.intersection(mask_geometry)
                
                if not intersection.is_empty:
                    # Calculate proportion of tissue within the patch
                    # Handle GeometryCollection/MultiPolygon results from intersection
                    if intersection.geom_type == 'MultiPolygon' or intersection.geom_type == 'GeometryCollection':
                        # Sum areas of all valid polygons within the collection
                        intersected_area = sum(p.area for p in intersection.geoms if p.geom_type == 'Polygon' and p.is_valid)
                    else:
                        intersected_area = intersection.area
                    
                    if patch_polygon.area > 0 and (intersected_area / patch_polygon.area) >= min_proportion:
                        valid_indices.append(i)
        
        return coords_level0[valid_indices]
        
    def __len__(self) -> int:
        """Returns the total number of valid patches."""
        return len(self.valid_coords_level0)
    
    def __iter__(self) -> 'WSIPatcher':
        """Returns the iterator object itself."""
        self.current_index = 0
        return self
    
    def __next__(self) -> Union[Tuple[np.ndarray, int, int], Tuple[int, int]]:
        """Returns the next patch or coordinate."""
        if self.current_index >= len(self):
            raise StopIteration
        item = self.__getitem__(self.current_index)
        self.current_index += 1
        return item
    
    def __getitem__(self, index: int) -> Union[Tuple[np.ndarray, int, int], Tuple[Image.Image, int, int], Tuple[int, int]]:
        """Gets a patch or coordinate by its index.

        Args:
            index: The index of the valid patch coordinate.

        Returns:
            If `coords_only` is False, returns a tuple of (patch_image, x_level0, y_level0).
            If `coords_only` is True, returns a tuple of (x_level0, y_level0).

        Raises:
            IndexError: If the index is out of range.
        """
        if 0 <= index < len(self):
            x_level0, y_level0 = self.valid_coords_level0[index]
            if self.coords_only:
                return x_level0, y_level0
            
            # Use `get_patch_at_level0_coords` to read and resize the patch
            patch_data = self.get_patch_at_level0_coords(x_level0, y_level0)
            return patch_data, x_level0, y_level0
        else:
            raise IndexError("Index out of range")
        
    def get_patch_at_level0_coords(self, x_level0: int, y_level0: int) -> Union[np.ndarray, Image.Image]:
        """Reads and resizes a single patch at the given level 0 coordinates."""
        
        # Read the region from the WSI backend at the determined `self.level`
        # `x_level0`, `y_level0` are level 0 coordinates. These need to be scaled to `self.level`.
        # `patch_size_level` is the size to read at `self.level`.
        
        # Convert level 0 coordinates to coordinates at `self.level`
        x_at_level = int(round(x_level0 / self.wsi.level_downsamples()[self.level]))
        y_at_level = int(round(y_level0 / self.wsi.level_downsamples()[self.level]))

        patch_image_data = self.wsi.read_region(
            location=(x_at_level, y_at_level), # Coordinates at the chosen pyramid level
            level=self.level,
            size=(self.patch_size_level, self.patch_size_level), # Size to read at that pyramid level
            read_as='pil' if self.pil else 'numpy' # Request format based on `pil` flag
        )

        # Resize the patch to the final `self.patch_size_output` if necessary
        if self.patch_size_output != self.patch_size_level:
            if self.pil and isinstance(patch_image_data, Image.Image):
                patch_image_data = patch_image_data.resize((self.patch_size_output, self.patch_size_output), Image.Resampling.LANCZOS)
            elif isinstance(patch_image_data, np.ndarray):
                patch_image_data = cv2.resize(patch_image_data, (self.patch_size_output, self.patch_size_output), interpolation=cv2.INTER_AREA) # INTER_AREA for shrinking
                if patch_image_data.ndim == 2: # Convert grayscale to 3 channels if it somehow becomes 2D
                    patch_image_data = np.stack([patch_image_data]*3, axis=-1)
                elif patch_image_data.shape[-1] == 4: # Drop alpha channel if present
                    patch_image_data = patch_image_data[:, :, :3]
        
        return patch_image_data
    
    def get_cols_rows(self) -> Tuple[int, int]:
        """ Get the number of columns and rows in the patch grid. """
        # This function is only meaningful if custom_coords was NOT used.
        if self.custom_coords is not None:
            warnings.warn("`get_cols_rows` is not meaningful when `custom_coords` are used.")
            return (0, 0)
        return self.cols, self.rows
      

    def visualize(self, target_width: int = 1000) -> Image.Image:
        """ 
        Overlays patch coordinates computed by the WSIPatcher onto a scaled thumbnail of the WSI.
        It creates a visualization of the patcher coordinates and returns it as a PIL Image.

        Args:
            target_width (int): Desired width of the visualization thumbnail.

        Returns
        -------
        Image.Image
            Patch visualization
        """
        # Ensure WSI object is fully loaded
        self.wsi._lazy_initialize()

        # Calculate thumbnail dimensions and scale factor
        wsi_width, wsi_height = self.wsi.get_dimensions()
        downsample_factor = target_width / wsi_width if wsi_width > 0 else 1.0
        thumbnail_width = int(wsi_width * downsample_factor)
        thumbnail_height = int(wsi_height * downsample_factor)

        # Get thumbnail as NumPy array for OpenCV drawing
        canvas_np = np.array(self.wsi.get_thumbnail((thumbnail_width, thumbnail_height))).astype(np.uint8)

        # Calculate patch size at thumbnail resolution for drawing rectangles
        # patch_size_src is Level 0 patch size. Scale it down by downsample_factor
        thumbnail_patch_size = max(1, int(self.patch_size_src * downsample_factor))
        thickness = max(1, thumbnail_patch_size // 50) # Dynamic thickness

        # Draw patches (valid_coords_level0 holds the Level 0 coordinates)
        for (x_lvl0, y_lvl0) in self.valid_coords_level0:
            x_thumb = int(x_lvl0 * downsample_factor)
            y_thumb = int(y_lvl0 * downsample_factor)
            
            # Draw rectangle with red color
            cv2.rectangle(
                canvas_np, 
                (x_thumb, y_thumb), 
                (x_thumb + thumbnail_patch_size, y_thumb + thumbnail_patch_size), 
                (0, 0, 255), # BGR for red
                thickness
            )

        # Overlay tissue contours if available
        if self.mask is not None and not self.mask.empty:
            # `overlay_gdf_on_thumbnail` expects RGB thumbnail and will convert to BGR for drawing.
            # Scale for overlay_gdf_on_thumbnail is from GeoJSON Level0 coords to thumbnail pixels.
            # This is exactly `downsample_factor`.
            overlay_gdf_on_thumbnail(self.mask, canvas_np, "dummy_path", downsample_factor, tissue_color=(0, 255, 0))
        
        # Add informative text (optional)
        text_area_height = 130 # For text info area
        text_offset_x = int(thumbnail_width * 0.02) # 2% from left
        text_spacing_y = 25 # Vertical spacing
        
        # Create a semi-transparent background for text if desired
        # canvas_np[0:text_area_height, 0:min(thumbnail_width, 350)] = (canvas_np[0:text_area_height, 0:min(thumbnail_width, 350)] * 0.7).astype(np.uint8)

        # Convert to BGR for text drawing
        if canvas_np.shape[-1] == 3 and canvas_np[0,0,0] > canvas_np[0,0,2]: # Simple check for RGB vs BGR
            canvas_np = cv2.cvtColor(canvas_np, cv2.COLOR_RGB2BGR)

        cv2.putText(canvas_np, f'Patches: {len(self)}', (text_offset_x, text_spacing_y), 
                    cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 1, cv2.LINE_AA)
        cv2.putText(canvas_np, f'WSI Dims: {wsi_width}x{wsi_height}', (text_offset_x, text_spacing_y * 2), 
                    cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1, cv2.LINE_AA)
        cv2.putText(canvas_np, f'WSI MPP: {self.wsi.mpp:.2f} um/px', (text_offset_x, text_spacing_y * 3), 
                    cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1, cv2.LINE_AA)
        cv2.putText(canvas_np, f'WSI Mag: {self.wsi.mag}x', (text_offset_x, text_spacing_y * 4), 
                    cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1, cv2.LINE_AA)
        cv2.putText(canvas_np, f'Patch: {self.patch_size_output}px @ {self.dst_mag}x ({self.overlap}ovlp)', 
                    (text_offset_x, text_spacing_y * 5), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1, cv2.LINE_AA)

        # Convert back to RGB for PIL Image
        canvas_np = cv2.cvtColor(canvas_np, cv2.COLOR_BGR2RGB)
        return Image.fromarray(canvas_np)
    

class OpenSlideWSIPatcher(WSIPatcher):
    """A WSIPatcher implementation for OpenSlide-backed WSIs."""

    def _prepare_backend_patching_params(self) -> Tuple[int, int, int]:
        """
        Calculates patching parameters specific to OpenSlide.
        Determines the optimal OpenSlide pyramid level to read from.
        """
        # `overall_downsample_factor` is the total downsample from Level 0 to target output resolution.
        # OpenSlide's `get_best_level_for_downsample` will find the best level that is *at least*
        # as high resolution as needed.
        level, custom_downsample_at_level = self.wsi.get_best_level_and_custom_downsample(self.overall_downsample_factor)
        
        # `level_downsample` is the actual downsample factor of the chosen `level` relative to level 0.
        level_downsample = self.wsi.level_downsamples()[level]
        
        # `patch_size_at_level` is the size in pixels to read from the chosen `level`.
        # This is `patch_size_output` scaled by `custom_downsample_at_level` (which means if custom_downsample_at_level is 2.0,
        # it means current level is 2x lower resolution than needed for patch_size_output, so we need to read a 2x larger patch from this level).
        # OR more simply: patch_size_at_level = (patch_size_output * overall_downsample_factor) / actual_level_downsample
        # which simplifies to patch_size_output * custom_downsample_at_level
        patch_size_level = round(self.patch_size_output * custom_downsample_at_level)
        
        # `overlap_at_level` is the overlap in pixels at the chosen `level`.
        overlap_level = round(self.overlap * custom_downsample_at_level)
        
        return level, patch_size_level, overlap_level


class CuImageWSIPatcher(WSIPatcher):
    """A WSIPatcher implementation for cuCIM-backed WSIs."""

    def _prepare_backend_patching_params(self) -> Tuple[int, int, int]:
        """
        Calculates patching parameters specific to cuCIM.
        Determines the optimal cuCIM pyramid level to read from.
        """
        # `overall_downsample_factor` is the total downsample from Level 0 to target output resolution.
        level, custom_downsample_at_level = self.wsi.get_best_level_and_custom_downsample(self.overall_downsample_factor)
        
        # `level_downsample` is the actual downsample factor of the chosen `level` relative to level 0.
        level_downsample = self.wsi.level_downsamples()[level]
        
        # `patch_size_at_level` is the size in pixels to read from the chosen `level`.
        patch_size_level = round(self.patch_size_output * custom_downsample_at_level)
        
        # `overlap_at_level` is the overlap in pixels at the chosen `level`.
        overlap_level = round(self.overlap * custom_downsample_at_level)
        
        return level, patch_size_level, overlap_level


class NumpyWSIPatcher(WSIPatcher):
    """A WSIPatcher implementation for NumPy array-backed WSIs."""

    def _prepare_backend_patching_params(self) -> Tuple[int, int, int]:
        """
        Sets patching parameters for a NumPy array.
        Since NumPy arrays are single-resolution, level is set to 0 (effectively level 0).
        `patch_size_level` and `overlap_level` are directly scaled versions from Level 0 to input for model.
        """
        # For a NumPy array, there's effectively only one level, which we call 0.
        level = 0 
        
        # The `overall_downsample_factor` applies directly here.
        # patch_size_level is the size of the region to read from the NumPy array
        # to later resize to `self.patch_size_output`.
        # Example: patch_size_output=256, overall_downsample_factor=2 (for 40x->20x)
        # patch_size_level = round(256 * 2) = 512.
        patch_size_level = round(self.patch_size_output * self.overall_downsample_factor)
        
        overlap_level = round(self.overlap * self.overall_downsample_factor)
        
        # Special handling: if patch_size_level is 0, make it 1 to avoid division by zero errors or invalid dimensions.
        if patch_size_level == 0:
            warnings.warn("Calculated patch_size_level is 0. Setting to 1 to prevent errors. Check input patch_size and scaling factors.")
            patch_size_level = 1

        return level, patch_size_level, overlap_level
```

```python
# sauron/feature_extraction/wsi/dataset.py
# New file, adapted from trident/wsi_objects/WSIPatcherDataset.py

from torch.utils.data import Dataset
from typing import Callable, Tuple, Union
import numpy as np
from PIL import Image

from sauron.feature_extraction.wsi.patching import WSIPatcher


class WSIPatcherDataset(Dataset):
    """ Dataset from a WSI patcher to directly read tiles on a slide  """
    
    def __init__(self, patcher: WSIPatcher, transform: Optional[Callable] = None):
        """
        Initializes the dataset.

        Args:
            patcher (WSIPatcher): An instance of WSIPatcher configured to yield image patches.
            transform (Optional[Callable]): A callable transform to apply to each patch (e.g., torchvision.transforms).
        """
        self.patcher = patcher
        self.transform = transform
                              
    def __len__(self) -> int:
        return len(self.patcher)
    
    def __getitem__(self, index: int) -> Union[Tuple[torch.Tensor, Tuple[int, int]], Tuple[Tuple[int, int]]]:
        """
        Retrieves a patch and its coordinates.

        Args:
            index (int): The index of the patch to retrieve.

        Returns:
            Union[Tuple[torch.Tensor, Tuple[int, int]], Tuple[Tuple[int, int]]]:
                If patcher is `coords_only=False`: (transformed_patch_tensor, (x_coord_level0, y_coord_level0)).
                If patcher is `coords_only=True`: ((x_coord_level0, y_coord_level0)).
        """
        # patcher[index] will return (image_data, x, y) or (x, y) based on coords_only flag
        item = self.patcher[index]

        if self.patcher.coords_only:
            # item is (x, y)
            return item
        else:
            # item is (image_data, x, y)
            image_data, x, y = item
            
            if self.transform:
                # Ensure image_data is PIL Image if transform expects it, or numpy if transform handles that
                if isinstance(image_data, np.ndarray):
                    image_data = Image.fromarray(image_data) # Convert to PIL for common torchvision transforms
                transformed_image = self.transform(image_data)
                return transformed_image, (x, y)
            else:
                # If no transform, return original image data and coords
                # Convert to tensor if it's numpy array and no transform is applied, for consistency with ML pipelines
                if isinstance(image_data, np.ndarray):
                    # Ensure channels-first (CxHxW) for PyTorch if no transform is applied
                    if image_data.ndim == 3 and image_data.shape[2] == 3: # HWC
                        image_data = np.transpose(image_data, (2, 0, 1)) # CHW
                    return torch.from_numpy(image_data).float(), (x, y)
                return image_data, (x, y) # Return as is if already PIL or other format

```

```python
# feature_extract
# Updated script to use the new sauron.feature_extraction.processor.Processor

import argparse
import logging
import os
import sys
from termcolor import colored
from queue import Queue
from threading import Thread

# Import the new Processor class
from sauron.feature_extraction.processor import Processor
# Import concurrency utilities for parallel caching pipeline
from sauron.feature_extraction.concurrency import batch_producer, batch_consumer


# Configure logger
logging.basicConfig(
    level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s"
)
logger = logging.getLogger(__name__)


def build_parser():
    """
    Parse command-line arguments for the Sauron feature extraction script.
    Adapted from trident's `run_batch_of_slides.py` parser.
    """
    parser = argparse.ArgumentParser(description='Run Sauron Whole Slide Image Processing')

    # Generic arguments 
    parser.add_argument('--gpu', type=int, default=0, help='GPU index to use for processing tasks.')
    parser.add_argument('--task', type=str, default='seg', 
                        choices=['seg', 'coords', 'feat', 'all', 'cache'], 
                        help='Task to run: seg (segmentation), coords (save tissue coordinates), feat (extract features), all (run all steps), cache (populate WSI cache only).')
    parser.add_argument('--job_dir', type=str, required=True, help='Directory to store outputs.')
    parser.add_argument('--skip_errors', action='store_true', default=False, 
                        help='Skip errored slides and continue processing.')
    parser.add_argument('--max_workers', type=int, default=None, 
                        help='Maximum number of workers for data loading (e.g., in DataLoader). If None, inferred based on CPU cores.')
    parser.add_argument('--batch_size', type=int, default=64, 
                        help="Batch size used for segmentation and feature extraction. Will be overridden by "
                        "`seg_batch_size` and `feat_batch_size` if specified. Defaults to 64.")

    # Caching argument for fast WSI processing
    parser.add_argument(
        '--wsi_cache', type=str, default=None,
        help='Path to a local cache (e.g., SSD) used to speed up access to WSIs stored on slower drives (e.g., HDD). '
             'If provided, WSIs are copied here before processing.'
    )
    parser.add_argument(
        '--cache_batch_size', type=int, default=32,
        help='Maximum number of slides to cache locally at once when using --wsi_cache. Helps control disk usage.'
    )
    parser.add_argument('--clear_cache', action='store_true', default=False, 
                        help='If using --wsi_cache, delete cached WSIs after processing each batch.')

    # Slide-related arguments
    parser.add_argument('--wsi_dir', type=str, required=True, 
                        help='Directory containing WSI files (can be nested if --search_nested is used).')
    parser.add_argument('--wsi_ext', type=str, nargs='+', default=None, 
                        help='List of allowed file extensions for WSI files (e.g., .svs .tif). If None, common extensions are used.')
    parser.add_argument('--custom_mpp_keys', type=str, nargs='+', default=None,
                    help='Custom keys used to store the resolution as MPP (micron per pixel) in WSI metadata.')
    parser.add_argument('--custom_list_of_wsis', type=str, default=None,
                    help='Path to a CSV file specifying a custom list of WSIs to process. Must contain a "wsi" column and optionally an "mpp" column.')
    parser.add_argument('--reader_type', type=str, choices=['openslide', 'image', 'cucim'], default=None,
                    help='Force the use of a specific WSI image reader. Options are ["openslide", "image", "cucim"]. Defaults to None (auto-determine which reader to use).')
    parser.add_argument("--search_nested", action="store_true",
                        help=("If set, recursively search for whole-slide images (WSIs) within all subdirectories of "
                              "`wsi_dir`. Uses `os.walk` to include slides from nested folders. "
                              "Defaults to False (only top-level slides are included)."))

    # Segmentation arguments 
    parser.add_argument('--segmenter', type=str, default='hest', 
                        choices=['hest', 'grandqc'], 
                        help='Type of tissue vs background segmenter model to use. Options are HEST or GrandQC.')
    parser.add_argument('--seg_conf_thresh', type=float, default=0.5, 
                    help='Confidence threshold to apply to binarize segmentation predictions. Lower this threshold to retain more tissue. Defaults to 0.5. Try 0.4 as 2nd option.')
    parser.add_argument('--remove_holes', action='store_true', default=False, 
                        help='If set, removes holes detected within tissue regions from the segmentation mask.')
    parser.add_argument('--remove_artifacts', action='store_true', default=False, 
                        help='If set, runs an additional GrandQC-based model to remove artifacts (including penmarks, blurs, stains, etc.) from the tissue segmentation.')
    parser.add_argument('--remove_penmarks', action='store_true', default=False, 
                        help='If set (and --remove_artifacts is not set), runs a specialized GrandQC-based model to remove only penmarks from the tissue segmentation.')
    parser.add_argument('--seg_batch_size', type=int, default=None, 
                        help='Batch size for segmentation. Defaults to None (use `batch_size` argument instead).')
    
    # Patching arguments
    parser.add_argument('--mag', type=int, choices=[5, 10, 20, 40, 80], default=20, 
                        help='Magnification level (e.g., 20 for 20x) at which to extract patches and features.')
    parser.add_argument('--patch_size', type=int, default=512, 
                        help='Side length of square patches in pixels at the specified magnification.')
    parser.add_argument('--overlap', type=int, default=0, 
                        help='Absolute overlap between adjacent patches in pixels (at the specified magnification). Defaults to 0.')
    parser.add_argument('--min_tissue_proportion', type=float, default=0., 
                        help='Minimum proportion of the patch area that must contain tissue to be kept. Between 0. and 1.0. Defaults to 0. (any tissue).')
    parser.add_argument('--coords_dir_name', type=str, default=None, # Changed from coords_dir
                        help='Name of the directory to save/restore tissue coordinates (relative to job_dir). If None, auto-generated.')
    
    # Feature extraction arguments 
    parser.add_argument('--patch_encoder', type=str, default='conch_v15', 
                        choices=[ # List all supported patch encoders from sauron.feature_extraction.models.patch_encoders.factory
                                 'conch_v1', 'uni_v1', 'uni_v2', 'ctranspath', 'phikon', 
                                 'resnet50', 'gigapath', 'virchow', 'virchow2', 
                                 'hoptimus0', 'hoptimus1', 'phikon_v2', 'conch_v15', 'musk', 'hibou_l',
                                 'kaiko-vits8', 'kaiko-vits16', 'kaiko-vitb8', 'kaiko-vitb16',
                                 'kaiko-vitl14', 'lunit-vits8', 'midnight12k'],
                        help='Patch encoder model to use for feature extraction.')
    parser.add_argument(
        '--patch_encoder_ckpt_path', type=str, default=None,
        help=(
            "Optional local path to a patch encoder checkpoint (.pt, .pth, .bin, or .safetensors). "
            "This overrides the default download mechanism and model registry. "
            "Useful for offline environments or custom checkpoints."
        )
    )
    parser.add_argument('--slide_encoder', type=str, default=None, 
                        choices=[ # List all supported slide encoders from sauron.feature_extraction.models.slide_encoders.factory
                                 'threads', 'titan', 'prism', 'gigapath', 'chief', 'madeleine',
                                 # Mean-pooling variants (derived from patch encoders)
                                 'mean-virchow', 'mean-virchow2', 'mean-conch_v1', 'mean-conch_v15', 'mean-ctranspath',
                                 'mean-gigapath', 'mean-resnet50', 'mean-hoptimus0', 'mean-phikon', 'mean-phikon_v2',
                                 'mean-musk', 'mean-uni_v1', 'mean-uni_v2', 'mean-hibou_l', 'mean-lunit-vits8', 'mean-midnight12k',
                                 'mean-kaiko-vits8', 'mean-kaiko-vits16', 'mean-kaiko-vitb8', 'mean-kaiko-vitb16', 'mean-kaiko-vitl14'
                                 ], 
                        help='Slide encoder model to use for feature extraction. If specified, will automatically extract required patch features.')
    parser.add_argument('--feat_batch_size', type=int, default=None, 
                        help='Batch size for feature extraction. Defaults to None (use `batch_size` argument instead).')
    
    return parser


def parse_arguments():
    return build_parser().parse_args()


def initialize_processor(args, wsi_dir_for_processor: str):
    """
    Initialize the Sauron Processor with arguments.
    `wsi_dir_for_processor` is typically `args.wsi_dir` or a batch cache directory.
    """
    return Processor(
        job_dir=args.job_dir,
        wsi_source=wsi_dir_for_processor, # This is the directory the processor will *read from*
        wsi_ext=args.wsi_ext,
        wsi_cache=args.wsi_cache,
        clear_cache=args.clear_cache,
        skip_errors=args.skip_errors,
        custom_mpp_keys=args.custom_mpp_keys,
        custom_list_of_wsis=args.custom_list_of_wsis,
        max_workers=args.max_workers,
        reader_type=args.reader_type,
        search_nested=args.search_nested,
    )


def run_task(processor: Processor, args):
    """
    Execute the specified task using the Sauron Processor.
    """

    device_str = f'cuda:{args.gpu}' if torch.cuda.is_available() else 'cpu'

    if args.task == 'seg':
        # Instantiate segmentation model and artifact remover if requested by user
        segmentation_model = segmentation_model_factory(
            args.segmenter,
            confidence_thresh=args.seg_conf_thresh,
        )
        artifact_remover_model = None
        if args.remove_artifacts:
            artifact_remover_model = segmentation_model_factory(
                'grandqc_artifact', # This model handles general artifacts
                # remove_penmarks_only=False (default behavior)
            )
        elif args.remove_penmarks: # Only if remove_artifacts is NOT set
            artifact_remover_model = segmentation_model_factory(
                'grandqc_artifact', 
                remove_penmarks_only=True # Specialized for penmarks
            )

        # run segmentation 
        processor.run_segmentation_job(
            segmentation_model=segmentation_model,
            seg_mag=segmentation_model.target_mag, # Use model's recommended target_mag
            holes_are_tissue= not args.remove_holes,
            artifact_remover_model=artifact_remover_model,
            batch_size=args.seg_batch_size if args.seg_batch_size is not None else args.batch_size,
            device=device_str,
        )
    elif args.task == 'coords':
        # Derive coords_dir_name if not provided
        coords_dir_name = args.coords_dir_name or f'{args.mag}x_{args.patch_size}px_{args.overlap}px_overlap'
        processor.run_patching_job(
            target_magnification=args.mag,
            patch_size=args.patch_size,
            overlap=args.overlap,
            patch_dir_name=coords_dir_name, # Pass as `patch_dir_name`
            min_tissue_proportion=args.min_tissue_proportion
        )
    elif args.task == 'feat':
        # Derive coords_dir_name if not provided, for feature extraction
        coords_dir_for_feat = args.coords_dir_name or f'{args.mag}x_{args.patch_size}px_{args.overlap}px_overlap'

        if args.slide_encoder is None: 
            # Patch Feature Extraction
            patch_encoder = patch_encoder_factory(args.patch_encoder, weights_path=args.patch_encoder_ckpt_path)
            processor.run_patch_feature_extraction_job(
                coords_h5_dir=os.path.join(args.job_dir, coords_dir_for_feat, 'patches'), # Path to the 'patches' subfolder
                patch_encoder=patch_encoder,
                device=device_str,
                saveas='h5', # Hardcoded to h5 for now, as it's common for MIL
                batch_limit=args.feat_batch_size if args.feat_batch_size is not None else args.batch_size,
            )
        else:
            # Slide Feature Extraction
            slide_encoder = slide_encoder_factory(args.slide_encoder)
            processor.run_slide_feature_extraction_job(
                coords_h5_dir=os.path.join(args.job_dir, coords_dir_for_feat), # Path to the job_dir/patch_dir_name
                slide_encoder=slide_encoder,
                device=device_str,
                saveas='h5', # Hardcoded to h5 for now
                batch_limit_for_patch_features=args.feat_batch_size if args.feat_batch_size is not None else args.batch_size,
            )
    elif args.task == 'cache':
        # In this mode, we only populate the cache, the main loop in main() handles it
        # The processor's populate_cache method will be called.
        processor.populate_cache()
    else:
        raise ValueError(f'Invalid task: {args.task}')


def main():

    args = parse_arguments()
    
    # === Handle Caching / Parallel processing ===
    if args.wsi_cache and args.task != 'cache': # If caching is enabled for processing tasks (not just populating)
        # We need to run a producer-consumer setup
        from multiprocessing import Lock
        # Lock is needed for batch_producer to prevent race conditions on shared data or output.
        # But for this simple scenario, queue communication suffices.
        
        queue = Queue(maxsize=1) # Queue to hold batch IDs for processing
        
        # Collect all valid slides first (from original source directory)
        all_valid_slides_from_source, all_valid_rel_paths_from_source, all_mpp_from_source_csv = \
            Processor(
                job_dir=args.job_dir,
                wsi_source=args.wsi_dir,
                wsi_ext=args.wsi_ext,
                custom_list_of_wsis=args.custom_list_of_wsis,
                search_nested=args.search_nested,
                max_workers=args.max_workers,
                reader_type=args.reader_type,
                wsi_cache=None, # Don't pass cache here, we are just collecting paths
                clear_cache=False, # Not relevant for path collection
                skip_errors=True, # Allow path collection to skip errors and report.
            ).wsis # Processor's __init__ collects and creates WSI objects

        # Extract just the original paths from the WSI objects
        original_wsi_paths = [w.original_path for w in all_valid_slides_from_source]

        logger.info(f"[MAIN] Found {len(original_wsi_paths)} valid slides in {args.wsi_dir}.")

        # Determine how many slides to warm up in the first batch
        num_warmup_slides = min(len(original_wsi_paths), args.cache_batch_size)
        warmup_slides = original_wsi_paths[:num_warmup_slides]

        # Explicitly warm up the first batch outside the producer thread
        warmup_dir = os.path.join(args.wsi_cache, "batch_0")
        logger.info(f"[MAIN] Warmup caching first batch: {warmup_dir}")
        batch_producer_processor = initialize_processor(args, args.wsi_dir) # Use original wsi_dir for producer's path collection
        batch_producer_processor.populate_cache(start_idx=0) # Only populate the first batch
        queue.put(0) # Put ID for first batch to trigger consumer

        # Factory function for consumer to create a Processor instance pointing to the batch cache
        def processor_factory_for_consumer(batch_local_dir: str) -> Processor:
            # Create a new argparse.Namespace object for the local processor instance
            local_args = argparse.Namespace(**vars(args))
            local_args.wsi_dir = batch_local_dir # Point processor to the local cache batch directory
            local_args.wsi_cache = None # Disable caching for this local processor
            local_args.custom_list_of_wsis = None # Custom list already filtered, now process locally
            local_args.search_nested = False # Already collected, no need to search nested locally
            local_args.clear_cache = args.clear_cache # Pass through clear_cache decision
            local_args.custom_mpp_keys = args.custom_mpp_keys # Pass through MPP keys
            local_args.reader_type = args.reader_type # Pass through reader type
            local_args.skip_errors = args.skip_errors # Pass through skip_errors
            local_args.max_workers = args.max_workers # Pass through max_workers
            
            # The WSI objects in this local processor will now be constructed from `batch_local_dir`
            # and automatically have their `original_path` set to the remote source by `Processor.__init__`
            # `Processor.__init__` also re-initializes WSI objects correctly, including potential MPP from CSV.
            return initialize_processor(local_args, wsi_dir_for_processor=batch_local_dir)

        # Function to run the desired task for the consumer
        def run_task_for_consumer_fn(processor_instance: Processor, task_name: str):
            # Temporarily set the task for the current run within this thread context
            original_task_arg = args.task
            args.task = task_name 
            try:
                run_task(processor_instance, args)
            finally:
                args.task = original_task_arg # Restore original task arg

        producer = Thread(target=batch_producer, args=(
            queue, original_wsi_paths, num_warmup_slides, args.cache_batch_size, args.wsi_cache
        ))

        consumer = Thread(target=batch_consumer, args=(
            queue, args.task, args.wsi_cache, processor_factory_for_consumer, run_task_for_consumer_fn
        ))

        logger.info("[MAIN] Starting producer and consumer threads for parallel processing.")
        producer.start()
        consumer.start()
        producer.join() # Wait for producer to finish (all slides copied)
        consumer.join() # Wait for consumer to finish (all slides processed)
        
    else:
        # === Sequential mode or cache-only task ===
        if args.task == 'cache':
            # In cache-only mode, we just populate the cache and exit.
            # No need for consumer/producer threads.
            processor_instance = initialize_processor(args, args.wsi_dir)
            processor_instance.populate_cache()
            logger.info("Cache population task completed.")
        else:
            # Run tasks sequentially
            processor_instance = initialize_processor(args, args.wsi_dir)
            tasks = ['seg', 'coords', 'feat'] if args.task == 'all' else [args.task]
            for task_name in tasks:
                args.task = task_name # Set current task
                run_task(processor_instance, args)
            logger.info("Sequential processing task completed.")


if __name__ == "__main__":
    main()

```
Directory structure:
└── sauron/
    ├── README.md
    ├── feature_extraction_plan
    ├── launch_job.sh
    ├── LICENSE
    ├── MANIFEST.in
    ├── requirements.txt
    ├── setup.py
    ├── train_mil.py
    └── sauron/
        ├── __init__.py
        ├── cli.py
        ├── requirements.txt
        ├── data/
        │   ├── classMILDataset.py
        │   ├── data_utils.py
        │   ├── dataset_factory.py
        │   └── survMILDataset.py
        ├── losses/
        │   └── surv_loss.py
        ├── mil_models/
        │   ├── ABMIL.py
        │   ├── activations.py
        │   ├── DiffABMIL.py
        │   ├── MambaMIL.py
        │   ├── MaxMIL.py
        │   ├── MeanMIL.py
        │   ├── models_factory.py
        │   ├── S4MIL.py
        │   ├── TransMIL.py
        │   ├── WIKGMIL.py
        │   └── mamba_ssm/
        │       ├── __init__.py
        │       ├── models/
        │       │   ├── __init__.py
        │       │   ├── config_mamba.py
        │       │   └── mixer_seq_simple.py
        │       ├── modules/
        │       │   ├── __init__.py
        │       │   ├── bimamba.py
        │       │   ├── mamba_simple.py
        │       │   └── srmamba.py
        │       ├── ops/
        │       │   ├── __init__.py
        │       │   ├── selective_scan_interface.py
        │       │   └── triton/
        │       │       ├── __init__.py
        │       │       ├── layernorm.py
        │       │       └── selective_state_update.py
        │       └── utils/
        │           ├── __init__.py
        │           └── generation.py
        ├── parse/
        │   └── cli_parsers.py
        ├── training/
        │   ├── __init__.py
        │   ├── cli_runner.py
        │   ├── lightning_module.py
        │   └── pipeline.py
        └── utils/
            ├── callbacks.py
            ├── drawing_utils.py
            ├── environment_setup.py
            ├── filehandler.py
            ├── generic_utils.py
            ├── metrics.py
            ├── optimizers.py
            └── process_args.py

================================================
FILE: README.md
================================================
# sauron
Multiple Instance Learning framework for multi-modal classification with histopathology data


================================================
FILE: launch_job.sh
================================================
#!/bin/bash
# This script launches the Sauron feature extraction job.

# Exit immediately if a command exits with a non-zero status.
# Treat unset variables as an error.
# The return value of a pipeline is the value of the last (rightmost) command to exit with a non-zero status, or zero if all commands in the pipeline exit successfully.
set -euo pipefail

# --- Configuration Variables ---
# Modify these variables according to your setup and desired job.

# No need for PYTHON_SCRIPT anymore, as we call the installed command.

# Directories and Paths
JOB_DIR="/media/mydrive/TCGA/dummy-tcga-brca/job_dir"
WSI_DIR="/media/mydrive/TCGA/dummy-tcga-brca/"
WSI_CACHE="" # Set to "" to disable WSI caching

# GPU & Task
CUDA_VISIBLE_DEVICES="0" # GPU ID(s) to use, e.g., "0" or "0,1"
TASK="all"               # seg, coords, feat, all, cache

# WSI Discovery and Reader
WSI_EXT=".svs"            # Space-separated list for multiple extensions: "svs tif ndpi"
CUSTOM_MPP_KEYS="mpp"    # Space-separated list for multiple keys: "mpp slide_mpp"
CUSTOM_LIST_OF_WSIS=""   # Path to a CSV file (e.g., "/path/to/my_wsis.csv"). Leave empty "" to use all WSIs in WSI_DIR.
READER_TYPE=""           # Force WSI reader: openslide, image, cucim. Leave empty "" for auto-determination.
SEARCH_NESTED="false"    # Set to "true" to recursively search WSI_DIR subdirectories

# Parallelization & Batching
MAX_WORKERS="16"          # Maximum number of workers. Set to "" for inferred (based on CPU cores).
BATCH_SIZE="64"          # General batch size. Can be overridden by seg_batch_size/feat_batch_size.

# Caching Options
CLEAR_CACHE="false"      # Set to "true" to delete cached WSIs after processing each batch.
CACHE_BATCH_SIZE="32"    # Max number of slides to cache locally at once when --wsi_cache is used.

# Error Handling
SKIP_ERRORS="false"      # Set to "true" to skip errored slides and continue processing.

# Segmentation Arguments
SEGMENTER="hest"         # hest, grandqc
SEG_CONF_THRESH="0.5"    # Confidence threshold for segmentation.
REMOVE_HOLES="false"     # Set to "true" to remove holes from segmentation mask.
REMOVE_ARTIFACTS="false" # Set to "true" to remove artifacts using GrandQC.
REMOVE_PENMARKS="false"  # Set to "true" to remove penmarks specifically (if REMOVE_ARTIFACTS is false).
SEG_BATCH_SIZE=""        # Batch size for segmentation. Set to "" to use BATCH_SIZE.

# Patching Arguments
MAG="20"                 # Magnification level for patching (e.g., 20 for 20x).
PATCH_SIZE="512"         # Side length of square patches in pixels.
OVERLAP="0"              # Absolute overlap between adjacent patches in pixels.
MIN_TISSUE_PROPORTION="0.0" # Minimum proportion of patch area that must contain tissue.
COORDS_DIR_NAME=""       # Name of directory to save/restore coordinates. Set to "" for auto-generated.

# Feature Extraction Arguments
PATCH_ENCODER="conch_v15"       # Patch encoder model (e.g., resnet50, conch_v15). See --help for choices.
PATCH_ENCODER_CKPT_PATH=""      # Local path to a custom checkpoint. Set to "" to use default registry.
SLIDE_ENCODER=""                # Slide encoder model (e.g., threads, mean-virchow). Set to "" to skip slide-level features.
FEAT_BATCH_SIZE=""              # Batch size for feature extraction. Set to "" to use BATCH_SIZE.

# --- Constructing the Command ---
declare -a ARGS

# Required arguments (or always included)
ARGS+=(
    "--job_dir" "$JOB_DIR"
    "--wsi_dir" "$WSI_DIR"
    "--task" "$TASK"
    "--batch_size" "$BATCH_SIZE"
    "--mag" "$MAG"
    "--patch_size" "$PATCH_SIZE"
    "--overlap" "$OVERLAP"
    "--min_tissue_proportion" "$MIN_TISSUE_PROPORTION"
    "--patch_encoder" "$PATCH_ENCODER"
    "--segmenter" "$SEGMENTER"
)

# Optional arguments (only add if their value is not empty or if boolean "true")
[ -n "$WSI_EXT" ] && ARGS+=("--wsi_ext" $WSI_EXT) # No quotes for $WSI_EXT here to allow shell word splitting for nargs
[ -n "$WSI_CACHE" ] && ARGS+=("--wsi_cache" "$WSI_CACHE")
[ "$CLEAR_CACHE" = "true" ] && ARGS+=("--clear_cache")
[ -n "$CACHE_BATCH_SIZE" ] && ARGS+=("--cache_batch_size" "$CACHE_BATCH_SIZE")
[ "$SKIP_ERRORS" = "true" ] && ARGS+=("--skip_errors")
[ -n "$CUSTOM_MPP_KEYS" ] && ARGS+=("--custom_mpp_keys" $CUSTOM_MPP_KEYS) # No quotes for $CUSTOM_MPP_KEYS to allow shell word splitting for nargs
[ -n "$CUSTOM_LIST_OF_WSIS" ] && ARGS+=("--custom_list_of_wsis" "$CUSTOM_LIST_OF_WSIS")
[ -n "$MAX_WORKERS" ] && ARGS+=("--max_workers" "$MAX_WORKERS")
[ "$SEARCH_NESTED" = "true" ] && ARGS+=("--search_nested")
[ -n "$READER_TYPE" ] && ARGS+=("--reader_type" "$READER_TYPE")

# Segmentation specific arguments
[ -n "$SEG_BATCH_SIZE" ] && ARGS+=("--seg_batch_size" "$SEG_BATCH_SIZE")
[ -n "$SEG_CONF_THRESH" ] && ARGS+=("--seg_conf_thresh" "$SEG_CONF_THRESH")
[ "$REMOVE_HOLES" = "true" ] && ARGS+=("--remove_holes")
[ "$REMOVE_ARTIFACTS" = "true" ] && ARGS+=("--remove_artifacts")
[ "$REMOVE_PENMARKS" = "true" ] && ARGS+=("--remove_penmarks")

# Patching/Coordinates specific arguments
[ -n "$COORDS_DIR_NAME" ] && ARGS+=("--coords_dir_name" "$COORDS_DIR_NAME")

# Feature extraction specific arguments
[ -n "$PATCH_ENCODER_CKPT_PATH" ] && ARGS+=("--patch_encoder_ckpt_path" "$PATCH_ENCODER_CKPT_PATH")
[ -n "$SLIDE_ENCODER" ] && ARGS+=("--slide_encoder" "$SLIDE_ENCODER")
[ -n "$FEAT_BATCH_SIZE" ] && ARGS+=("--feat_batch_size" "$FEAT_BATCH_SIZE")

# --- Execution ---
# Set the CUDA_VISIBLE_DEVICES environment variable
export CUDA_VISIBLE_DEVICES="$CUDA_VISIBLE_DEVICES"

echo "Starting Sauron feature extraction job via installed command..."
echo "Using GPU(s): $CUDA_VISIBLE_DEVICES"
echo "Job Directory: $JOB_DIR"
echo "WSI Directory: $WSI_DIR"
echo "Full command being executed:"
echo "sauron-extract ${ARGS[@]}"
echo "--------------------------------------------------"

# Execute the installed Sauron command with the constructed arguments
sauron-extract "${ARGS[@]}"

echo "--------------------------------------------------"
echo "Sauron Feature Extraction job completed."


================================================
FILE: LICENSE
================================================
# Attribution-NonCommercial-NoDerivatives 4.0 International

> *Creative Commons Corporation (“Creative Commons”) is not a law firm and does not provide legal services or legal advice. Distribution of Creative Commons public licenses does not create a lawyer-client or other relationship. Creative Commons makes its licenses and related information available on an “as-is” basis. Creative Commons gives no warranties regarding its licenses, any material licensed under their terms and conditions, or any related information. Creative Commons disclaims all liability for damages resulting from their use to the fullest extent possible.*
>
> ### Using Creative Commons Public Licenses
>
> Creative Commons public licenses provide a standard set of terms and conditions that creators and other rights holders may use to share original works of authorship and other material subject to copyright and certain other rights specified in the public license below. The following considerations are for informational purposes only, are not exhaustive, and do not form part of our licenses.
>
> * __Considerations for licensors:__ Our public licenses are intended for use by those authorized to give the public permission to use material in ways otherwise restricted by copyright and certain other rights. Our licenses are irrevocable. Licensors should read and understand the terms and conditions of the license they choose before applying it. Licensors should also secure all rights necessary before applying our licenses so that the public can reuse the material as expected. Licensors should clearly mark any material not subject to the license. This includes other CC-licensed material, or material used under an exception or limitation to copyright. [More considerations for licensors](http://wiki.creativecommons.org/Considerations_for_licensors_and_licensees#Considerations_for_licensors).
>
> * __Considerations for the public:__ By using one of our public licenses, a licensor grants the public permission to use the licensed material under specified terms and conditions. If the licensor’s permission is not necessary for any reason–for example, because of any applicable exception or limitation to copyright–then that use is not regulated by the license. Our licenses grant only permissions under copyright and certain other rights that a licensor has authority to grant. Use of the licensed material may still be restricted for other reasons, including because others have copyright or other rights in the material. A licensor may make special requests, such as asking that all changes be marked or described. Although not required by our licenses, you are encouraged to respect those requests where reasonable. [More considerations for the public](http://wiki.creativecommons.org/Considerations_for_licensors_and_licensees#Considerations_for_licensees).

## Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International Public License

By exercising the Licensed Rights (defined below), You accept and agree to be bound by the terms and conditions of this Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International Public License ("Public License"). To the extent this Public License may be interpreted as a contract, You are granted the Licensed Rights in consideration of Your acceptance of these terms and conditions, and the Licensor grants You such rights in consideration of benefits the Licensor receives from making the Licensed Material available under these terms and conditions.

### Section 1 – Definitions.

a. __Adapted Material__ means material subject to Copyright and Similar Rights that is derived from or based upon the Licensed Material and in which the Licensed Material is translated, altered, arranged, transformed, or otherwise modified in a manner requiring permission under the Copyright and Similar Rights held by the Licensor. For purposes of this Public License, where the Licensed Material is a musical work, performance, or sound recording, Adapted Material is always produced where the Licensed Material is synched in timed relation with a moving image.

b. __Copyright and Similar Rights__ means copyright and/or similar rights closely related to copyright including, without limitation, performance, broadcast, sound recording, and Sui Generis Database Rights, without regard to how the rights are labeled or categorized. For purposes of this Public License, the rights specified in Section 2(b)(1)-(2) are not Copyright and Similar Rights.

e. __Effective Technological Measures__ means those measures that, in the absence of proper authority, may not be circumvented under laws fulfilling obligations under Article 11 of the WIPO Copyright Treaty adopted on December 20, 1996, and/or similar international agreements.

f. __Exceptions and Limitations__ means fair use, fair dealing, and/or any other exception or limitation to Copyright and Similar Rights that applies to Your use of the Licensed Material.

h. __Licensed Material__ means the artistic or literary work, database, or other material to which the Licensor applied this Public License.

i. __Licensed Rights__ means the rights granted to You subject to the terms and conditions of this Public License, which are limited to all Copyright and Similar Rights that apply to Your use of the Licensed Material and that the Licensor has authority to license.

h. __Licensor__ means the individual(s) or entity(ies) granting rights under this Public License.

i. __NonCommercial__ means not primarily intended for or directed towards commercial advantage or monetary compensation. For purposes of this Public License, the exchange of the Licensed Material for other material subject to Copyright and Similar Rights by digital file-sharing or similar means is NonCommercial provided there is no payment of monetary compensation in connection with the exchange.

j. __Share__ means to provide material to the public by any means or process that requires permission under the Licensed Rights, such as reproduction, public display, public performance, distribution, dissemination, communication, or importation, and to make material available to the public including in ways that members of the public may access the material from a place and at a time individually chosen by them.

k. __Sui Generis Database Rights__ means rights other than copyright resulting from Directive 96/9/EC of the European Parliament and of the Council of 11 March 1996 on the legal protection of databases, as amended and/or succeeded, as well as other essentially equivalent rights anywhere in the world.

l. __You__ means the individual or entity exercising the Licensed Rights under this Public License. Your has a corresponding meaning.

### Section 2 – Scope.

a. ___License grant.___

   1. Subject to the terms and conditions of this Public License, the Licensor hereby grants You a worldwide, royalty-free, non-sublicensable, non-exclusive, irrevocable license to exercise the Licensed Rights in the Licensed Material to:

        A. reproduce and Share the Licensed Material, in whole or in part, for NonCommercial purposes only; and

        B. produce and reproduce, but not Share, Adapted Material for NonCommercial purposes only.

   2. __Exceptions and Limitations.__ For the avoidance of doubt, where Exceptions and Limitations apply to Your use, this Public License does not apply, and You do not need to comply with its terms and conditions.

   3. __Term.__ The term of this Public License is specified in Section 6(a).

   4. __Media and formats; technical modifications allowed.__ The Licensor authorizes You to exercise the Licensed Rights in all media and formats whether now known or hereafter created, and to make technical modifications necessary to do so. The Licensor waives and/or agrees not to assert any right or authority to forbid You from making technical modifications necessary to exercise the Licensed Rights, including technical modifications necessary to circumvent Effective Technological Measures. For purposes of this Public License, simply making modifications authorized by this Section 2(a)(4) never produces Adapted Material.

   5. __Downstream recipients.__

        A. __Offer from the Licensor – Licensed Material.__ Every recipient of the Licensed Material automatically receives an offer from the Licensor to exercise the Licensed Rights under the terms and conditions of this Public License.

        B. __No downstream restrictions.__ You may not offer or impose any additional or different terms or conditions on, or apply any Effective Technological Measures to, the Licensed Material if doing so restricts exercise of the Licensed Rights by any recipient of the Licensed Material.

   6. __No endorsement.__ Nothing in this Public License constitutes or may be construed as permission to assert or imply that You are, or that Your use of the Licensed Material is, connected with, or sponsored, endorsed, or granted official status by, the Licensor or others designated to receive attribution as provided in Section 3(a)(1)(A)(i).

b. ___Other rights.___

   1. Moral rights, such as the right of integrity, are not licensed under this Public License, nor are publicity, privacy, and/or other similar personality rights; however, to the extent possible, the Licensor waives and/or agrees not to assert any such rights held by the Licensor to the limited extent necessary to allow You to exercise the Licensed Rights, but not otherwise.

   2. Patent and trademark rights are not licensed under this Public License.

   3. To the extent possible, the Licensor waives any right to collect royalties from You for the exercise of the Licensed Rights, whether directly or through a collecting society under any voluntary or waivable statutory or compulsory licensing scheme. In all other cases the Licensor expressly reserves any right to collect such royalties, including when the Licensed Material is used other than for NonCommercial purposes.

### Section 3 – License Conditions.

Your exercise of the Licensed Rights is expressly made subject to the following conditions.

a. ___Attribution.___

   1. If You Share the Licensed Material, You must:

      A. retain the following if it is supplied by the Licensor with the Licensed Material:

         i. identification of the creator(s) of the Licensed Material and any others designated to receive attribution, in any reasonable manner requested by the Licensor (including by pseudonym if designated);

         ii. a copyright notice;

         iii. a notice that refers to this Public License;

         iv. a notice that refers to the disclaimer of warranties;

         v. a URI or hyperlink to the Licensed Material to the extent reasonably practicable;

      B. indicate if You modified the Licensed Material and retain an indication of any previous modifications; and

      C. indicate the Licensed Material is licensed under this Public License, and include the text of, or the URI or hyperlink to, this Public License.
 
        For the avoidance of doubt, You do not have permission under this Public License to Share Adapted Material.

   2. You may satisfy the conditions in Section 3(a)(1) in any reasonable manner based on the medium, means, and context in which You Share the Licensed Material. For example, it may be reasonable to satisfy the conditions by providing a URI or hyperlink to a resource that includes the required information.

   3. If requested by the Licensor, You must remove any of the information required by Section 3(a)(1)(A) to the extent reasonably practicable.

### Section 4 – Sui Generis Database Rights.

Where the Licensed Rights include Sui Generis Database Rights that apply to Your use of the Licensed Material:

a. for the avoidance of doubt, Section 2(a)(1) grants You the right to extract, reuse, reproduce, and Share all or a substantial portion of the contents of the database for NonCommercial purposes only and provided You do not Share Adapted Material;

b. if You include all or a substantial portion of the database contents in a database in which You have Sui Generis Database Rights, then the database in which You have Sui Generis Database Rights (but not its individual contents) is Adapted Material; and

c. You must comply with the conditions in Section 3(a) if You Share all or a substantial portion of the contents of the database.

For the avoidance of doubt, this Section 4 supplements and does not replace Your obligations under this Public License where the Licensed Rights include other Copyright and Similar Rights.

### Section 5 – Disclaimer of Warranties and Limitation of Liability.

a. __Unless otherwise separately undertaken by the Licensor, to the extent possible, the Licensor offers the Licensed Material as-is and as-available, and makes no representations or warranties of any kind concerning the Licensed Material, whether express, implied, statutory, or other. This includes, without limitation, warranties of title, merchantability, fitness for a particular purpose, non-infringement, absence of latent or other defects, accuracy, or the presence or absence of errors, whether or not known or discoverable. Where disclaimers of warranties are not allowed in full or in part, this disclaimer may not apply to You.__

b. __To the extent possible, in no event will the Licensor be liable to You on any legal theory (including, without limitation, negligence) or otherwise for any direct, special, indirect, incidental, consequential, punitive, exemplary, or other losses, costs, expenses, or damages arising out of this Public License or use of the Licensed Material, even if the Licensor has been advised of the possibility of such losses, costs, expenses, or damages. Where a limitation of liability is not allowed in full or in part, this limitation may not apply to You.__

c. The disclaimer of warranties and limitation of liability provided above shall be interpreted in a manner that, to the extent possible, most closely approximates an absolute disclaimer and waiver of all liability.

### Section 6 – Term and Termination.

a. This Public License applies for the term of the Copyright and Similar Rights licensed here. However, if You fail to comply with this Public License, then Your rights under this Public License terminate automatically.

b. Where Your right to use the Licensed Material has terminated under Section 6(a), it reinstates:

   1. automatically as of the date the violation is cured, provided it is cured within 30 days of Your discovery of the violation; or

   2. upon express reinstatement by the Licensor.

   For the avoidance of doubt, this Section 6(b) does not affect any right the Licensor may have to seek remedies for Your violations of this Public License.

c. For the avoidance of doubt, the Licensor may also offer the Licensed Material under separate terms or conditions or stop distributing the Licensed Material at any time; however, doing so will not terminate this Public License.

d. Sections 1, 5, 6, 7, and 8 survive termination of this Public License.

### Section 7 – Other Terms and Conditions.

a. The Licensor shall not be bound by any additional or different terms or conditions communicated by You unless expressly agreed.

b. Any arrangements, understandings, or agreements regarding the Licensed Material not stated herein are separate from and independent of the terms and conditions of this Public License.

### Section 8 – Interpretation.

a. For the avoidance of doubt, this Public License does not, and shall not be interpreted to, reduce, limit, restrict, or impose conditions on any use of the Licensed Material that could lawfully be made without permission under this Public License.

b. To the extent possible, if any provision of this Public License is deemed unenforceable, it shall be automatically reformed to the minimum extent necessary to make it enforceable. If the provision cannot be reformed, it shall be severed from this Public License without affecting the enforceability of the remaining terms and conditions.

c. No term or condition of this Public License will be waived and no failure to comply consented to unless expressly agreed to by the Licensor.

d. Nothing in this Public License constitutes or may be interpreted as a limitation upon, or waiver of, any privileges and immunities that apply to the Licensor or You, including from the legal processes of any jurisdiction or authority.

> Creative Commons is not a party to its public licenses. Notwithstanding, Creative Commons may elect to apply one of its public licenses to material it publishes and in those instances will be considered the “Licensor.” Except for the limited purpose of indicating that material is shared under a Creative Commons public license or as otherwise permitted by the Creative Commons policies published at [creativecommons.org/policies](http://creativecommons.org/policies), Creative Commons does not authorize the use of the trademark “Creative Commons” or any other trademark or logo of Creative Commons without its prior written consent including, without limitation, in connection with any unauthorized modifications to any of its public licenses or any other arrangements, understandings, or agreements concerning use of licensed material. For the avoidance of doubt, this paragraph does not form part of the public licenses.
>
> Creative Commons may be contacted at [creativecommons.org](http://creativecommons.org).


================================================
FILE: MANIFEST.in
================================================
include LICENSE
include README.md
include requirements.txt
recursive-include sauron *.json


================================================
FILE: requirements.txt
================================================
termcolor



================================================
FILE: setup.py
================================================
# setup.py (at the root of your project)
import os
import re

import setuptools

# --- Package Metadata ---
NAME = "sauron"
DESCRIPTION = "A comprehensive deep learning framework for Whole Slide Image analysis, including feature extraction and MIL training."
URL = "https://github.com/iucomppath/sauron"  # Replace with your project's GitHub URL
AUTHOR = "Siddhesh Thakur"  # Replace with your name
AUTHOR_EMAIL = "sid.cre8er@gmail.com"  # Replace with your email
LICENSE = (
    "Apache-2.0"  # Or whatever license you are using (e.g., Apache-2.0, BSD-3-Clause)
)


# --- Version Management ---
# Load the version from sauron/__init__.py
def get_version():
    """Reads the version from sauron/__init__.py without importing the package."""
    version_file = os.path.join(os.path.dirname(__file__), NAME, "__init__.py")
    with open(version_file, "r", encoding="utf-8") as f:
        version_match = re.search(r"^__version__ = ['\"]([^'\"]*)['\"]", f.read(), re.M)
        if version_match:
            return version_match.group(1)
        raise RuntimeError("Unable to find version string.")


VERSION = get_version()

# --- Long Description ---
# Read the long description from README.md
try:
    with open("README.md", "r", encoding="utf-8") as fh:
        LONG_DESCRIPTION = fh.read()
except FileNotFoundError:
    LONG_DESCRIPTION = DESCRIPTION  # Fallback if README.md is not found


# --- Dependencies ---
# Read requirements from requirements.txt
def get_requirements(filename="requirements.txt"):
    """Reads the list of requirements from a file."""
    requirements_path = os.path.join(os.path.dirname(__file__), filename)
    with open(requirements_path, "r", encoding="utf-8") as f:
        # Filter out comments and empty lines
        return [line.strip() for line in f if line.strip() and not line.startswith("#")]


INSTALL_REQUIRES = get_requirements()

# --- Setup Configuration ---
setuptools.setup(
    name=NAME,
    version=VERSION,
    author=AUTHOR,
    author_email=AUTHOR_EMAIL,
    description=DESCRIPTION,
    long_description=LONG_DESCRIPTION,
    long_description_content_type="text/markdown",
    url=URL,
    license=LICENSE,
    # Automatically find all packages under the 'sauron' directory
    packages=setuptools.find_packages(
        exclude=["tests*", "docs*"]
    ),  # Exclude test/doc directories
    python_requires=">=3.8",  # Specify minimum Python version
    install_requires=INSTALL_REQUIRES,
    # Include non-Python files specified in MANIFEST.in
    include_package_data=True,
    # Define command-line entry points
    entry_points={
        "console_scripts": [
            # These now point to the functions within sauron/cli.py
            "sauron-extract = sauron.cli:feature_extract_main",
            "sauron-train = sauron.cli:train_mil_main",
        ],
    },
    # Classifiers help users find your project on PyPI and understand its compatibility.
    classifiers=[
        # Development Status
        "Development Status :: 3 - Alpha",  # Or 4 - Beta, 5 - Production/Stable
        # Intended Audience
        "Intended Audience :: Science/Research",
        "Intended Audience :: Developers",
        # License
        f"License :: OSI Approved :: {LICENSE} License",
        # Programming Language
        "Programming Language :: Python :: 3",
        "Programming Language :: Python :: 3.8",
        "Programming Language :: Python :: 3.9",
        "Programming Language :: Python :: 3.10",
        "Programming Language :: Python :: 3.11",
        "Programming Language :: Python :: 3.12",
        # Topic
        "Topic :: Scientific/Engineering :: Medical Science Apps.",
        "Topic :: Scientific/Engineering :: Image Processing",
        "Topic :: Scientific/Engineering :: Artificial Intelligence",
        "Operating System :: OS Independent",
    ],
    keywords=[
        "whole slide imaging",
        "wsi",
        "pathology",
        "deep learning",
        "feature extraction",
        "multiple instance learning",
        "pytorch",
        "cancer research",
        "histopathology",
    ],
)



================================================
FILE: train_mil.py
================================================
import argparse
import json
import os

import pandas as pd
from torch.utils.tensorboard import SummaryWriter

from sauron.data.dataset_factory import determine_split_directory, get_data_manager
from sauron.parse.cli_parsers import get_args
from sauron.training.pipeline import train_fold
from sauron.utils.environment_setup import (
    create_results_directory,
    log_experiment_details,
    seed_everything,
    setup_device,
)
from sauron.utils.generic_utils import (
    log_results,
    save_pkl,
)


def run_experiment_folds(
    data_manager,
    args: argparse.Namespace,
    experiment_main_results_dir: str,
) -> pd.DataFrame:
    """
    Manages the k-fold cross-validation loop.
    """
    if args.task_type.lower() == "classification":
        metric_keys = [
            "test_auc",
            "val_auc",
            "test_acc",
            "val_acc",
        ]
    elif args.task_type.lower() == "survival":
        metric_keys = [
            "test_c_index",
            "val_c_index",
        ]
    else:
        raise ValueError(f"Unknown task_type: {args.task_type} for defining metrics.")

    all_fold_metrics = {key: [] for key in metric_keys}

    if args.task_type.lower() == "classification":
        data_manager.create_k_fold_splits(
            num_folds=args.k, test_set_size=getattr(args, "test_frac", 0.1)
        )
        num_actual_folds = data_manager.get_number_of_folds()
        if num_actual_folds == 0 and args.k > 0:
            print(
                "No K-folds generated (num_folds=0 in DataManager), running as single train/test split if test_frac > 0."
            )
            if args.k <= 1:
                loop_range = range(1)
                print(f"Running a single train/val/test split (args.k={args.k}).")
            else:
                raise ValueError(
                    f"args.k={args.k} but DataManager created 0 folds. Check data or split logic."
                )
        else:
            loop_range = range(args.k_start, min(args.k_end, num_actual_folds))

    elif args.task_type.lower() == "survival":
        data_manager.create_splits_from_generating_function(
            k=args.k,
            val_num=getattr(args, "val_num_survival", (0.15, 0.15)),
            test_num=getattr(args, "test_num_survival", (0.15, 0.15)),
            label_frac=getattr(args, "label_frac", 1.0),
            custom_test_ids=getattr(args, "custom_test_ids", None),
        )
        loop_range = range(args.k_start, args.k_end)
    else:
        raise ValueError(f"Task type {args.task_type} split creation not defined.")

    for i in loop_range:
        print(f"\n{'=' * 10} Processing Fold: {i} {'=' * 10}")

        if args.task_type.lower() == "classification":
            data_manager.set_current_fold(fold_index=i)
        elif args.task_type.lower() == "survival":
            if not data_manager.set_next_fold_from_generator(
                start_from_fold=i if i == args.k_start else None
            ):
                print(
                    f"SurvivalDataManager's split generator exhausted before reaching fold {i}."
                )
                break

        mil_dataset_params = {
            "backbone": args.backbone,
            "patch_size": args.patch_size,
            "use_hdf5": getattr(args, "use_hdf5", False),
            "cache_enabled": getattr(args, "preloading", "no").lower() == "yes",
        }

        if args.task_type.lower() == "survival":
            mil_dataset_params["mode"] = getattr(args, "survival_mode", "pathomic")

        train_dataset, val_dataset, test_dataset = data_manager.get_mil_datasets(
            **mil_dataset_params
        )

        if getattr(args, "preloading", "no").lower() == "yes":
            print(f"Preloading data for fold {i}...")
            if train_dataset:
                train_dataset.preload_data()
            if val_dataset:
                val_dataset.preload_data()
            if test_dataset:
                test_dataset.preload_data()

        if getattr(args, "save_splits", False):
            split_file = os.path.join(
                args.split_dir_determined, f"fold_{i}_patient_ids.csv"
            )
            data_manager.save_current_split_patient_ids(split_file)

        fold_results_dir = os.path.join(experiment_main_results_dir, f"fold_{i}")
        os.makedirs(fold_results_dir, exist_ok=True)

        fold_results_tuple = train_fold(
            train_dataset=train_dataset,
            val_dataset=val_dataset,
            test_dataset=test_dataset,
            cur_fold_num=i,
            args=args,
            experiment_base_results_dir=fold_results_dir,
        )

        patient_level_results_dict, *fold_metrics_values = fold_results_tuple

        if len(fold_metrics_values) != len(metric_keys):
            raise ValueError(
                f"Mismatch in metrics from train_fold ({len(fold_metrics_values)}) vs expected ({len(metric_keys)}). "
                f"Task: {args.task_type}. Returned: {fold_metrics_values}"
            )

        for key_idx, metric_value in enumerate(fold_metrics_values):
            metric_name = metric_keys[key_idx]
            all_fold_metrics[metric_name].append(metric_value)

        if patient_level_results_dict:
            fold_results_pkl_path = os.path.join(
                fold_results_dir, "patient_level_results.pkl"
            )
            save_pkl(fold_results_pkl_path, patient_level_results_dict)
            print(
                f"Saved patient-level results for fold {i} to {fold_results_pkl_path}"
            )

    num_folds_run = (
        len(all_fold_metrics[metric_keys[0]])
        if metric_keys and all_fold_metrics[metric_keys[0]]
        else 0
    )
    if num_folds_run == 0:
        print("No folds were successfully processed. Returning empty DataFrame.")
        return pd.DataFrame()

    fold_num_series = list(range(args.k_start, args.k_start + num_folds_run))
    return pd.DataFrame({"fold_num": fold_num_series, **all_fold_metrics})


def main_experiment_runner(args: argparse.Namespace):
    """
    Main function to run the entire experiment.
    """
    _ = setup_device()
    seed_everything(args.seed)

    experiment_main_results_dir = create_results_directory(
        args.results_dir, args.exp_code, args.seed
    )
    args.results_dir = experiment_main_results_dir
    log_experiment_details(args, experiment_main_results_dir)

    args.split_dir_determined = determine_split_directory(
        getattr(args, "split_dir_base", None),
        args.task_name,
        getattr(args, "label_frac", 1.0),
        args.k > 1,
    )
    os.makedirs(args.split_dir_determined, exist_ok=True)

    with SummaryWriter(
        log_dir=os.path.join(experiment_main_results_dir, "summary_all_folds")
    ) as summary_writer:
        args.k_start = max(0, args.k_start)
        args.k_end = args.k if args.k_end == -1 or args.k_end > args.k else args.k_end

        if args.k_start >= args.k_end and args.k > 0:
            print(
                f"Warning: k_start ({args.k_start}) is >= k_end ({args.k_end}). No folds will be run."
            )
            return

        manager_params = {
            "task_name": args.task_name,
            "task_type": args.task_type,
            "csv_path": args.dataset_csv,
            "data_directory": args.data_root_dir,
            "seed": args.seed,
            "verbose": getattr(args, "verbose_data", True),
            "label_column": getattr(args, "label_col", "label"),
            "patient_id_col_name": getattr(args, "patient_id_col", "case_id"),
            "slide_id_col_name": getattr(args, "slide_id_col", "slide_id"),
            "filter_criteria": json.loads(args.filter_criteria)
            if hasattr(args, "filter_criteria") and args.filter_criteria
            else None,
            "ignore_labels": args.ignore_labels.split(",")
            if hasattr(args, "ignore_labels") and args.ignore_labels
            else None,
            "patient_label_aggregation": getattr(
                args, "patient_label_aggregation", "max"
            ),
            "shuffle": getattr(args, "shuffle_data", False),
            "time_column": getattr(args, "time_col", None),
            "event_column": getattr(args, "event_col", None),
            "n_bins": getattr(args, "n_bins_survival", 4),
            "filter_dict": json.loads(args.filter_dict_survival)
            if hasattr(args, "filter_dict_survival") and args.filter_dict_survival
            else None,
            "omic_csv_path": getattr(args, "omic_csv", None),
            "omic_patient_id_col": getattr(args, "omic_patient_id_col", "case_id"),
            "apply_sig": getattr(args, "apply_sig_survival", False),
            "signatures_csv_path": getattr(args, "signatures_csv", None),
            "shuffle_slide_data": getattr(args, "shuffle_data_survival", False),
        }

        label_mapping_str = getattr(args, "label_mapping", None)
        if label_mapping_str:
            try:
                manager_params["label_mapping"] = json.loads(label_mapping_str)
            except json.JSONDecodeError:
                raise ValueError(
                    f"Invalid JSON string for label_mapping: {label_mapping_str}"
                )
        else:
            manager_params["label_mapping"] = None

        data_manager_instance = get_data_manager(**manager_params)

        args.n_classes = data_manager_instance.num_classes
        print(f"DataManager initialized. Number of classes: {args.n_classes}")
        if args.n_classes == 0 and args.task_type.lower() == "classification":
            print(
                "Warning: Number of classes is 0 for classification task. Check data and label mapping."
            )

        print(
            f"Starting experiment: {args.exp_code} with up to {args.k} folds (from {args.k_start} to {args.k_end - 1})"
        )
        overall_results_df = run_experiment_folds(
            data_manager_instance, args, experiment_main_results_dir
        )

        if not overall_results_df.empty:
            log_results(overall_results_df, args, summary_writer)
            print("\nAggregated Results over Folds:")
            print(overall_results_df)
        else:
            print("No results to log as no folds were processed.")


if __name__ == "__main__":
    args = get_args()  # Parse arguments

    if not hasattr(args, "task_name"):
        args.task_name = args.task
    if not hasattr(args, "k_fold"):
        args.k_fold = args.k

    main_experiment_runner(args)
    print("Experiment Finished!")



================================================
FILE: sauron/__init__.py
================================================
# sauron/__init__.py
__version__ = "0.1.0"  # Or any initial version



================================================
FILE: sauron/cli.py
================================================
# sauron/cli.py
import sys

# Import the refactored main functions from their new locations
from sauron.feature_extraction.cli_runner import run_feature_extraction_job

# IMPORTANT: Ensure sauron/parse/argparse.py is renamed to sauron/parse/cli_parsers.py
from sauron.parse.cli_parsers import (
    get_mil_args,
    parse_feature_extraction_arguments,
)
from sauron.training.cli_runner import run_mil_training_job

# def feature_extract_main():
#     """
#     Entry point for the 'sauron-extract' command.
#     Parses arguments and runs the feature extraction pipeline.
#     """
#     args = parse_feature_extraction_arguments()
#     print(f"Launching Sauron Feature Extraction with arguments: {args}")
#     run_feature_extraction_job(args)
#     print("Sauron Feature Extraction job completed.")


def train_mil_main():
    """
    Entry point for the 'sauron-train' command.
    Parses arguments and runs the MIL training pipeline.
    """
    args = get_mil_args()
    # The `train_mil.py` script included some argument compatibility logic,
    # replicate it here if `get_mil_args` doesn't fully handle it.
    if not hasattr(args, "task_name"):
        args.task_name = args.task
    if not hasattr(args, "k_fold"):
        args.k_fold = args.k

    print(f"Launching Sauron MIL Training with arguments: {args}")
    run_mil_training_job(args)
    print("Sauron MIL Training job completed.")


if __name__ == "__main__":
    # This block allows for direct testing of `sauron/cli.py` during development.
    # In a real installation, setuptools handles calling `feature_extract_main` or `train_mil_main`.

    # Example for direct testing:
    # Set sys.argv to simulate command-line arguments
    # sys.argv = ["cli.py", "feature_extract", "--job_dir", "./test_job", "--wsi_dir", "./test_wsi", "--task", "cache", "--gpu", "0"]
    # feature_extract_main()

    # sys.argv = ["cli.py", "train_mil", "--task", "my_cls_task", "--task_type", "classification", "--exp_code", "test_exp", "--data_root_dir", "./data", "--dataset_csv", "./data/dataset.csv"]
    # train_mil_main()

    print("To run, install the package and use 'sauron-extract' or 'sauron-train'.")
    print("For help, run 'sauron-extract --help' or 'sauron-train --help'.")



================================================
FILE: sauron/requirements.txt
================================================
# requirements.txt

# Core ML
torch>=2.0.0,<2.1.0 # Specify a range known to work with mamba-ssm/causal-conv1d
torchvision
torchaudio

# Mamba Dependencies (ensure mamba-ssm is installed separately as per its instructions)
# pip install causal-conv1d==1.1.1
# pip install packaging
# cd mamba && pip install .
einops
ninja # Often needed for C++ extensions
packaging
transformers # Used by mamba utils/hf.py and benchmarks

# Data & Metrics
numpy
pandas>=2.0.0,<2.3.0
scikit-learn
scikit-survival==0.22.2
lifelines # Used in original MambaMIL README

# Utilities
tqdm
h5py

# Logging & Visualization (Optional but recommended)
tensorboard
tensorboardX
matplotlib
seaborn
# wandb # Uncomment if using Weights & Biases

openslide-python
Pillow
opencv-python-headless
geopandas
shapely
pyvips

# Other potential dependencies based on imports observed
opt_einsum # Used in S4MIL


================================================
FILE: sauron/data/classMILDataset.py
================================================
from __future__ import annotations

import os
from typing import Dict, List, Optional, Tuple, Union

import h5py
import numpy as np
import pandas as pd
import torch
from scipy.stats import mode
from sklearn.model_selection import StratifiedKFold, train_test_split
from torch.utils.data import Dataset


class ClassificationDataManager:
    def __init__(
        self,
        csv_path: str,
        data_directory: Union[str, Dict[str, str]],
        label_column: str = "label",  # Actual column name in CSV for labels
        label_mapping: Optional[Dict[str, int]] = None,
        patient_id_col_name: str = "case_id",  # Actual col name in CSV for patient ID
        slide_id_col_name: str = "slide_id",  # Actual col name in CSV for slide ID
        shuffle: bool = False,
        random_seed: int = 7,
        verbose: bool = True,
        filter_criteria: Optional[Dict[str, List[str]]] = None,
        ignore_labels: Optional[List[str]] = None,  # Original string labels to ignore
        patient_stratification: bool = False,  # If true, __len__ uses patient_data
        patient_label_aggregation: str = "max",  # 'max' or 'majority'
    ):
        self.csv_path = csv_path
        self.data_directory = data_directory
        self.provided_label_column = label_column
        self.label_mapping = label_mapping or {}
        self.patient_id_col_name = patient_id_col_name
        self.slide_id_col_name = slide_id_col_name
        self.random_seed = random_seed
        self.verbose = verbose
        self.patient_stratification = (
            patient_stratification  # Used by __len__ if this class were a Dataset
        )

        self.train_patient_ids: Optional[List[str]] = None
        self.val_patient_ids: Optional[List[str]] = None
        self.test_patient_ids: Optional[List[str]] = None

        self.train_slide_indices: Optional[List[int]] = None
        self.val_slide_indices: Optional[List[int]] = None
        self.test_slide_indices: Optional[List[int]] = None

        self.kfold_splits: Optional[List[Tuple[np.ndarray, np.ndarray]]] = None
        self.train_val_patient_data_for_kfold: Optional[pd.DataFrame] = None

        # Load and preprocess slide data
        try:
            raw_slide_data = pd.read_csv(csv_path)
        except FileNotFoundError as e:
            raise FileNotFoundError(f"CSV file not found: {csv_path}") from e

        # Standardize column names internally
        self.slide_data = self._rename_cols(raw_slide_data)

        self.slide_data = self._filter_data(self.slide_data, filter_criteria)
        self.slide_data = self._prepare_labels(self.slide_data, ignore_labels)

        if not self.label_mapping:  # Infer mapping if not provided
            unique_labels = self.slide_data["label_str"].unique()
            self.label_mapping = {
                label: i for i, label in enumerate(sorted(unique_labels))
            }
            if verbose:
                print(f"Inferred label_mapping: {self.label_mapping}")

        # Apply the final mapping after potential inference
        self.slide_data["label"] = self.slide_data["label_str"].map(self.label_mapping)
        # Drop rows where mapping resulted in NaN (e.g. label_str not in inferred map)
        self.slide_data.dropna(subset=["label"], inplace=True)
        self.slide_data["label"] = self.slide_data["label"].astype(int)

        self.num_classes = len(set(self.label_mapping.values()))
        if self.num_classes == 0 and len(self.slide_data) > 0:
            raise ValueError(
                "Number of classes is 0. Check label_column and label_mapping. "
                f"Unique labels found: {self.slide_data['label_str'].unique()}"
            )

        if shuffle:
            self.slide_data = self.slide_data.sample(
                frac=1, random_state=random_seed
            ).reset_index(drop=True)

        self._aggregate_patient_data(patient_label_aggregation)
        self._prepare_class_indices()

        if verbose:
            self._print_summary()

    def _rename_cols(self, df: pd.DataFrame) -> pd.DataFrame:
        df = df.copy()
        if (
            self.patient_id_col_name != "case_id"
            and self.patient_id_col_name in df.columns
        ):
            df.rename(columns={self.patient_id_col_name: "case_id"}, inplace=True)
        if (
            self.slide_id_col_name != "slide_id"
            and self.slide_id_col_name in df.columns
        ):
            df.rename(columns={self.slide_id_col_name: "slide_id"}, inplace=True)
        if "case_id" not in df.columns:
            raise ValueError(
                f"Patient ID column '{self.patient_id_col_name}' (expected 'case_id') not found in CSV."
            )
        if "slide_id" not in df.columns:
            raise ValueError(
                f"Slide ID column '{self.slide_id_col_name}' (expected 'slide_id') not found in CSV."
            )
        return df

    def _filter_data(
        self, data: pd.DataFrame, filter_criteria: Optional[Dict[str, List[str]]] = None
    ) -> pd.DataFrame:
        if filter_criteria:
            mask = pd.Series(True, index=data.index)
            for column, values in filter_criteria.items():
                if column not in data.columns:
                    print(f"Warning: Filter column '{column}' not found in data.")
                    continue
                mask &= data[column].isin(values)
            data = data[mask].reset_index(drop=True)
        return data

    def _prepare_labels(
        self, data: pd.DataFrame, ignore_labels_str: Optional[List[str]]
    ) -> pd.DataFrame:
        data = data.copy()
        if self.provided_label_column not in data.columns:
            raise ValueError(
                f"Provided label column '{self.provided_label_column}' not found in CSV."
            )

        # Keep original string labels in 'label_str' for mapping and ignoring
        data["label_str"] = data[self.provided_label_column].astype(str)

        if ignore_labels_str:
            data = data[~data["label_str"].isin(ignore_labels_str)].reset_index(
                drop=True
            )

        # Integer mapping will be applied after potential inference in __init__
        return data

    def _aggregate_patient_data(self, aggregation_method: str = "max") -> None:
        if (
            "case_id" not in self.slide_data.columns
            or "label" not in self.slide_data.columns
        ):
            print(
                "Warning: 'case_id' or 'label' not fully prepared for patient aggregation."
            )
            self.patient_data = pd.DataFrame(columns=["case_id", "label"])
            return

        patients = self.slide_data["case_id"].unique()
        patient_labels_agg = []

        for patient_id in patients:
            labels = self.slide_data.loc[
                self.slide_data["case_id"] == patient_id, "label"
            ].values
            if len(labels) == 0:
                continue  # Should not happen if patient_id is from slide_data

            if aggregation_method == "max":
                aggregated_label = labels.max()
            elif aggregation_method == "majority":
                aggregated_label = mode(labels, keepdims=True).mode[0]
            else:
                raise ValueError(
                    f"Invalid patient_label_aggregation: {aggregation_method}"
                )
            patient_labels_agg.append(aggregated_label)

        self.patient_data = pd.DataFrame(
            {"case_id": patients, "label": patient_labels_agg}
        )

    def _prepare_class_indices(self) -> None:
        if self.num_classes > 0:
            self.patient_class_indices = [
                np.where(self.patient_data["label"] == cls_label)[0]
                for cls_label in range(self.num_classes)
            ]
            self.slide_class_indices = [
                np.where(self.slide_data["label"] == cls_label)[0]
                for cls_label in range(self.num_classes)
            ]
        else:
            self.patient_class_indices = []
            self.slide_class_indices = []

    def _print_summary(self) -> None:
        print("--- Classification DataManager Summary ---")
        print(f"CSV Path: {self.csv_path}")
        print(f"Label Column (original): {self.provided_label_column}")
        print(f"Patient ID Column (original): {self.patient_id_col_name}")
        print(f"Slide ID Column (original): {self.slide_id_col_name}")
        print(f"Label Mapping: {self.label_mapping}")
        print(f"Number of Classes: {self.num_classes}")
        print(f"Total unique slides: {self.slide_data['slide_id'].nunique()}")
        print(f"Total unique patients: {self.patient_data['case_id'].nunique()}")

        if self.num_classes > 0:
            print("\nSlide-level counts (after mapping):")
            print(self.slide_data["label"].value_counts(sort=False))
            print("\nPatient-level counts (after aggregation and mapping):")
            print(self.patient_data["label"].value_counts(sort=False))
        else:
            print("\nNo classes defined or no data loaded.")
        print("----------------------------------------")

    def create_k_fold_splits(
        self, num_folds: int = 5, test_set_size: float = 0.1
    ) -> None:
        if self.patient_data.empty:
            raise ValueError("Patient data is empty. Cannot create splits.")

        # Create initial test set from patients
        if test_set_size > 0:
            train_val_patients_df, test_patients_df = train_test_split(
                self.patient_data,
                test_size=test_set_size,
                stratify=self.patient_data["label"],
                random_state=self.random_seed,
            )
            self.test_patient_ids = test_patients_df["case_id"].tolist()
        else:
            train_val_patients_df = self.patient_data.copy()
            self.test_patient_ids = []

        if num_folds > 0 and not train_val_patients_df.empty:
            skf = StratifiedKFold(
                n_splits=num_folds, shuffle=True, random_state=self.random_seed
            )
            # Ensure indices are reset for iloc to work correctly with skf.split
            self.train_val_patient_data_for_kfold = train_val_patients_df.reset_index(
                drop=True
            )
            self.kfold_splits = list(
                skf.split(
                    self.train_val_patient_data_for_kfold,
                    self.train_val_patient_data_for_kfold["label"],
                )
            )
        else:
            # If no k-folds, all train_val_patients become train, and val is empty
            self.train_patient_ids = train_val_patients_df["case_id"].tolist()
            self.val_patient_ids = []
            self.kfold_splits = None  # Explicitly set to None

        if self.verbose:
            print(
                f"Created {num_folds}-fold splits with test set size {test_set_size}."
            )
            print(f"Total patients for K-Fold Train/Val: {len(train_val_patients_df)}")
            print(f"Total patients for Test: {len(self.test_patient_ids)}")

    def set_current_fold(self, fold_index: int = 0) -> None:
        if (
            self.kfold_splits is None and self.train_patient_ids is not None
        ):  # Test set only or no kfold case
            # Test patient IDs are already set in create_k_fold_splits
            # Train/Val patient IDs are also set if no kfold
            pass  # Patient IDs are already set
        elif self.kfold_splits is None or self.train_val_patient_data_for_kfold is None:
            raise ValueError(
                "K-Fold splits have not been created. Call create_k_fold_splits() first."
            )
        elif fold_index >= len(self.kfold_splits):
            raise ValueError(
                f"Fold index {fold_index} is out of bounds for {len(self.kfold_splits)} folds."
            )
        else:
            train_patient_indices_in_kfold_df, val_patient_indices_in_kfold_df = (
                self.kfold_splits[fold_index]
            )

            train_patients_df = self.train_val_patient_data_for_kfold.iloc[
                train_patient_indices_in_kfold_df
            ]
            val_patients_df = self.train_val_patient_data_for_kfold.iloc[
                val_patient_indices_in_kfold_df
            ]

            self.train_patient_ids = train_patients_df["case_id"].tolist()
            self.val_patient_ids = val_patients_df["case_id"].tolist()
            # self.test_patient_ids was set during create_k_fold_splits

        # Map patient IDs to slide indices
        self.train_slide_indices = (
            self.slide_data[
                self.slide_data["case_id"].isin(self.train_patient_ids)
            ].index.tolist()
            if self.train_patient_ids
            else []
        )
        self.val_slide_indices = (
            self.slide_data[
                self.slide_data["case_id"].isin(self.val_patient_ids)
            ].index.tolist()
            if self.val_patient_ids
            else []
        )
        self.test_slide_indices = (
            self.slide_data[
                self.slide_data["case_id"].isin(self.test_patient_ids)
            ].index.tolist()
            if self.test_patient_ids
            else []
        )

        if self.verbose:
            print(f"Set to fold {fold_index}:")
            print(
                f"  Train patients: {len(self.train_patient_ids)}, Train slides: {len(self.train_slide_indices)}"
            )
            print(
                f"  Val patients: {len(self.val_patient_ids)}, Val slides: {len(self.val_slide_indices)}"
            )
            print(
                f"  Test patients: {len(self.test_patient_ids)}, Test slides: {len(self.test_slide_indices)}"
            )

    def get_mil_datasets(
        self,
        backbone: str,
        patch_size: str = "",
        use_hdf5: bool = False,
        cache_enabled: bool = False,
    ) -> Tuple[
        Optional[WSIMILDataset], Optional[WSIMILDataset], Optional[WSIMILDataset]
    ]:
        if (
            self.train_slide_indices is None
            or self.val_slide_indices is None
            or self.test_slide_indices is None
        ):
            raise ValueError(
                "Splits/fold not set. Call create_k_fold_splits() and then set_current_fold() first."
            )

        train_df_split = self.slide_data.iloc[self.train_slide_indices].reset_index(
            drop=True
        )
        val_df_split = self.slide_data.iloc[self.val_slide_indices].reset_index(
            drop=True
        )
        test_df_split = self.slide_data.iloc[self.test_slide_indices].reset_index(
            drop=True
        )

        common_params = {
            "data_directory": self.data_directory,
            "num_classes": self.num_classes,
            "backbone": backbone,
            "patch_size": patch_size,
            "use_hdf5": use_hdf5,
            "cache_enabled": cache_enabled,
        }

        train_dataset = (
            WSIMILDataset(slide_data_df=train_df_split, **common_params)
            if not train_df_split.empty
            else None
        )
        val_dataset = (
            WSIMILDataset(slide_data_df=val_df_split, **common_params)
            if not val_df_split.empty
            else None
        )
        test_dataset = (
            WSIMILDataset(slide_data_df=test_df_split, **common_params)
            if not test_df_split.empty
            else None
        )

        return train_dataset, val_dataset, test_dataset

    def get_number_of_folds(self) -> int:
        return len(self.kfold_splits) if self.kfold_splits is not None else 0

    def save_current_split_patient_ids(self, filename: str) -> None:
        if (
            self.train_patient_ids is None
            or self.val_patient_ids is None
            or self.test_patient_ids is None
        ):
            raise ValueError("Splits not set. Call set_current_fold() first.")

        # Pad lists to the same length for DataFrame creation
        max_len = max(
            len(self.train_patient_ids or []),
            len(self.val_patient_ids or []),
            len(self.test_patient_ids or []),
        )

        train_ids_padded = (self.train_patient_ids or []) + [None] * (
            max_len - len(self.train_patient_ids or [])
        )
        val_ids_padded = (self.val_patient_ids or []) + [None] * (
            max_len - len(self.val_patient_ids or [])
        )
        test_ids_padded = (self.test_patient_ids or []) + [None] * (
            max_len - len(self.test_patient_ids or [])
        )

        splits_df = pd.DataFrame(
            {
                "train_patient_id": train_ids_padded,
                "val_patient_id": val_ids_padded,
                "test_patient_id": test_ids_padded,
            }
        )
        splits_df.to_csv(filename, index=False)
        if self.verbose:
            print(f"Current split patient IDs saved to {filename}")

    def summarize_current_splits(
        self, return_summary_df: bool = False
    ) -> Optional[pd.DataFrame]:
        if self.train_slide_indices is None:
            print("No split set. Call set_current_fold() first.")
            return None

        summary_data = {}
        class_map_inv = {v: k for k, v in self.label_mapping.items()}

        for split_name, slide_indices in [
            ("train", self.train_slide_indices),
            ("val", self.val_slide_indices),
            ("test", self.test_slide_indices),
        ]:
            if slide_indices is None:
                continue
            labels = self.slide_data.loc[slide_indices, "label"]
            counts = labels.value_counts().sort_index()
            summary_data[split_name] = {
                class_map_inv.get(cls_idx, f"UnknownClass_{cls_idx}"): count
                for cls_idx, count in counts.items()
            }
            if self.verbose:
                print(f"\n--- {split_name.upper()} Split Summary ---")
                print(f"Number of slides: {len(slide_indices)}")
                print(
                    f"Number of patients: {len(self.slide_data.loc[slide_indices, 'case_id'].unique())}"
                )
                for cls_idx, count in counts.items():
                    class_name = class_map_inv.get(cls_idx, f"UnknownClass_{cls_idx}")
                    print(f"  Class {cls_idx} ({class_name}): {count} slides")

        if return_summary_df:
            df = pd.DataFrame(summary_data).fillna(0).astype(int)
            return df
        return None


class WSIMILDataset(Dataset):
    def __init__(
        self,
        slide_data_df: pd.DataFrame,
        data_directory: Union[str, Dict[str, str]],
        num_classes: int,  # For consistency, though label is in slide_data_df
        backbone: Optional[str] = None,
        patch_size: str = "",
        use_hdf5: bool = False,
        cache_enabled: bool = False,
    ):
        self.slide_data = slide_data_df  # DataFrame for this specific split
        self.data_directory = data_directory
        self.num_classes = num_classes
        self.backbone = backbone
        self.patch_size = (
            str(patch_size) if patch_size is not None else ""
        )  # Ensure string
        self.use_hdf5 = use_hdf5
        self.cache_enabled = cache_enabled
        self.data_cache: Dict[str, torch.Tensor] = {}

        if not self.use_hdf5 and not self.backbone:
            print(
                "Warning: WSIMILDataset initialized for .pt files but backbone is not set. Call set_backbone()."
            )

    def __len__(self) -> int:
        return len(self.slide_data)

    def __getitem__(
        self, idx: int
    ) -> Union[Tuple[torch.Tensor, int], Tuple[torch.Tensor, int, np.ndarray]]:
        row = self.slide_data.iloc[idx]
        slide_id = row["slide_id"]
        label = row["label"]  # Assumes 'label' is already integer mapped

        current_data_dir_path: str
        if isinstance(self.data_directory, dict):
            source_col = "source"  # This column must exist in slide_data_df if data_directory is a dict
            if source_col not in row.index:
                raise ValueError(
                    f"'{source_col}' column missing in slide_data for multi-source data_directory. Slide ID: {slide_id}"
                )
            source = row[source_col]
            if source not in self.data_directory:
                raise ValueError(
                    f"Source '{source}' for slide '{slide_id}' not found in data_directory keys: {list(self.data_directory.keys())}"
                )
            current_data_dir_path = self.data_directory[source]
        else:  # data_directory is a string
            current_data_dir_path = self.data_directory

        if not os.path.isdir(current_data_dir_path):
            raise FileNotFoundError(
                f"Data directory for slide {slide_id} not found: {current_data_dir_path}"
            )

        if not self.use_hdf5:
            if not self.backbone:
                raise ValueError(
                    "Backbone must be set for loading .pt files. Call set_backbone()."
                )

            # Handle cases like patch_size='512' or patch_size='' for root, vs. patch_size='256' for subdir
            patch_subdir = ""
            if (
                self.patch_size and self.patch_size != "512"
            ):  # Assuming '512' means no subdir or handled differently
                patch_subdir = self.patch_size

            # Construct path: data_dir / (optional_patch_subdir) / pt_files / backbone / slide_id.pt
            file_path = os.path.join(
                current_data_dir_path,
                patch_subdir,
                "pt_files",
                self.backbone,
                f"{slide_id}.pt",
            )

            cached_features = self.data_cache.get(file_path)
            if cached_features is not None:
                return cached_features, label

            try:
                features = torch.load(file_path)
                if self.cache_enabled:
                    self.data_cache[file_path] = features
                return features, label
            except FileNotFoundError as e:
                new_error_msg = f"Feature file not found: {file_path}. Check slide_id, backbone, patch_size, and data_directory structure."
                print(
                    f"Details: Slide ID='{slide_id}', Backbone='{self.backbone}', Patch Size='{self.patch_size}', Dir='{current_data_dir_path}'"
                )
                raise FileNotFoundError(new_error_msg) from e
            except Exception as e:
                raise RuntimeError(f"Error loading {file_path}: {e}") from e
        else:
            file_path = os.path.join(
                current_data_dir_path, "h5_files", f"{slide_id}.h5"
            )
            # HDF5 typically isn't cached in memory this way due to size, but could be if small
            try:
                with h5py.File(file_path, "r") as hdf5_file:
                    features = torch.from_numpy(hdf5_file["features"][:])
                    # Coordinates are optional, decide if they are always needed
                    if "coords" in hdf5_file:
                        coordinates = hdf5_file["coords"][:]
                        return features, label, coordinates
                    return features, label
            except OSError as e:
                raise OSError(f"HDF5 file not found or corrupted: {file_path}") from e

    def set_backbone(self, backbone: str) -> None:
        if self.verbose:
            print(f"Setting backbone for MILDataset: {backbone}")
        self.backbone = backbone

    def set_patch_size(self, size: Union[str, int]) -> None:
        if self.verbose:
            print(f"Setting patch size for MILDataset: {size}")
        self.patch_size = str(size)  # Ensure string

    def load_from_hdf5(self, use_hdf5: bool) -> None:
        self.use_hdf5 = use_hdf5

    def preload_data(self, num_threads: int = 8) -> None:
        if not self.cache_enabled:
            print(
                "Warning: Preloading data but cache_enabled is False. Data will not be stored in memory."
            )
            self.cache_enabled = True  # Enable it for preloading

        print(f"Preloading {len(self)} items into cache using {num_threads} threads...")
        from multiprocessing.pool import ThreadPool

        indices = list(range(len(self)))
        with ThreadPool(num_threads) as pool:
            pool.map(self.__getitem__, indices)
        print("Preloading complete.")

    @property
    def verbose(
        self,
    ):  # Simple way to make it accessible if needed, or pass as init arg
        return True  # Or control via an __init__ param



================================================
FILE: sauron/data/data_utils.py
================================================
import collections
import math
import os
from typing import Iterator, List, Optional, Tuple, Union

import numpy as np
import pandas as pd
import torch
import torch.nn as nn  # Not used in this snippet, but kept if other utils in file use it
import torch.optim as optim  # Not used in this snippet
from torch.utils.data import (
    DataLoader,
    Dataset,  # Added for type hinting
    RandomSampler,
    Sampler,
    SequentialSampler,
    WeightedRandomSampler,
)


def make_weights_for_balanced_classes(
    dataset: Dataset,  # Use the base torch Dataset for broader compatibility
    # Assumes dataset has:
    # 1. A way to get all labels: e.g., dataset.get_all_labels() -> List[int]
    # OR 2. Direct access to labels: e.g., dataset.labels or dataset.slide_data['label']
    # OR 3. `get_label(idx)` method as in your original (less efficient for all labels)
) -> torch.Tensor:
    """
    Creates weights for WeightedRandomSampler for handling class imbalance.
    This version is more flexible.

    Args:
        dataset: A PyTorch Dataset instance. It needs a way to access all labels.
                 Common patterns:
                 - `dataset.labels` (a list or tensor of all labels)
                 - `dataset.slide_data['label']` (if slide_data is a pandas DataFrame)
                 - A method like `dataset.get_all_labels()`

    Returns:
        torch.Tensor: A 1D tensor of weights for each sample in the dataset.
    """
    # Try to get labels in a more efficient way
    all_labels: Optional[List[int]] = None
    if hasattr(dataset, "labels") and isinstance(
        dataset.labels, (list, np.ndarray, torch.Tensor)
    ):
        all_labels = list(dataset.labels)
    elif (
        hasattr(dataset, "slide_data")
        and isinstance(dataset.slide_data, pd.DataFrame)
        and "label" in dataset.slide_data.columns
    ):
        all_labels = dataset.slide_data["label"].tolist()
    elif hasattr(dataset, "get_all_labels") and callable(dataset.get_all_labels):
        all_labels = dataset.get_all_labels()
    elif hasattr(dataset, "getlabel") and callable(
        dataset.getlabel
    ):  # Fallback to less efficient original
        print(
            "Warning: Using inefficient `getlabel(idx)` for each sample to compute weights. "
            "Consider adding a `labels` attribute or `get_all_labels()` method to your dataset for performance."
        )
        all_labels = [dataset.getlabel(i) for i in range(len(dataset))]
    else:
        raise AttributeError(
            "Dataset does not have a recognized way to access all labels "
            "(e.g., 'labels' attribute, 'slide_data[\"label\"]', "
            "or 'get_all_labels()' / 'getlabel()' method)."
        )

    if not all_labels:  # Should be caught by AttributeError, but as a safeguard
        raise ValueError("Could not extract labels from the dataset.")

    label_counts = collections.Counter(all_labels)
    num_samples = len(all_labels)

    if not label_counts:  # No labels or empty dataset
        return torch.ones(num_samples, dtype=torch.double)

    # Calculate weight for each class: N / (num_classes * count_for_that_class)
    # This is a common formula. Original: N_total_classes / count_for_that_class
    # Let's stick to a more standard approach: 1. / count_for_that_class then assign
    # Or total_samples / (num_distinct_classes * count_of_class_i)

    # Simpler: weight = 1.0 / count_of_sample's_class
    # Then WeightedRandomSampler handles it.
    # Or, if we want to give more weight to RARE classes:
    # weight_per_class = {label: num_samples / count for label, count in label_counts.items()}

    # The original implementation implied:
    # weight_per_class[class_idx] = N_total_unique_classes / num_samples_in_class_idx
    # This means dataset.slide_cls_ids was [[indices_for_class_0], [indices_for_class_1], ...]
    # Let's try to replicate that logic if `slide_cls_ids` is available, otherwise use Counter.

    if hasattr(dataset, "slide_cls_ids") and dataset.slide_cls_ids is not None:
        # This was the original logic structure
        num_distinct_classes = len(dataset.slide_cls_ids)  # N in original code
        if num_distinct_classes == 0:  # No classes defined in slide_cls_ids
            print("Warning: dataset.slide_cls_ids is empty. Returning uniform weights.")
            return torch.ones(num_samples, dtype=torch.double)

        weight_per_class_val = [0.0] * num_distinct_classes
        for i, cls_ids in enumerate(dataset.slide_cls_ids):
            if len(cls_ids) > 0:
                weight_per_class_val[i] = float(num_distinct_classes) / len(cls_ids)
            else:  # Class has no samples
                weight_per_class_val[i] = 0  # Or some other handling for empty classes

        # Check if getlabel exists before using it.
        if not hasattr(dataset, "getlabel") or not callable(dataset.getlabel):
            raise AttributeError(
                "Dataset has 'slide_cls_ids' but no 'getlabel(idx)' method "
                "to map sample index to class label for weighting."
            )

        weights = [
            weight_per_class_val[dataset.getlabel(idx)] for idx in range(num_samples)
        ]

    else:  # Fallback to using all_labels and Counter if slide_cls_ids not present
        print(
            "Warning: `dataset.slide_cls_ids` not found or is None. "
            "Calculating weights based on overall label counts."
        )
        if not label_counts:  # Already checked but good to be safe
            return torch.ones(num_samples, dtype=torch.double)

        # Standard approach: weight for a class is 1 / (number of samples in that class)
        # Then, for each sample, assign the weight of its class.
        # More robust against missing classes in label_counts if all_labels has them.
        max_label = max(all_labels) if all_labels else -1
        weight_per_class_val = [0.0] * (max_label + 1)
        for label, count in label_counts.items():
            if count > 0:
                # weight_per_class_val[label] = 1.0 / count # Basic
                weight_per_class_val[label] = num_samples / (
                    len(label_counts) * count
                )  # Another common one

        weights = [weight_per_class_val[label] for label in all_labels]

    return torch.tensor(weights, dtype=torch.double)


class SubsetSequentialSampler(Sampler[int]):  # Use Generic type hint
    """Samples elements sequentially from a given list of indices, always in the same order.

    Args:
        indices (List[int]): a sequence of indices
    """

    def __init__(self, indices: List[int]):
        if not isinstance(indices, list):
            raise TypeError("indices should be a list.")
        if not all(isinstance(i, int) for i in indices):
            raise TypeError("all elements in indices should be integers.")

        self.indices = indices

    def __iter__(self) -> Iterator[int]:
        return iter(self.indices)

    def __len__(self) -> int:
        return len(self.indices)


def collate_mil_features(
    batch: List[
        Tuple[torch.Tensor, int]
    ],  # Assumes batch item is (features_tensor, label_int)
) -> Tuple[torch.Tensor, torch.Tensor]:
    """
    Collate function for MIL when batch items are (Tensor_Features, Label_Int).
    Concatenates all feature tensors (bags) along dimension 0.
    Labels are converted to a LongTensor.
    """
    # Filter out None items if any dataset returns None (e.g. for failed loads, though ideally handled in Dataset)
    # batch = [item for item in batch if item is not None and item[0] is not None]
    # if not batch:
    #     # Handle empty batch case, e.g. return empty tensors or raise error
    #     # This depends on how the training loop handles it.
    #     # For now, assume batch is never empty after filtering.
    #     # If it can be, the caller (DataLoader) might need a custom batch_sampler.
    #     return torch.empty(0), torch.empty(0, dtype=torch.long)

    try:
        # Assuming item[0] is a tensor of features for one WSI (bag of instances)
        # These features are typically [Number_of_Patches, Feature_Dimension]
        # Concatenating them makes [Total_Patches_in_Batch, Feature_Dimension]
        # This is standard for some MIL approaches where the model processes all patches from batch.
        features = torch.cat([item[0] for item in batch], dim=0)
        labels = torch.tensor([item[1] for item in batch], dtype=torch.long)
    except Exception as e:
        print("Error during collation (collate_mil_features):")
        for i, item in enumerate(batch):
            print(
                f"  Item {i}: type={type(item)}, len={len(item) if isinstance(item, (tuple,list)) else 'N/A'}"
            )
            if isinstance(item, (tuple, list)) and len(item) > 0:
                print(
                    f"    Item[0] type: {type(item[0])}, shape: {item[0].shape if hasattr(item[0], 'shape') else 'N/A'}"
                )
                print(f"    Item[1] type: {type(item[1])}")
        raise RuntimeError(f"Collation failed: {e}") from e

    return features, labels


def collate_mil_survival(
    batch: List[
        Tuple[
            torch.Tensor, Union[torch.Tensor, Tuple[torch.Tensor, ...]], int, float, int
        ]
    ],
    # Expected item: (path_features, omic_features, combined_label, event_time, censorship_status)
    # omic_features can be a single Tensor or a Tuple of Tensors (for coattn)
) -> Tuple[
    torch.Tensor,
    Union[torch.Tensor, Tuple[torch.Tensor, ...]],
    torch.Tensor,
    torch.Tensor,
    torch.Tensor,
]:
    """
    Collate function for Survival MIL tasks.
    Handles path features, omic features (single or tuple for coattn), and survival labels.
    """
    path_features_list = [
        item[0] for item in batch if item[0].numel() > 0
    ]  # Filter empty path features
    omic_features_list = [
        item[1] for item in batch
    ]  # Keep all, model must handle empty omics

    combined_labels = torch.tensor([item[2] for item in batch], dtype=torch.long)
    event_times = torch.tensor([item[3] for item in batch], dtype=torch.float32)
    censorship_statuses = torch.tensor([item[4] for item in batch], dtype=torch.int)

    # Collate path features (bag of instances)
    collated_path_features = (
        torch.cat(path_features_list, dim=0) if path_features_list else torch.empty(0)
    )

    # Collate omic features
    collated_omic_features: Union[torch.Tensor, Tuple[torch.Tensor, ...]]
    if isinstance(omic_features_list[0], torch.Tensor):  # All omics are single tensors
        # Stack if they are per-patient vectors, cat if they are bags of omic instances (unlikely here)
        # Assuming omic_features_list[i] is [omic_feature_dim] for patient i
        collated_omic_features = (
            torch.stack(omic_features_list, dim=0)
            if omic_features_list
            else torch.empty(0)
        )
    elif isinstance(
        omic_features_list[0], tuple
    ):  # Omic features are tuples (e.g., for CoAttn)
        # Transpose the list of tuples: [(o1a,o2a,o3a), (o1b,o2b,o3b)] -> [(o1a,o1b), (o2a,o2b), (o3a,o3b)]
        num_omic_modalities = len(omic_features_list[0])
        collated_omic_modalities = []
        for i in range(num_omic_modalities):
            modality_tensors = [
                omic_tuple[i]
                for omic_tuple in omic_features_list
                if omic_tuple[i].numel() > 0
            ]
            if modality_tensors:
                collated_omic_modalities.append(torch.stack(modality_tensors, dim=0))
            else:  # All patients had empty tensor for this modality
                collated_omic_modalities.append(torch.empty(0))
        collated_omic_features = tuple(collated_omic_modalities)
    else:
        raise TypeError(
            f"Unsupported omic feature type in batch: {type(omic_features_list[0])}"
        )

    return (
        collated_path_features,
        collated_omic_features,
        combined_labels,
        event_times,
        censorship_statuses,
    )


def get_dataloader(  # Renamed from get_split_loader for generality
    dataset: Dataset,  # Use base torch Dataset
    batch_size: int = 1,  # Default MIL batch_size is 1 (one WSI per "batch")
    shuffle: bool = False,  # For training, typically True
    use_weighted_sampler: bool = False,  # If True, enables weighted sampling for imbalance
    num_workers: int = 4,
    pin_memory: bool = True,
    collate_fn_type: str = "classification",  # "classification" or "survival"
    # Removed `testing` flag, as subset sampling is usually for debugging/prototyping
    # and can be handled by passing a Subset of the dataset if needed.
    # device: Optional[torch.device] = None, # Not strictly needed for DataLoader creation
) -> DataLoader:
    """
    Creates a PyTorch DataLoader for a given dataset.

    Args:
        dataset: The PyTorch Dataset instance.
        batch_size: How many samples per batch to load. For MIL, often 1.
        shuffle: Set to True to have the data reshuffled at every epoch (usually for training).
        use_weighted_sampler: If True, uses WeightedRandomSampler for class balancing.
                              Requires `make_weights_for_balanced_classes` to work with the dataset.
        num_workers: How many subprocesses to use for data loading.
        pin_memory: If True, copies Tensors into CUDA pinned memory before returning them.
        collate_fn_type: Specifies the type of collate function to use.
                         "classification" for standard (features, label) items.
                         "survival" for (path_feat, omic_feat, comb_label, time, event) items.

    Returns:
        torch.DataLoader: The configured DataLoader.
    """
    # Determine if CUDA is available and adjust defaults if necessary
    on_gpu = torch.cuda.is_available()
    current_num_workers = (
        num_workers if on_gpu else 0
    )  # Often set to 0 for CPU for simplicity
    current_pin_memory = pin_memory if on_gpu else False

    sampler: Optional[Sampler] = None
    # Shuffle and weighted_sampler are mutually exclusive with explicitly provided sampler.
    # WeightedRandomSampler implies random sampling.
    if use_weighted_sampler:
        if shuffle is False:  # WeightedRandomSampler inherently shuffles.
            print(
                "Warning: `use_weighted_sampler=True` implies shuffling. `shuffle=False` will be ignored."
            )
        try:
            weights = make_weights_for_balanced_classes(dataset)
            # num_samples for WeightedRandomSampler is how many samples to draw in an epoch
            # Usually len(dataset) to see each sample (on average) once.
            sampler = WeightedRandomSampler(
                weights, num_samples=len(dataset), replacement=True
            )
            shuffle = False  # Sampler handles shuffling, so DataLoader's shuffle should be False
        except (AttributeError, ValueError) as e:
            print(
                f"Could not create weighted sampler: {e}. Falling back to standard RandomSampler if shuffle=True."
            )
            if shuffle:
                sampler = RandomSampler(dataset)
            else:
                sampler = SequentialSampler(
                    dataset
                )  # Should not happen if shuffle=True was intended
    elif shuffle:
        sampler = RandomSampler(dataset)
    else:  # No shuffle, no weighted sampling
        sampler = SequentialSampler(dataset)

    # Select collate_fn based on type
    if collate_fn_type.lower() == "classification":
        collate_function = collate_mil_features
    elif collate_fn_type.lower() == "survival":
        collate_function = collate_mil_survival
    else:
        raise ValueError(
            f"Unknown collate_fn_type: {collate_fn_type}. "
            "Choose 'classification' or 'survival'."
        )

    return DataLoader(
        dataset,
        batch_size=batch_size,
        sampler=sampler,
        collate_fn=collate_function,
        num_workers=current_num_workers,
        pin_memory=current_pin_memory,
        drop_last=False,  # Typically False for MIL unless batch_size > 1 and partial batches are an issue
    )



================================================
FILE: sauron/data/dataset_factory.py
================================================
import os
from typing import Any, Dict, Optional, Union

# Assuming your new dataset classes are in these locations
from sauron.data.classMILDataset import ClassificationDataManager
from sauron.data.survMILDataset import SurvivalDataManager

# Keep supported tasks as in your original, or manage elsewhere
SUPPORTED_TASKS = ["TGCT", "BRCA", "COAD", "UCEC", "LUAD"]


def get_data_manager(
    task_name: str,
    task_type: str,
    csv_path: str,
    data_directory: Union[str, Dict[str, str]],
    seed: int = 7,
    verbose: bool = True,
    **kwargs: Any,  # Catch-all for other specific params
) -> Union[ClassificationDataManager, SurvivalDataManager]:
    """
    Factory function to create a DataManager instance for a given task and type.

    Args for Classification (via kwargs):
        label_column (str): Name of the column in CSV containing original labels.
        label_mapping (Dict[str, int]): Mapping from string labels to integer classes.
        patient_id_col_name (str): Column name for patient IDs. Default 'case_id'.
        slide_id_col_name (str): Column name for slide IDs. Default 'slide_id'.
        filter_criteria (Dict): For filtering rows in CSV.
        ignore_labels (List[str]): String labels to ignore.
        patient_label_aggregation (str): 'max' or 'majority'.
        shuffle (bool): Whether to shuffle loaded data. Default False. (for ClassificationDataManager)

    Args for Survival (via kwargs):
        time_column (str): Name of the column for survival time.
        event_column (str): Name of the column for event status (0=censored, 1=event).
        patient_id_col_name (str): Column name for patient IDs. Default 'case_id'.
        slide_id_col_name (str): Column name for slide IDs. Default 'slide_id'.
        n_bins (int): Number of bins for discretizing survival time. Default 4.
        filter_dict (Dict): For filtering rows in CSV.
        omic_csv_path (str): Path to CSV with omic features.
        omic_patient_id_col (str): Patient ID column in omic CSV.
        apply_sig (bool): For coattn mode signatures.
        signatures_csv_path (str): Path to signatures CSV for coattn.
        shuffle_slide_data (bool): Shuffle initial slide data. Default False. (for SurvivalDataManager)
    """
    task_name_upper = task_name.upper()
    if task_name_upper not in SUPPORTED_TASKS:
        # Consider if this check is still needed if task_type drives logic
        print(
            f"Warning: Task '{task_name}' not in predefined SUPPORTED_TASKS, but proceeding based on task_type."
        )

    if not os.path.exists(csv_path):
        raise FileNotFoundError(f"Dataset CSV file not found: {csv_path}")

    if task_type.lower() == "classification":
        # Extract classification-specific args from kwargs with defaults
        cls_kwargs = {
            "label_column": kwargs.get("label_column", "label"),
            "label_mapping": kwargs.get("label_mapping"),  # Allow None, DM can infer
            "patient_id_col_name": kwargs.get("patient_id_col_name", "case_id"),
            "slide_id_col_name": kwargs.get("slide_id_col_name", "slide_id"),
            "shuffle": kwargs.get(
                "shuffle", False
            ),  # Specific to ClassificationDataManager's initial load
            "filter_criteria": kwargs.get("filter_criteria"),
            "ignore_labels": kwargs.get("ignore_labels"),
            "patient_stratification": kwargs.get(
                "patient_stratification", False
            ),  # Legacy, not directly used by DM for len
            "patient_label_aggregation": kwargs.get("patient_label_aggregation", "max"),
        }
        return ClassificationDataManager(
            csv_path=csv_path,
            data_directory=data_directory,
            random_seed=seed,
            verbose=verbose,
            **cls_kwargs,
        )
    elif task_type.lower() == "survival":
        # Extract survival-specific args from kwargs with defaults
        surv_kwargs = {
            "time_column": kwargs.get("time_column"),
            "event_column": kwargs.get("event_column"),
            "patient_id_col_name": kwargs.get("patient_id_col_name", "case_id"),
            "slide_id_col_name": kwargs.get("slide_id_col_name", "slide_id"),
            "n_bins": kwargs.get("n_bins", 4),
            "shuffle_slide_data": kwargs.get(
                "shuffle_slide_data", False
            ),  # Specific to SurvivalDataManager
            "filter_dict": kwargs.get("filter_dict"),
            "eps": kwargs.get("eps", 1e-6),
            "omic_csv_path": kwargs.get("omic_csv_path"),
            "omic_patient_id_col": kwargs.get("omic_patient_id_col", "case_id"),
            "apply_sig": kwargs.get("apply_sig", False),
            "signatures_csv_path": kwargs.get("signatures_csv_path"),
        }
        if not surv_kwargs["time_column"] or not surv_kwargs["event_column"]:
            raise ValueError(
                "time_column and event_column must be provided for survival tasks."
            )

        return SurvivalDataManager(
            csv_path=csv_path,
            data_directory=data_directory,
            random_seed=seed,
            verbose=verbose,
            **surv_kwargs,
        )
    else:
        raise ValueError(
            f"Unsupported task_type: '{task_type}'. Must be 'classification' or 'survival'."
        )


def determine_split_directory(  # This function seems more for organizing output/split files
    base_split_dir: Optional[str],
    task_name: str,
    label_frac: float = 1.0,
    k_fold: bool = True,
) -> str:
    """
    Determines a directory path for storing/loading data splits or results.
    (This might be used externally to the DataManager for saving outputs).
    """
    if base_split_dir is None:
        # Example: splits/TGCT/label_frac_100_kfold or splits/TGCT/label_frac_10_holdout
        split_subdir = f"{task_name.upper()}"
        # Suffix based on k_fold and label_frac (if not full dataset)
        suffix = "_kfold" if k_fold else "_holdout"
        if label_frac < 1.0:  # Only add label_frac if it's a subset
            split_subdir += f"_label_frac_{int(label_frac * 100)}"

        determined_dir = os.path.join("results_or_splits", split_subdir + suffix)
    else:
        determined_dir = base_split_dir

    # Ensure directory exists if it's for saving
    # os.makedirs(determined_dir, exist_ok=True) # Uncomment if you want auto-creation
    return determined_dir



================================================
FILE: sauron/data/survMILDataset.py
================================================
from __future__ import annotations

import os
from typing import Dict, List, Optional, Tuple, Union

import h5py  # For potential H5 feature loading
import numpy as np
import pandas as pd
import torch
from sklearn.model_selection import train_test_split  # Using this for simple splits
from sklearn.preprocessing import StandardScaler
from torch.utils.data import Dataset

# Assuming generate_split and nth are from utils.utils as in original
# If these are complex, they might need to be part of this class or simplified
from sauron.utils.utils import (  # Placeholder if this path is correct
    generate_split,
    nth,
)


class SurvivalDataManager:
    def __init__(
        self,
        csv_path: str,
        data_directory: Union[
            str, Dict[str, str]
        ],  # Path to feature root dir or dict by source
        time_column: str,  # e.g., 'survival_months'
        event_column: str,  # e.g., 'censorship' (0 for censored, 1 for event)
        patient_id_col_name: str = "case_id",
        slide_id_col_name: str = "slide_id",  # Though survival is often patient-level for MIL
        n_bins: int = 4,  # For discretizing survival time
        shuffle_slide_data: bool = False,  # Shuffle initial slide data loaded from CSV
        random_seed: int = 7,
        verbose: bool = True,
        patient_stratification_for_len: bool = True,  # If this manager were a dataset, how to calc len
        filter_dict: Optional[Dict[str, List[str]]] = None,
        eps: float = 1e-6,  # Epsilon for binning
        omic_csv_path: Optional[str] = None,  # Path to CSV with omic features
        omic_patient_id_col: Optional[str] = "case_id",  # Patient ID col in omic CSV
        apply_sig: bool = False,  # For coattn mode signatures
        signatures_csv_path: Optional[str] = None,  # Path to signatures CSV for coattn
    ):
        self.csv_path = csv_path
        self.data_directory = data_directory
        self.time_column = time_column
        self.event_column = event_column
        self.patient_id_col_name = patient_id_col_name
        self.slide_id_col_name = slide_id_col_name
        self.n_bins = n_bins
        self.random_seed = random_seed
        self.verbose = verbose
        self.filter_dict = filter_dict
        self.eps = eps
        self.omic_csv_path = omic_csv_path
        self.omic_patient_id_col = omic_patient_id_col
        self.apply_sig = apply_sig
        self.signatures_csv_path = signatures_csv_path

        np.random.seed(self.random_seed)

        # Load and preprocess slide/patient data
        raw_data = pd.read_csv(csv_path, low_memory=False)
        self.slide_data = self._rename_cols(
            raw_data
        )  # Standardizes to "case_id", "slide_id"
        self.slide_data = self._filter_data(self.slide_data, self.filter_dict)

        if shuffle_slide_data:
            self.slide_data = self.slide_data.sample(
                frac=1, random_state=self.random_seed
            ).reset_index(drop=True)

        # Patient data is primary for survival; slide_id links features to patient
        # We'll use patient_df for labels, splits, and as base for MIL dataset items
        self.patient_df = self.slide_data.drop_duplicates(subset=["case_id"]).copy()
        self.patient_df = self._discretize_survival(
            self.patient_df
        )  # Adds 'label' (combined) and 'disc_label' (bin)

        self.num_classes = len(
            self.survival_label_map
        )  # Number of (bin, event_status) combinations
        self._create_patient_slide_dictionary()  # self.patient_slide_dict

        # Omic data handling
        self.omic_features_df: Optional[pd.DataFrame] = None
        self.omic_scalers: Optional[Dict[str, StandardScaler]] = (
            None  # For different omic types if needed
        )
        self.signatures: Optional[pd.DataFrame] = None
        self.omic_names_for_coattn: Optional[List[List[str]]] = None
        self.omic_sizes_for_coattn: Optional[List[int]] = None

        if self.omic_csv_path:
            self._load_omic_data()
        if self.apply_sig and self.signatures_csv_path:
            self._load_signatures()
            if self.omic_features_df is not None:
                self._prepare_coattn_omic_names()

        self.all_patient_ids = self.patient_df["case_id"].tolist()
        self._prepare_class_indices_for_split()  # For stratified splitting using 'label'

        # Split related attributes
        self.train_patient_ids: Optional[List[str]] = None
        self.val_patient_ids: Optional[List[str]] = None
        self.test_patient_ids: Optional[List[str]] = None
        self.split_generator = None  # For k-fold from original generate_split

        if verbose:
            self._print_summary()

    def _rename_cols(self, df: pd.DataFrame) -> pd.DataFrame:
        df = df.copy()
        if (
            self.patient_id_col_name != "case_id"
            and self.patient_id_col_name in df.columns
        ):
            df.rename(columns={self.patient_id_col_name: "case_id"}, inplace=True)
        if (
            self.slide_id_col_name != "slide_id"
            and self.slide_id_col_name in df.columns
        ):
            df.rename(columns={self.slide_id_col_name: "slide_id"}, inplace=True)
        if "case_id" not in df.columns:
            raise ValueError(
                f"Patient ID column '{self.patient_id_col_name}' (expected 'case_id') not found."
            )
        # slide_id is not strictly necessary if data_directory structure uses case_id for H5/PT files
        # but good to have for mapping if multiple slides per patient contribute features.
        if "slide_id" not in df.columns:
            print(
                f"Warning: Slide ID column '{self.slide_id_col_name}' (expected 'slide_id') not found. Assuming patient-level features or direct case_id mapping for files."
            )
            # If slide_id is critical for feature loading, this might need adjustment or error.
            # For now, let's assume if multiple .pt files per patient, slide_id is used to find them.
            # If one feature file per patient (e.g. patient_id.h5), then slide_id is less critical.
            if (
                "slide_id" not in df.columns
                and self.patient_id_col_name == self.slide_id_col_name
            ):  # If slide_id is same as patient_id
                df["slide_id"] = df["case_id"]
            elif (
                "slide_id" not in df.columns
            ):  # Create a dummy slide_id if not present, can be case_id
                df["slide_id"] = df["case_id"]

        if self.time_column not in df.columns:
            raise ValueError(f"Time column '{self.time_column}' not found.")
        if self.event_column not in df.columns:
            raise ValueError(f"Event column '{self.event_column}' not found.")
        return df

    def _filter_data(
        self, data: pd.DataFrame, filter_criteria: Optional[Dict[str, List[str]]]
    ) -> pd.DataFrame:
        if filter_criteria:
            mask = pd.Series(True, index=data.index)
            for column, values in filter_criteria.items():
                if column not in data.columns:
                    print(f"Warning: Filter column '{column}' not found in data.")
                    continue
                mask &= data[column].isin(values)
            return data[mask].reset_index(drop=True)
        return data

    def _discretize_survival(self, patient_df: pd.DataFrame) -> pd.DataFrame:
        df = patient_df.copy()

        # Patients with event (event_column == 1, assuming 0 is censored)
        uncensored_df = df[df[self.event_column] == 1]
        if (
            len(uncensored_df) < self.n_bins
        ):  # Not enough uncensored patients to form bins
            print(
                f"Warning: Only {len(uncensored_df)} uncensored patients. Reducing n_bins to {max(1, len(uncensored_df)) if len(uncensored_df) > 0 else 1}."
            )
            current_n_bins = max(1, len(uncensored_df)) if len(uncensored_df) > 0 else 1
            if current_n_bins == 0:  # No uncensored patients, create a single bin
                self.survival_bins = np.array(
                    [
                        df[self.time_column].min() - self.eps,
                        df[self.time_column].max() + self.eps,
                    ]
                )
                # assign all to bin 0
                df["disc_label"] = 0
            else:  # use qcut on uncensored for bin edges
                _, self.survival_bins = pd.qcut(
                    uncensored_df[self.time_column],
                    q=current_n_bins,
                    retbins=True,
                    labels=False,
                )
                self.survival_bins[0] = (
                    df[self.time_column].min() - self.eps
                )  # Adjust outer bins
                self.survival_bins[-1] = df[self.time_column].max() + self.eps
                df["disc_label"] = pd.cut(
                    df[self.time_column],
                    bins=self.survival_bins,
                    retbins=False,
                    labels=False,
                    right=False,
                    include_lowest=True,
                ).astype(int)

        else:  # Sufficient uncensored patients
            # Determine bins based on uncensored patients' survival times
            _, q_bins_uncensored = pd.qcut(
                uncensored_df[self.time_column],
                q=self.n_bins,
                retbins=True,
                labels=False,
            )

            # Adjust bins to cover the whole range of survival times (including censored)
            self.survival_bins = np.concatenate(
                (
                    [df[self.time_column].min() - self.eps],
                    q_bins_uncensored[1:-1],
                    [df[self.time_column].max() + self.eps],
                )
            )

            # Ensure bins are monotonically increasing (pd.qcut can sometimes produce non-monotonic due to ties)
            self.survival_bins = np.unique(self.survival_bins)
            if (
                len(self.survival_bins) < self.n_bins + 1
            ):  # If unique reduced bins too much
                print(
                    f"Warning: After unique, number of bins reduced. Consider fewer n_bins or check data distribution."
                )
                # Fallback to simple linspace if qcut fails badly
                if len(self.survival_bins) <= 2:
                    self.survival_bins = np.linspace(
                        df[self.time_column].min() - self.eps,
                        df[self.time_column].max() + self.eps,
                        self.n_bins + 1,
                    )

            df["disc_label"] = pd.cut(
                df[self.time_column],
                bins=self.survival_bins,
                retbins=False,
                labels=False,
                right=False,
                include_lowest=True,
            ).astype(int)

        # Create combined label: (time_bin, event_status)
        self.survival_label_map = {}
        label_counter = 0
        # Iterate over actual unique bin numbers and event statuses
        actual_bins = sorted(df["disc_label"].unique())
        actual_event_statuses = sorted(df[self.event_column].unique())

        for bin_val in actual_bins:
            for event_val in actual_event_statuses:  # Typically 0 and 1
                self.survival_label_map[(bin_val, int(event_val))] = label_counter
                label_counter += 1

        df["label"] = df.apply(
            lambda x: self.survival_label_map[
                (x["disc_label"], int(x[self.event_column]))
            ],
            axis=1,
        )
        return df

    def _create_patient_slide_dictionary(self):
        self.patient_slide_dict = {}
        if (
            "slide_id" in self.slide_data.columns
            and "case_id" in self.slide_data.columns
        ):
            for patient_id, group in self.slide_data.groupby("case_id"):
                self.patient_slide_dict[patient_id] = group["slide_id"].tolist()
        else:  # If no slide_id, assume one "feature entity" per patient, named by case_id
            for patient_id in self.patient_df["case_id"]:
                self.patient_slide_dict[patient_id] = [
                    patient_id
                ]  # Use patient_id as the "slide_id"

    def _load_omic_data(self):
        if not self.omic_csv_path or not self.omic_patient_id_col:
            return
        try:
            omic_df = pd.read_csv(self.omic_csv_path)
            if self.omic_patient_id_col not in omic_df.columns:
                raise ValueError(
                    f"Omic patient ID column '{self.omic_patient_id_col}' not found in omic CSV."
                )

            omic_df.set_index(self.omic_patient_id_col, inplace=True)

            # Align omic data with patient_df (patients present in the main CSV)
            self.omic_features_df = omic_df.reindex(self.patient_df["case_id"]).fillna(
                0
            )  # Or other imputation

            # Identify feature columns (assuming all non-ID columns are features)
            # This might need to be more robust if omic CSV has other metadata
            self.omic_feature_names = [col for col in self.omic_features_df.columns]

            if self.verbose and self.omic_features_df is not None:
                print(
                    f"Loaded omic data for {len(self.omic_features_df)} patients, {len(self.omic_feature_names)} features."
                )
                print(
                    f"Patients in main data: {len(self.patient_df)}, Patients with omic data: {len(self.omic_features_df.dropna(how='all'))}"
                )

        except FileNotFoundError:
            print(f"Warning: Omic CSV file not found at {self.omic_csv_path}")
            self.omic_features_df = None
        except Exception as e:
            print(f"Error loading omic data: {e}")
            self.omic_features_df = None

    def _load_signatures(self):
        if not self.signatures_csv_path:
            return
        try:
            self.signatures = pd.read_csv(self.signatures_csv_path)
            if self.verbose:
                print(f"Loaded signatures from {self.signatures_csv_path}")
        except FileNotFoundError:
            print(f"Warning: Signatures CSV not found at {self.signatures_csv_path}")
            self.signatures = None

    def _prepare_coattn_omic_names(self):
        if self.signatures is None or self.omic_features_df is None:
            return

        self.omic_names_for_coattn = []
        # Original code implies signatures CSV has columns, each being a gene set / signature name
        # And values are gene names, possibly with suffixes like _mut, _cnv, _rnaseq
        available_omic_cols = pd.Series(self.omic_features_df.columns)

        for sig_col_name in self.signatures.columns:
            # Genes in the current signature
            genes_in_sig = self.signatures[sig_col_name].dropna().unique()

            # Construct potential feature names (gene + suffix)
            potential_features = []
            for gene in genes_in_sig:
                for suffix in [
                    "_mut",
                    "_cnv",
                    "_rnaseq",
                    "",
                ]:  # Add empty for base gene name
                    potential_features.append(f"{gene}{suffix}")

            # Find which of these potential features actually exist in our omic_features_df
            intersecting_features = sorted(
                list(set(potential_features) & set(available_omic_cols))
            )
            self.omic_names_for_coattn.append(intersecting_features)

        self.omic_sizes_for_coattn = [
            len(names) for names in self.omic_names_for_coattn
        ]
        if self.verbose:
            print("Prepared omic names for CoAttn:")
            for i, names in enumerate(self.omic_names_for_coattn):
                print(f"  Signature {i}: {len(names)} features")

    def _prepare_class_indices_for_split(self) -> None:
        # For stratified splitting based on the combined (bin, event) label
        self.patient_class_labels_for_stratification = self.patient_df["label"].values
        self.num_unique_stratification_labels = len(
            np.unique(self.patient_class_labels_for_stratification)
        )

        self.patient_indices_by_strat_label = [
            np.where(self.patient_class_labels_for_stratification == strat_label)[0]
            for strat_label in range(
                self.num_unique_stratification_labels
            )  # Assumes labels are 0 to N-1
        ]

    def _print_summary(self) -> None:
        print("--- Survival DataManager Summary ---")
        print(f"CSV Path: {self.csv_path}")
        print(f"Time Column: {self.time_column}, Event Column: {self.event_column}")
        print(f"N Bins for Discretization: {self.n_bins}")
        print(f"Actual Survival Bins Edges: {self.survival_bins}")
        print(f"Survival Label Map (bin, event) -> int: {self.survival_label_map}")
        print(f"Number of Combined Survival Classes: {self.num_classes}")
        print(f"Total unique patients in manager: {len(self.patient_df)}")
        print(f"Patient-level combined label counts:")
        print(self.patient_df["label"].value_counts(sort=False))
        if self.omic_features_df is not None:
            print(
                f"Omic features loaded for {self.omic_features_df.shape[0]} patients, with {self.omic_features_df.shape[1]} features."
            )
        if self.signatures is not None:
            print(
                f"Signatures loaded. {len(self.omic_names_for_coattn or [])} sets for CoAttn."
            )
        print("------------------------------------")

    def create_splits_from_generating_function(
        self,
        k=3,
        val_num=(25, 25),
        test_num=(40, 40),
        label_frac=1.0,
        custom_test_ids=None,
    ):
        """Uses the original generate_split logic.
        This creates a generator for k folds. You call set_next_fold_from_generator() to advance.
        """
        if not hasattr(
            self, "patient_indices_by_strat_label"
        ):  # Ensure _prepare_class_indices_for_split was called
            self._prepare_class_indices_for_split()

        settings = {
            "n_splits": k,
            "val_num": val_num,  # These numbers might be absolute counts or % depending on generate_split
            "test_num": test_num,
            "label_frac": label_frac,
            "seed": self.random_seed,
            "custom_test_ids": custom_test_ids,  # This needs careful handling if IDs are strings
            "cls_ids": self.patient_indices_by_strat_label,  # Indices into patient_df
            "samples": len(self.patient_df),  # Total number of patients
        }
        self.split_generator = generate_split(**settings)
        self.num_folds_generated = k  # Assuming k folds are generated
        if self.verbose:
            print(f"Initialized split generator for {k} folds.")

    def set_next_fold_from_generator(
        self, start_from_fold: Optional[int] = None
    ) -> bool:
        if self.split_generator is None:
            raise RuntimeError(
                "Split generator not initialized. Call create_splits_from_generating_function() first."
            )

        try:
            if start_from_fold is not None:  # Fast-forward generator
                # Ensure generator is reset or handled if called multiple times with start_from_fold
                # This might require re-initializing the generator if it's stateful and consumed.
                # For now, assume nth can be used if the generator is fresh or resettable.
                split_indices_in_patient_df = nth(self.split_generator, start_from_fold)
            else:
                split_indices_in_patient_df = next(
                    self.split_generator
                )  # Advances the generator
        except StopIteration:
            if self.verbose:
                print("No more folds in the generator.")
            return False  # No more folds

        train_indices, val_indices, test_indices = split_indices_in_patient_df

        self.train_patient_ids = self.patient_df.iloc[train_indices]["case_id"].tolist()
        self.val_patient_ids = self.patient_df.iloc[val_indices]["case_id"].tolist()
        self.test_patient_ids = self.patient_df.iloc[test_indices]["case_id"].tolist()

        if self.verbose:
            print(
                f"Set fold: Train Pat. {len(self.train_patient_ids)}, Val Pat. {len(self.val_patient_ids)}, Test Pat. {len(self.test_patient_ids)}"
            )

        # Normalize omic data if present, fitting on current TRAIN split
        if self.omic_features_df is not None and self.train_patient_ids:
            self._fit_omic_scaler(self.train_patient_ids)
        return True

    def get_mil_datasets(
        self,
        mode: str,  # 'path', 'omic', 'pathomic', 'coattn'
        use_hdf5: bool = False,  # For path features
        backbone: Optional[str] = None,  # For path .pt features
        patch_size: str = "",  # For path .pt features
        cache_enabled: bool = False,
    ) -> Tuple[
        Optional[SurvivalMILDataset],
        Optional[SurvivalMILDataset],
        Optional[SurvivalMILDataset],
    ]:
        if self.train_patient_ids is None:  # Check if a fold has been set
            raise ValueError(
                "Splits not set. Call a split creation method and set_next_fold...() first."
            )

        common_params = {
            "patient_slide_dict": self.patient_slide_dict,
            "data_directory": self.data_directory,
            "time_column": self.time_column,
            "event_column": self.event_column,
            "disc_label_column": "disc_label",  # Column name in patient_df for discrete time bin
            "combined_label_column": "label",  # Column name in patient_df for (bin,event) label
            "mode": mode,
            "use_hdf5": use_hdf5,
            "backbone": backbone,
            "patch_size": patch_size,
            "cache_enabled": cache_enabled,
            "omic_names_for_coattn": self.omic_names_for_coattn,  # Pass coattn specific omic names
        }

        datasets = []
        for split_name, patient_ids_list in [
            ("train", self.train_patient_ids),
            ("val", self.val_patient_ids),
            ("test", self.test_patient_ids),
        ]:
            if not patient_ids_list:
                datasets.append(None)
                continue

            # Get patient data for this split
            current_split_patient_df = self.patient_df[
                self.patient_df["case_id"].isin(patient_ids_list)
            ].reset_index(drop=True)

            current_split_omic_df = None
            if self.omic_features_df is not None:
                # Select omic features for current patients
                omic_for_split = self.omic_features_df.loc[patient_ids_list]
                # Apply scaler if fitted (scaler is fitted on train_patient_ids)
                if (
                    self.omic_scalers and "all" in self.omic_scalers
                ):  # Assuming one scaler for all omics for now
                    scaled_omic_data = self.omic_scalers["all"].transform(
                        omic_for_split[self.omic_feature_names]
                    )
                    current_split_omic_df = pd.DataFrame(
                        scaled_omic_data,
                        columns=self.omic_feature_names,
                        index=omic_for_split.index,
                    )
                else:
                    current_split_omic_df = omic_for_split[
                        self.omic_feature_names
                    ].copy()  # No scaling or scaler not ready

            dataset = SurvivalMILDataset(
                patient_data_df=current_split_patient_df,
                omic_features_df_scaled=current_split_omic_df,  # Pass potentially scaled omics
                **common_params,
            )
            datasets.append(dataset)

        return tuple(datasets)

    def _fit_omic_scaler(self, train_patient_ids_for_scaling: List[str]):
        if self.omic_features_df is None or not train_patient_ids_for_scaling:
            self.omic_scalers = None
            return

        # Ensure all train_patient_ids are in omic_features_df index
        train_patient_ids_for_scaling = [
            pid
            for pid in train_patient_ids_for_scaling
            if pid in self.omic_features_df.index
        ]
        if not train_patient_ids_for_scaling:
            print(
                "Warning: No training patients found in omic_features_df for scaling."
            )
            self.omic_scalers = None
            return

        train_omic_data = self.omic_features_df.loc[
            train_patient_ids_for_scaling, self.omic_feature_names
        ]

        # For now, one scaler for all omic features. Can be extended.
        scaler = StandardScaler()
        scaler.fit(train_omic_data)
        self.omic_scalers = {"all": scaler}
        if self.verbose:
            print(
                "Fitted StandardScaler for omic features on the current training split."
            )

    def save_current_split_patient_ids(self, filename: str):
        if self.train_patient_ids is None:
            raise ValueError("Splits not set.")
        max_len = max(
            len(self.train_patient_ids or []),
            len(self.val_patient_ids or []),
            len(self.test_patient_ids or []),
        )

        df = pd.DataFrame(
            {
                "train_ids": pd.Series(self.train_patient_ids or []).reindex(
                    range(max_len)
                ),
                "val_ids": pd.Series(self.val_patient_ids or []).reindex(
                    range(max_len)
                ),
                "test_ids": pd.Series(self.test_patient_ids or []).reindex(
                    range(max_len)
                ),
            }
        )
        df.to_csv(filename, index=False)
        if self.verbose:
            print(f"Survival split patient IDs saved to {filename}")


class SurvivalMILDataset(Dataset):
    def __init__(
        self,
        patient_data_df: pd.DataFrame,  # DF for patients in this specific split
        patient_slide_dict: Dict[
            str, List[str]
        ],  # Full {patient_id: [slide_ids]} mapping
        data_directory: Union[str, Dict[str, str]],
        time_column: str,
        event_column: str,
        disc_label_column: str,  # e.g. "disc_label"
        combined_label_column: str,  # e.g. "label"
        mode: str,  # 'path', 'omic', 'pathomic', 'coattn'
        use_hdf5: bool = False,
        backbone: Optional[str] = None,
        patch_size: str = "",
        cache_enabled: bool = False,
        omic_features_df_scaled: Optional[
            pd.DataFrame
        ] = None,  # Scaled omic features for this split
        omic_names_for_coattn: Optional[List[List[str]]] = None,
    ):
        self.patient_data = patient_data_df
        self.patient_slide_dict = patient_slide_dict
        self.data_directory = data_directory
        self.time_col = time_column
        self.event_col = event_column
        self.disc_label_col = disc_label_column
        self.combined_label_col = combined_label_column
        self.mode = mode
        self.use_hdf5 = use_hdf5  # For path features
        self.backbone = backbone  # For path .pt features
        self.patch_size = str(patch_size) if patch_size is not None else ""
        self.cache_enabled = cache_enabled
        self.path_features_cache: Dict[
            str, torch.Tensor
        ] = {}  # Cache for loaded slide features

        self.omic_features = (
            omic_features_df_scaled  # Already selected and scaled for this split
        )
        self.omic_names_for_coattn = omic_names_for_coattn

        valid_modes = [
            "path",
            "omic",
            "pathomic",
            "coattn",
            "cluster",
        ]  # Added cluster from original
        if self.mode not in valid_modes:
            raise ValueError(
                f"Mode '{self.mode}' not implemented. Valid modes: {valid_modes}"
            )

        if "omic" in self.mode and self.omic_features is None:
            print(
                f"Warning: Mode '{self.mode}' requires omic features, but none were provided/loaded."
            )
        if self.mode == "coattn" and not self.omic_names_for_coattn:
            print(
                "Warning: Mode 'coattn' selected, but omic_names_for_coattn not provided."
            )
        if "path" in self.mode and not self.use_hdf5 and not self.backbone:
            print("Warning: Path-based mode with .pt files, but backbone is not set.")

    def __len__(self) -> int:
        return len(self.patient_data)

    def _load_path_features(
        self, patient_case_id: str, slide_ids_for_patient: List[str]
    ) -> torch.Tensor:
        all_path_features = []
        for slide_id in slide_ids_for_patient:
            # Determine data_dir for this slide (if self.data_directory is a dict)
            current_data_dir_path = self.data_directory
            if isinstance(self.data_directory, dict):
                # This requires 'source' column in the main slide_data from which patient_slide_dict was built
                # For simplicity, assume if data_directory is dict, it applies globally or needs more complex handling
                # Or, assume patient_data_df has a 'source' column if data_directory is a dict
                row_for_source = (
                    self.patient_data[
                        self.patient_data["case_id"] == patient_case_id
                    ].iloc[0]
                    if "source" in self.patient_data.columns
                    else None
                )  # Hacky
                if row_for_source is not None and "source" in row_for_source.index:
                    source = row_for_source["source"]
                    if source not in self.data_directory:
                        raise ValueError(
                            f"Source '{source}' for patient '{patient_case_id}' not in data_directory keys."
                        )
                    current_data_dir_path = self.data_directory[source]
                # else:
                # If no source per patient, and data_directory is dict, this is an issue.
                # Default to first key or raise error if not resolvable.
                # For now, assume string or resolvable dict.

            if not self.use_hdf5:
                if not self.backbone:
                    raise ValueError("Backbone needed for .pt files.")
                patch_subdir = ""
                if self.patch_size and self.patch_size != "512":
                    patch_subdir = self.patch_size
                file_path = os.path.join(
                    current_data_dir_path,
                    patch_subdir,
                    "pt_files",
                    self.backbone,
                    f"{slide_id}.pt",
                )

                if file_path in self.path_features_cache:
                    wsi_bag = self.path_features_cache[file_path]
                else:
                    try:
                        wsi_bag = torch.load(file_path)
                        if self.cache_enabled:
                            self.path_features_cache[file_path] = wsi_bag
                    except FileNotFoundError:
                        # Try without patch_subdir for robustness if original was inconsistent
                        file_path_alt = os.path.join(
                            current_data_dir_path,
                            "pt_files",
                            self.backbone,
                            f"{slide_id}.pt",
                        )
                        try:
                            wsi_bag = torch.load(file_path_alt)
                            if self.cache_enabled:
                                self.path_features_cache[file_path_alt] = wsi_bag
                        except FileNotFoundError:
                            raise FileNotFoundError(
                                f"Path feature file not found for slide {slide_id} at {file_path} or {file_path_alt}"
                            )
                all_path_features.append(wsi_bag)
            else:  # use_hdf5
                # HDF5 usually per patient (case_id.h5) or per slide (slide_id.h5)
                # Assuming slide_id.h5 for now as per original classification.
                # If patient_id.h5, then only one loop needed.
                h5_file_path = os.path.join(
                    current_data_dir_path, "h5_files", f"{slide_id}.h5"
                )
                try:
                    with h5py.File(h5_file_path, "r") as hf:
                        features = torch.from_numpy(hf["features"][:])
                        all_path_features.append(features)
                except OSError:
                    raise OSError(
                        f"HDF5 file not found or corrupted for slide {slide_id} at {h5_file_path}"
                    )

        if not all_path_features:  # No features found for any slide_id of this patient
            # Return a dummy tensor or raise error. For MIL, usually expect some features.
            # Shape needs to match what model expects if this patient has no path data.
            # This depends on feature dim, e.g., (0, 1024)
            print(
                f"Warning: No path features loaded for patient {patient_case_id} (slides: {slide_ids_for_patient}). Returning zero tensor."
            )
            # Try to infer feature dim from backbone or a fixed value
            # This is tricky. For now, a small placeholder. Model must handle 0-dim input.
            return torch.zeros(
                (0, 1)
            )  # Or try to get feature dim if one file was loaded before

        return (
            torch.cat(all_path_features, dim=0)
            if all_path_features
            else torch.zeros((0, 1))
        )

    def __getitem__(self, idx: int) -> tuple:
        patient_row = self.patient_data.iloc[idx]
        case_id = patient_row["case_id"]

        # Labels and time/event info
        # discrete_time_bin = patient_row[self.disc_label_col] # Bin index
        combined_label = patient_row[
            self.combined_label_col
        ]  # (Bin, Event) mapped to int
        event_time = patient_row[self.time_col]
        censorship_status = patient_row[self.event_col]  # 0=censored, 1=event

        slide_ids = self.patient_slide_dict.get(
            case_id, [case_id]
        )  # Fallback to case_id if not in dict

        # Initialize outputs
        path_features = torch.empty(0)  # Placeholder
        omic_data_tensor = torch.empty(0)

        # --- Load Path Features (WSI bags) ---
        if self.mode in ["path", "pathomic", "coattn", "cluster"]:
            path_features = self._load_path_features(case_id, slide_ids)

        # --- Load Omic Features ---
        if self.omic_features is not None and self.mode in [
            "omic",
            "pathomic",
            "coattn",
            "cluster",
        ]:
            if case_id in self.omic_features.index:
                # For 'omic', 'pathomic', 'cluster' - all omics for the patient
                if self.mode != "coattn":
                    omic_data_tensor = torch.tensor(
                        self.omic_features.loc[case_id].values, dtype=torch.float32
                    )
                # For 'coattn' - specific sets of omics
                else:
                    if not self.omic_names_for_coattn:
                        # Return all omics if coattn names not set, or error, or empty
                        print(
                            f"Warning: CoAttn mode but no omic_names_for_coattn for patient {case_id}. Using all omics or empty."
                        )
                        omic_data_tensor = (
                            torch.tensor(
                                self.omic_features.loc[case_id].values,
                                dtype=torch.float32,
                            )
                            if not self.omic_features.empty
                            else torch.empty(0)
                        )

                        # Or, if CoAttn expects multiple omic tensors:
                        # For CoAttn, original returned multiple omic tensors.
                        # This structure needs to be decided. For now, one tensor or a list.
                        # To match original:
                        coattn_omic_tensors = []
                        if self.omic_names_for_coattn:
                            for i, sig_omic_names in enumerate(
                                self.omic_names_for_coattn
                            ):
                                if (
                                    sig_omic_names
                                ):  # If this signature has features defined
                                    # Ensure all names are in omic_features columns
                                    valid_sig_omic_names = [
                                        name
                                        for name in sig_omic_names
                                        if name in self.omic_features.columns
                                    ]
                                    if valid_sig_omic_names:
                                        coattn_omic_tensors.append(
                                            torch.tensor(
                                                self.omic_features.loc[
                                                    case_id, valid_sig_omic_names
                                                ].values,
                                                dtype=torch.float32,
                                            )
                                        )
                                    else:  # No valid features for this signature for this patient
                                        coattn_omic_tensors.append(
                                            torch.empty(0)
                                        )  # Placeholder for this signature
                                else:  # No features defined for this signature
                                    coattn_omic_tensors.append(torch.empty(0))
                        # This changes the return signature for __getitem__ based on mode.
                        # PyTorch DataLoader usually expects consistent return types.
                        # It's better to return a dict or a fixed tuple structure.
                        # For now, let's stick to path_features, omic_data_tensor, label, event_time, c
                        # And CoAttn model would need to split omic_data_tensor if it's concatenated.
                        # OR, if mode is 'coattn', omic_data_tensor could be a list/tuple of tensors.
                        # Let's assume for coattn, omic_data_tensor IS the list of tensors for now.
                        if self.mode == "coattn":
                            omic_data_tensor = (
                                tuple(coattn_omic_tensors)
                                if coattn_omic_tensors
                                else tuple(
                                    torch.empty(0)
                                    for _ in range(
                                        len(self.omic_names_for_coattn or [])
                                    )
                                )
                            )

            else:  # Patient not in omic_features (e.g. missing data)
                print(
                    f"Warning: Omic data not found for patient {case_id}. Returning empty tensor for omics."
                )
                if self.mode == "coattn" and self.omic_names_for_coattn:
                    omic_data_tensor = tuple(
                        torch.empty(0) for _ in self.omic_names_for_coattn
                    )
                else:  # For other omic modes
                    # Try to get expected shape for omic from the dataframe columns
                    num_omic_features = (
                        len(self.omic_features.columns)
                        if self.omic_features is not None
                        else 0
                    )
                    omic_data_tensor = torch.zeros(
                        num_omic_features, dtype=torch.float32
                    )

        # --- Construct return tuple based on mode ---
        # This needs to be consistent for the DataLoader.
        # (path_features, omic_features, label (combined), event_time, censorship_status)
        # If a mode doesn't use one, it can be a placeholder (e.g., torch.empty(0)).

        if self.mode == "path":
            return (
                path_features,
                torch.empty(0),
                combined_label,
                event_time,
                censorship_status,
            )
        elif self.mode == "omic":
            return (
                torch.empty(0),
                omic_data_tensor,
                combined_label,
                event_time,
                censorship_status,
            )
        elif self.mode == "pathomic":
            return (
                path_features,
                omic_data_tensor,
                combined_label,
                event_time,
                censorship_status,
            )
        elif self.mode == "coattn":
            # `omic_data_tensor` is already a tuple of tensors for coattn here
            return (
                path_features,
                omic_data_tensor,
                combined_label,
                event_time,
                censorship_status,
            )
        elif self.mode == "cluster":  # Original 'cluster' mode also had 'cluster_ids'
            # cluster_ids logic was: self.fname2ids[slide_id[:-4]+'.pt']
            # This fname2ids needs to be loaded and passed, typically by SurvivalDataManager
            # For now, returning placeholder for cluster_ids
            cluster_ids_placeholder = torch.empty(0)  # Placeholder
            return (
                path_features,
                cluster_ids_placeholder,
                omic_data_tensor,
                combined_label,
                event_time,
                censorship_status,
            )
        else:  # Should not happen due to check in __init__
            raise NotImplementedError(f"Mode {self.mode} data assembly not defined.")

    def set_backbone(self, backbone: str):
        self.backbone = backbone

    def set_patch_size(self, size: str):
        self.patch_size = str(size)

    def load_from_hdf5(self, use_hdf5: bool):
        self.use_hdf5 = use_hdf5



================================================
FILE: sauron/losses/surv_loss.py
================================================
import numpy as np
import torch


def nll_loss(hazards, S, Y, c, alpha=0.4, eps=1e-7):
    batch_size = len(Y)
    Y = Y.view(batch_size, 1)  # ground truth bin, 1,2,...,k
    c = c.view(batch_size, 1).float()  # censorship status, 0 or 1
    if S is None:
        S = torch.cumprod(
            1 - hazards, dim=1
        )  # surival is cumulative product of 1 - hazards
    # without padding, S(0) = S[0], h(0) = h[0]
    S_padded = torch.cat(
        [torch.ones_like(c), S], 1
    )  # S(-1) = 0, all patients are alive from (-inf, 0) by definition
    # after padding, S(0) = S[1], S(1) = S[2], etc, h(0) = h[0]
    # h[y] = h(1)
    # S[1] = S(1)
    uncensored_loss = -(1 - c) * (
        torch.log(torch.gather(S_padded, 1, Y).clamp(min=eps))
        + torch.log(torch.gather(hazards, 1, Y).clamp(min=eps))
    )
    censored_loss = -c * torch.log(torch.gather(S_padded, 1, Y + 1).clamp(min=eps))
    neg_l = censored_loss + uncensored_loss
    loss = (1 - alpha) * neg_l + alpha * uncensored_loss
    loss = loss.mean()
    return loss


def ce_loss(hazards, S, Y, c, alpha=0.4, eps=1e-7):
    batch_size = len(Y)
    Y = Y.view(batch_size, 1)  # ground truth bin, 1,2,...,k
    c = c.view(batch_size, 1).float()  # censorship status, 0 or 1
    if S is None:
        S = torch.cumprod(
            1 - hazards, dim=1
        )  # surival is cumulative product of 1 - hazards
    # without padding, S(0) = S[0], h(0) = h[0]
    # after padding, S(0) = S[1], S(1) = S[2], etc, h(0) = h[0]
    # h[y] = h(1)
    # S[1] = S(1)
    S_padded = torch.cat([torch.ones_like(c), S], 1)
    reg = -(1 - c) * (
        torch.log(torch.gather(S_padded, 1, Y) + eps)
        + torch.log(torch.gather(hazards, 1, Y).clamp(min=eps))
    )
    ce_l = -c * torch.log(torch.gather(S, 1, Y).clamp(min=eps)) - (1 - c) * torch.log(
        1 - torch.gather(S, 1, Y).clamp(min=eps)
    )
    loss = (1 - alpha) * ce_l + alpha * reg
    loss = loss.mean()
    return loss


class CrossEntropySurvLoss(object):
    def __init__(self, alpha=0.15):
        self.alpha = alpha

    def __call__(self, hazards, S, Y, c, alpha=None):
        if alpha is None:
            return ce_loss(hazards, S, Y, c, alpha=self.alpha)
        else:
            return ce_loss(hazards, S, Y, c, alpha=alpha)


# loss_fn(hazards=hazards, S=S, Y=Y_hat, c=c, alpha=0)
class NLLSurvLoss(object):
    def __init__(self, alpha=0.15):
        self.alpha = alpha

    def __call__(self, hazards, S, Y, c, alpha=None):
        if alpha is None:
            return nll_loss(hazards, S, Y, c, alpha=self.alpha)
        else:
            return nll_loss(hazards, S, Y, c, alpha=alpha)


class CoxSurvLoss(object):
    def __call__(self, hazards, S, c, **kwargs):
        # This calculation credit to Travers Ching https://github.com/traversc/cox-nnet
        # Cox-nnet: An artificial neural network method for prognosis prediction of high-throughput omics data

        # Ensure hazards, S (event times), and c (censoring status) are 1D tensors
        # hazards: Predicted log-risk scores for each patient. Shape: [batch_size]
        # S: Observed event/censoring times for each patient. Shape: [batch_size]
        # c: Censoring status (0 for event, 1 for censored) for each patient. Shape: [batch_size]

        # Squeeze inputs to ensure they are 1D, as Cox loss operates on scalar times/risks per patient.
        hazards = hazards.squeeze()
        S = S.squeeze()
        c = c.squeeze()

        # Basic validation for input dimensions after squeezing
        if hazards.dim() != 1 or S.dim() != 1 or c.dim() != 1:
            raise ValueError(
                "Input tensors hazards, S, and c must be 1-dimensional after squeezing."
                f" Got hazards.shape={hazards.shape}, S.shape={S.shape}, c.shape={c.shape}"
            )

        current_batch_len = len(S)

        # R_mat[i, j] = 1 if patient j is in the risk set of patient i (i.e., S[j] >= S[i]), else 0.
        # This can be computed efficiently using broadcasting, removing the slow numpy loop.
        # S.unsqueeze(0) makes S a row vector (1, batch_size)
        # S.unsqueeze(1) makes S a column vector (batch_size, 1)
        # The comparison (S.unsqueeze(0) >= S.unsqueeze(1)) then broadcasts to (batch_size, batch_size)
        # where result[row_i, col_j] = (S[col_j] >= S[row_i]).
        R_mat = (S.unsqueeze(0) >= S.unsqueeze(1)).float()

        # Ensure R_mat is on the same device as the other tensors (hazards, S, c)
        device = hazards.device
        R_mat = R_mat.to(device)

        theta = (
            hazards  # hazards should already be the log-risk scores, now confirmed 1D
        )
        exp_theta = torch.exp(theta)

        # Calculate the log sum over the risk set for each patient i: log(sum_{j in R_i} exp(theta_j))
        # torch.sum(exp_theta * R_mat, dim=1) sums exp_theta_j for all j where R_mat[i,j] is 1 (i.e., j is in risk set of i)
        log_risk_sum = torch.log(torch.sum(exp_theta * R_mat, dim=1))

        # The Cox proportional hazards partial log-likelihood:
        # L = - sum_{i: uncensored} (theta_i - log(sum_{j in R_i} exp(theta_j)))
        # (1 - c) acts as a mask, making the term zero for censored observations.
        loss_cox = -torch.mean((theta - log_risk_sum) * (1 - c))
        return loss_cox



================================================
FILE: sauron/mil_models/ABMIL.py
================================================
import torch
import torch.nn as nn
import torch.nn.functional as F

from sauron.utils.generic_utils import initialize_weights  # Assuming this exists

from .activations import get_activation_fn


class _BaseAttentionMIL(nn.Module):
    def __init__(
        self,
        in_dim: int,
        embed_dim: int,
        attention_hidden_dim: int,
        num_attention_outputs: int,  # Typically K=1 for MIL aggregation
        n_classes: int,
        dropout_rate: float = 0.25,
        activation: str = "relu",
        is_survival: bool = False,
    ):
        super().__init__()
        self.embed_dim = embed_dim
        self.attention_hidden_dim = attention_hidden_dim
        self.num_attention_outputs = num_attention_outputs  # K
        self.is_survival = is_survival
        self.n_classes = n_classes

        feature_extractor_layers = [nn.Linear(in_dim, self.embed_dim)]
        feature_extractor_layers.append(get_activation_fn(activation))
        if dropout_rate > 0:
            feature_extractor_layers.append(nn.Dropout(dropout_rate))
        self.feature_extractor = nn.Sequential(*feature_extractor_layers)

        self.classifier_layer = nn.Linear(
            self.embed_dim * self.num_attention_outputs, n_classes
        )
        # self.apply(initialize_weights) # Apply if this is a common practice for all sub-modules

    def _get_outputs(self, logits, attention_scores=None):
        # Predictions (highest logit index)
        predictions = torch.topk(logits, 1, dim=1)[1]

        if self.is_survival:
            hazards = torch.sigmoid(logits)
            survival_curves = torch.cumprod(1 - hazards, dim=1)
            return hazards, survival_curves, predictions, attention_scores, {}
        else:
            probabilities = F.softmax(logits, dim=1)
            return logits, probabilities, predictions, attention_scores, {}


class DAttention(
    nn.Module
):  # Original DAttention renamed for clarity if _BaseAttentionMIL is used
    def __init__(
        self,
        in_dim: int,
        n_classes: int,
        dropout_rate: float = 0.25,
        activation: str = "relu",
        is_survival: bool = False,
    ):
        super().__init__()
        self.embed_dim = 512  # L
        self.attention_hidden_dim = 128  # D
        self.num_attention_outputs = 1  # K

        self.is_survival = is_survival
        self.n_classes = n_classes

        feature_layers = [nn.Linear(in_dim, self.embed_dim)]
        feature_layers.append(get_activation_fn(activation))
        if dropout_rate > 0:
            feature_layers.append(nn.Dropout(dropout_rate))
        self.feature_extractor = nn.Sequential(*feature_layers)

        self.attention_net = nn.Sequential(
            nn.Linear(self.embed_dim, self.attention_hidden_dim),
            nn.Tanh(),
            nn.Linear(self.attention_hidden_dim, self.num_attention_outputs),
        )
        self.classifier = nn.Linear(
            self.embed_dim * self.num_attention_outputs, n_classes
        )
        self.apply(initialize_weights)

    def forward(self, input_tensor: torch.Tensor):
        # input_tensor: (batch_size, num_instances, in_dim)
        batch_size = input_tensor.shape[0]

        instance_features = self.feature_extractor(
            input_tensor
        )  # (batch_size, num_instances, embed_dim)

        attention_logits = self.attention_net(
            instance_features
        )  # (batch_size, num_instances, K)
        attention_logits = torch.transpose(
            attention_logits, 2, 1
        )  # (batch_size, K, num_instances)

        attention_scores = F.softmax(attention_logits, dim=2)  # Softmax over instances

        # M = KxL equivalent for batch: (batch_size, K, embed_dim)
        aggregated_features = torch.bmm(attention_scores, instance_features)

        # If K=1, aggregated_features is (batch_size, 1, embed_dim)
        # Flatten for classifier: (batch_size, K * embed_dim)
        aggregated_features_flat = aggregated_features.view(batch_size, -1)

        logits = self.classifier(aggregated_features_flat)  # (batch_size, n_classes)

        # Predictions (highest logit index)
        predictions = torch.topk(logits, 1, dim=1)[1]

        if self.is_survival:
            hazards = torch.sigmoid(logits)
            survival_curves = torch.cumprod(1 - hazards, dim=1)
            return (
                hazards,
                survival_curves,
                predictions,
                attention_logits.transpose(2, 1),
                {},
            )  # Return raw attention before softmax
        else:
            probabilities = F.softmax(logits, dim=1)
            return (
                logits,
                probabilities,
                predictions,
                attention_logits.transpose(2, 1),
                {},
            )


class GatedAttention(nn.Module):
    def __init__(
        self,
        in_dim: int,
        n_classes: int,
        dropout_rate: float = 0.25,
        activation: str = "relu",
        is_survival: bool = False,
    ):
        super().__init__()
        self.embed_dim = 512  # L
        self.attention_hidden_dim = 128  # D
        self.num_attention_outputs = 1  # K

        self.is_survival = is_survival
        self.n_classes = n_classes

        feature_layers = [nn.Linear(in_dim, self.embed_dim)]
        feature_layers.append(get_activation_fn(activation))
        if dropout_rate > 0:
            feature_layers.append(nn.Dropout(dropout_rate))
        self.feature_extractor = nn.Sequential(*feature_layers)

        self.attention_V = nn.Sequential(
            nn.Linear(self.embed_dim, self.attention_hidden_dim), nn.Tanh()
        )
        self.attention_U = nn.Sequential(
            nn.Linear(self.embed_dim, self.attention_hidden_dim), nn.Sigmoid()
        )
        self.attention_weights = nn.Linear(
            self.attention_hidden_dim, self.num_attention_outputs
        )

        self.classifier = nn.Linear(
            self.embed_dim * self.num_attention_outputs, n_classes
        )
        self.apply(initialize_weights)

    def forward(self, input_tensor: torch.Tensor):
        # input_tensor: (batch_size, num_instances, in_dim)
        batch_size = input_tensor.shape[0]

        instance_features = self.feature_extractor(
            input_tensor
        )  # (batch_size, num_instances, embed_dim)

        attention_values = self.attention_V(
            instance_features
        )  # (batch_size, num_instances, D)
        attention_units = self.attention_U(
            instance_features
        )  # (batch_size, num_instances, D)

        # Element-wise multiplication, then pass through weights layer
        unnormalized_attention_scores = self.attention_weights(
            attention_values * attention_units
        )  # (batch_size, num_instances, K)
        unnormalized_attention_scores = torch.transpose(
            unnormalized_attention_scores, 2, 1
        )  # (batch_size, K, num_instances)

        normalized_attention_scores = F.softmax(
            unnormalized_attention_scores, dim=2
        )  # Softmax over instances

        aggregated_features = torch.bmm(
            normalized_attention_scores, instance_features
        )  # (batch_size, K, embed_dim)
        flattened_aggregated_features = aggregated_features.view(
            batch_size, -1
        )  # (batch_size, K * embed_dim)

        logits = self.classifier(
            flattened_aggregated_features
        )  # (batch_size, n_classes)

        # Predictions (highest logit index)
        predictions = torch.topk(logits, 1, dim=1)[1]

        if self.is_survival:
            hazard_rates = torch.sigmoid(logits)
            survival_curves = torch.cumprod(1 - hazard_rates, dim=1)
            # A_raw for GatedAttention is less direct, similar to DAttention, returning unnormalized_attention_scores before softmax
            return (
                hazard_rates,
                survival_curves,
                predictions,
                unnormalized_attention_scores.transpose(2, 1),
                {},
            )
        else:
            probabilities = F.softmax(logits, dim=1)
            return (
                logits,
                probabilities,
                predictions,
                unnormalized_attention_scores.transpose(2, 1),
                {},
            )



================================================
FILE: sauron/mil_models/activations.py
================================================
import torch.nn as nn


def get_activation_fn(activation_name: str) -> nn.Module:
    """Returns the activation function module."""
    if activation_name.lower() == "relu":
        return nn.ReLU()
    elif activation_name.lower() == "gelu":
        return nn.GELU()
    else:
        raise ValueError(f"Unsupported activation function: {activation_name}")



================================================
FILE: sauron/mil_models/DiffABMIL.py
================================================
import torch
import torch.nn as nn
import torch.nn.functional as F

from sauron.utils.generic_utils import initialize_weights

from .activations import get_activation_fn


class DifferentiableAttentionMIL(nn.Module):
    def __init__(
        self,
        in_dim: int,
        n_classes: int,
        embed_dim: int = 512,
        num_heads: int = 8,
        dropout_rate: float = 0.25,
        activation: str = "relu",
        is_survival: bool = False,
    ):
        super().__init__()
        self.embed_dim = embed_dim  # L
        self.num_heads = num_heads
        self.head_dim = self.embed_dim // self.num_heads
        self.is_survival = is_survival
        self.n_classes = n_classes

        if self.embed_dim % self.num_heads != 0:
            raise ValueError("embed_dim must be divisible by num_heads")

        feature_layers = [nn.Linear(in_dim, self.embed_dim)]
        feature_layers.append(get_activation_fn(activation))
        if dropout_rate > 0:
            feature_layers.append(nn.Dropout(dropout_rate))
        self.feature_extractor = nn.Sequential(*feature_layers)

        self.query_proj = nn.Linear(self.embed_dim, self.embed_dim)
        self.key_proj = nn.Linear(self.embed_dim, self.embed_dim)
        self.value_proj = nn.Linear(self.embed_dim, self.embed_dim)

        # Output projection from attention, not used in original scaled_dot_product version for bag_repr
        # self.output_proj = nn.Linear(self.embed_dim, self.embed_dim)

        self.classifier = nn.Linear(self.embed_dim, n_classes)
        self.apply(initialize_weights)

    def forward(self, x: torch.Tensor):
        # x: (batch_size, num_instances, in_dim)
        batch_size, num_instances, _ = x.size()

        instance_features = self.feature_extractor(
            x
        )  # (batch_size, num_instances, embed_dim)

        q = self.query_proj(instance_features)  # (batch_size, num_instances, embed_dim)
        k = self.key_proj(instance_features)  # (batch_size, num_instances, embed_dim)
        v = self.value_proj(instance_features)  # (batch_size, num_instances, embed_dim)

        # Reshape for multi-head attention for F.scaled_dot_product_attention
        # (batch_size, num_heads, num_instances, head_dim)
        q = q.view(batch_size, num_instances, self.num_heads, self.head_dim).transpose(
            1, 2
        )
        k = k.view(batch_size, num_instances, self.num_heads, self.head_dim).transpose(
            1, 2
        )
        v = v.view(batch_size, num_instances, self.num_heads, self.head_dim).transpose(
            1, 2
        )

        # scaled_dot_product_attention expects (..., S, E) for query, (..., L, E) for key/value
        # Here, S = num_instances, L = num_instances, E = head_dim
        # Input q, k, v: (batch_size, num_heads, num_instances, head_dim)
        # F.scaled_dot_product_attention will operate on last 3 dims if N>3
        # Or reshape to (batch_size * num_heads, num_instances, head_dim)
        # q_reshaped = q.contiguous().view(batch_size * self.num_heads, num_instances, self.head_dim)
        # k_reshaped = k.contiguous().view(batch_size * self.num_heads, num_instances, self.head_dim)
        # v_reshaped = v.contiguous().view(batch_size * self.num_heads, num_instances, self.head_dim)
        # attn_output = F.scaled_dot_product_attention(q_reshaped, k_reshaped, v_reshaped)
        # attn_output = attn_output.view(batch_size, self.num_heads, num_instances, self.head_dim)

        # Direct passing if Pytorch version supports it
        attn_output = F.scaled_dot_product_attention(q, k, v, dropout_p=0.0)
        # attn_output shape: (batch_size, num_heads, num_instances, head_dim)

        # Reshape back and combine heads
        # (batch_size, num_instances, num_heads, head_dim) -> (batch_size, num_instances, embed_dim)
        attn_output = (
            attn_output.transpose(1, 2)
            .contiguous()
            .view(batch_size, num_instances, self.embed_dim)
        )

        # Aggregate over instances (mean pooling of instance representations after attention)
        bag_representation = attn_output.mean(dim=1)  # (batch_size, embed_dim)

        logits = self.classifier(bag_representation)  # (batch_size, n_classes)

        # Predictions (highest logit index)
        predictions = torch.topk(logits, 1, dim=1)[1]

        # A_raw (attention weights) is not directly returned by F.scaled_dot_product_attention
        # To get it, one would compute (Q @ K.transpose(-2, -1) / sqrt(dim_k)).softmax(dim=-1)
        attention_scores_raw = None  # Placeholder

        if self.is_survival:
            hazards = torch.sigmoid(logits)
            survival_curves = torch.cumprod(1 - hazards, dim=1)
            return hazards, survival_curves, predictions, attention_scores_raw, {}
        else:
            probabilities = F.softmax(logits, dim=1)
            return logits, probabilities, predictions, attention_scores_raw, {}



================================================
FILE: sauron/mil_models/MambaMIL.py
================================================
"""
MambaMIL
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from mamba_ssm import Mamba

from sauron.mil_models.mamba_ssm import BiMamba, SRMamba
from sauron.utils.generic_utils import initialize_weights


class MambaMIL(nn.Module):
    def __init__(
        self,
        in_dim,
        n_classes,
        dropout,
        act,
        survival=False,
        layer=2,
        rate=10,
        type="SRMamba",
    ):
        super(MambaMIL, self).__init__()
        self._fc1 = [nn.Linear(in_dim, 512)]
        if act.lower() == "relu":
            self._fc1 += [nn.ReLU()]
        elif act.lower() == "gelu":
            self._fc1 += [nn.GELU()]
        if dropout:
            self._fc1 += [nn.Dropout(dropout)]

        self._fc1 = nn.Sequential(*self._fc1)
        self.norm = nn.LayerNorm(512)
        self.layers = nn.ModuleList()
        self.survival = survival

        if type == "SRMamba":
            for _ in range(layer):
                self.layers.append(
                    nn.Sequential(
                        nn.LayerNorm(512),
                        SRMamba(
                            d_model=512,
                            d_state=16,
                            d_conv=4,
                            expand=2,
                        ),
                    )
                )
        elif type == "Mamba":
            for _ in range(layer):
                self.layers.append(
                    nn.Sequential(
                        nn.LayerNorm(512),
                        Mamba(
                            d_model=512,
                            d_state=16,
                            d_conv=4,
                            expand=2,
                        ),
                    )
                )
        elif type == "BiMamba":
            for _ in range(layer):
                self.layers.append(
                    nn.Sequential(
                        nn.LayerNorm(512),
                        BiMamba(
                            d_model=512,
                            d_state=16,
                            d_conv=4,
                            expand=2,
                        ),
                    )
                )
        else:
            raise NotImplementedError("Mamba [{}] is not implemented".format(type))

        self.n_classes = n_classes
        self.classifier = nn.Linear(512, self.n_classes)
        self.attention = nn.Sequential(
            nn.Linear(512, 128), nn.Tanh(), nn.Linear(128, 1)
        )
        self.rate = rate
        self.type = type

        self.apply(initialize_weights)

    def forward(self, x):
        if len(x.shape) == 2:
            x = x.expand(1, -1, -1)
        h = x.float()  # [B, n, 1024]

        h = self._fc1(h)  # [B, n, 256]

        if self.type == "SRMamba":
            for layer in self.layers:
                h_ = h
                h = layer[0](h)
                h = layer[1](h, rate=self.rate)
                h = h + h_
        elif self.type == "Mamba" or self.type == "BiMamba":
            for layer in self.layers:
                h_ = h
                h = layer[0](h)
                h = layer[1](h)
                h = h + h_

        h = self.norm(h)
        A = self.attention(h)  # [B, n, K]
        A = torch.transpose(A, 1, 2)
        A = F.softmax(A, dim=-1)  # [B, K, n]
        h = torch.bmm(A, h)  # [B, K, 512]
        h = h.squeeze(0)

        logits = self.classifier(h)  # [B, n_classes]
        Y_prob = F.softmax(logits, dim=1)
        Y_hat = torch.topk(logits, 1, dim=1)[1]
        A_raw = None
        results_dict = None
        if self.survival:
            Y_hat = torch.topk(logits, 1, dim=1)[1]
            hazards = torch.sigmoid(logits)
            S = torch.cumprod(1 - hazards, dim=1)
            return hazards, S, Y_hat, None, None
        return logits, Y_prob, Y_hat, A_raw, results_dict

    def relocate(self):
        device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        self._fc1 = self._fc1.to(device)
        self.layers = self.layers.to(device)

        self.attention = self.attention.to(device)
        self.norm = self.norm.to(device)
        self.classifier = self.classifier.to(device)



================================================
FILE: sauron/mil_models/MaxMIL.py
================================================
import torch
import torch.nn as nn
import torch.nn.functional as F

from sauron.utils.generic_utils import initialize_weights

from .activations import get_activation_fn


class MaxMIL(nn.Module):
    def __init__(
        self,
        in_dim: int,
        n_classes: int,
        hidden_dim: int = 512,  # Renamed from fixed 512
        dropout_rate: float = 0.25,
        activation: str = "relu",
        is_survival: bool = False,
    ):
        super().__init__()
        self.is_survival = is_survival
        self.n_classes = n_classes

        head_layers = [nn.Linear(in_dim, hidden_dim)]
        head_layers.append(get_activation_fn(activation))
        if dropout_rate > 0:
            head_layers.append(nn.Dropout(dropout_rate))
        # Final layer to n_classes (instance scores/logits)
        head_layers.append(nn.Linear(hidden_dim, n_classes))

        self.instance_scorer = nn.Sequential(*head_layers)
        self.apply(initialize_weights)

    def forward(self, x: torch.Tensor):
        # x: (batch_size, num_instances, in_dim)
        # If single bag (num_instances, in_dim), add batch dim
        if x.ndim == 2:
            x = x.unsqueeze(0)

        # Get instance-level scores/logits
        instance_logits = self.instance_scorer(
            x
        )  # (batch_size, num_instances, n_classes)

        # Max pooling over instances for each class logit
        # Result: (batch_size, n_classes)
        bag_logits, _ = torch.max(instance_logits, dim=1)

        # Predictions (highest logit index)
        predictions = torch.topk(bag_logits, 1, dim=1)[1]

        if self.is_survival:
            hazards = torch.sigmoid(bag_logits)
            survival_curves = torch.cumprod(1 - hazards, dim=1)
            return hazards, survival_curves, predictions, None, {}
        else:
            probabilities = F.softmax(bag_logits, dim=1)
            return bag_logits, probabilities, predictions, None, {}



================================================
FILE: sauron/mil_models/MeanMIL.py
================================================
import torch
import torch.nn as nn
import torch.nn.functional as F

from sauron.utils.generic_utils import initialize_weights

from .activations import get_activation_fn


class MeanMIL(nn.Module):
    def __init__(
        self,
        in_dim: int,
        n_classes: int,
        hidden_dim: int = 512,  # Renamed from fixed 512
        dropout_rate: float = 0.25,
        activation: str = "relu",
        is_survival: bool = False,
    ):
        super().__init__()
        self.is_survival = is_survival
        self.n_classes = n_classes

        head_layers = [nn.Linear(in_dim, hidden_dim)]
        head_layers.append(get_activation_fn(activation))
        if dropout_rate > 0:
            head_layers.append(nn.Dropout(dropout_rate))
        # Final layer to n_classes (instance scores/logits)
        head_layers.append(nn.Linear(hidden_dim, n_classes))

        self.instance_scorer = nn.Sequential(*head_layers)
        self.apply(initialize_weights)

    def forward(self, x: torch.Tensor):
        # x: (batch_size, num_instances, in_dim)
        # If single bag (num_instances, in_dim), add batch dim
        if x.ndim == 2:
            x = x.unsqueeze(0)

        # Get instance-level scores/logits
        instance_logits = self.instance_scorer(
            x
        )  # (batch_size, num_instances, n_classes)

        # Mean pooling over instances for each class logit
        # Result: (batch_size, n_classes)
        bag_logits = torch.mean(instance_logits, dim=1)

        # Predictions (highest logit index)
        predictions = torch.topk(bag_logits, 1, dim=1)[1]

        if self.is_survival:
            hazards = torch.sigmoid(bag_logits)
            survival_curves = torch.cumprod(1 - hazards, dim=1)
            return hazards, survival_curves, predictions, None, {}
        else:
            probabilities = F.softmax(bag_logits, dim=1)
            return bag_logits, probabilities, predictions, None, {}



================================================
FILE: sauron/mil_models/models_factory.py
================================================
import torch
import torch.nn as nn
from torch.utils.data import DataLoader
from tqdm import tqdm
import numpy as np

from sauron.mil_models.ABMIL import ABMIL
from sauron.mil_models.TransMIL import TransMIL
from sauron.mil_models.MaxMIL import MaxMIL
from sauron.mil_models.MeanMIL import MeanMIL
from sauron.mil_models.MambaMIL import MambaMIL, MambaMIL_L, MambaMIL_XL
from sauron.mil_models.S4MIL import S4MIL, S4MIL_L, S4MIL_XL
from sauron.mil_models.WIKGMIL import WIKGMIL
from sauron.mil_models.DiffABMIL import DiffABMIL
from sauron.feature_extraction.models.patch_encoders.factory import encoder_factory


def mil_model_factory(args, in_dim=None):
    if in_dim is None:
        in_dim = encoder_factory(args.backbone).embedding_dim
    if args.mil_model == 'abmil':
        return ABMIL(in_dim=in_dim, n_classes=args.n_classes)
    elif args.mil_model == 'transmil':
        return TransMIL(in_dim=in_dim, n_classes=args.n_classes)
    elif args.mil_model == 'maxmil':
        return MaxMIL(in_dim=in_dim, n_classes=args.n_classes)
    elif args.mil_model == 'meanmil':
        return MeanMIL(in_dim=in_dim, n_classes=args.n_classes)
    elif args.mil_model == 'mamba':
        return MambaMIL(in_dim=in_dim, n_classes=args.n_classes)
    elif args.mil_model == 'mamba_l':
        return MambaMIL_L(in_dim=in_dim, n_classes=args.n_classes)
    elif args.mil_model == 'mamba_xl':
        return MambaMIL_XL(in_dim=in_dim, n_classes=args.n_classes)
    elif args.mil_model == 's4':
        return S4MIL(in_dim=in_dim, n_classes=args.n_classes)
    elif args.mil_model == 's4_l':
        return S4MIL_L(in_dim=in_dim, n_classes=args.n_classes)
    elif args.mil_model == 's4_xl':
        return S4MIL_XL(in_dim=in_dim, n_classes=args.n_classes)
    elif args.mil_model == 'wikgmil':
        return WIKGMIL(in_dim=in_dim, n_classes=args.n_classes)
    elif args.mil_model == 'diffabmil':
        return DiffABMIL(in_dim=in_dim, n_classes=args.n_classes)
    else:
        raise ValueError(f"Unknown MIL model: {args.mil_model}")


================================================
FILE: sauron/mil_models/S4MIL.py
================================================
import math

import torch
import torch.nn as nn
import torch.nn.functional as F
from einops import rearrange, repeat

from sauron.utils.generic_utils import initialize_weights

from .activations import get_activation_fn

# S4DKernel and S4D components (assumed to be correct and kept as is, minor style adjustments)
# _c2r and _r2c are utility functions for complex numbers, often defined locally or imported
_c2r = torch.view_as_real
_r2c = torch.view_as_complex


class DropoutNd(nn.Module):
    def __init__(self, p: float = 0.5, tie: bool = True, transposed: bool = True):
        super().__init__()
        if not 0.0 <= p < 1.0:
            raise ValueError(f"dropout probability has to be in [0, 1), but got {p}")
        self.p = p
        self.tie = tie
        self.transposed = transposed
        # self.binomial = torch.distributions.binomial.Binomial(probs=1 - self.p) # Not used

    def forward(self, X):
        if self.training and self.p > 0:
            if not self.transposed:
                X = X.transpose(
                    -1, -2
                )  # More robust than rearrange for (B H L) -> (B L H)

            # Determine mask shape
            # For (B, H, L) if tie=True, mask is (B, H, 1) to broadcast over L
            # If tie=False, mask is (B, H, L)
            mask_shape = X.shape[:-1] + (1,) if self.tie else X.shape

            # Original had X.shape[:2], which assumed X was (B, D, ...).
            # If X is (B, H, L), then X.shape[:2] is (B,H) for tied mask over L.
            # mask_shape = X.shape[:2] + (1,) * (X.ndim - 2) if self.tie else X.shape

            mask = (
                torch.rand(*mask_shape, device=X.device) > self.p
            )  # Inverted logic for mask: 1 means keep
            X = X * mask * (1.0 / (1.0 - self.p))  # Scale by 1/(1-p)

            if not self.transposed:
                X = X.transpose(-1, -2)  # Transpose back
            return X
        return X


class S4DKernel(nn.Module):
    def __init__(
        self,
        d_model: int,
        N: int = 64,
        dt_min: float = 0.001,
        dt_max: float = 0.1,
        lr: float = None,
    ):
        super().__init__()
        H = d_model  # Renaming for clarity from original S4 papers
        log_dt = torch.rand(H) * (math.log(dt_max) - math.log(dt_min)) + math.log(
            dt_min
        )

        C_init = torch.randn(H, N // 2, dtype=torch.cfloat)
        self.C = nn.Parameter(_c2r(C_init))
        self._register_param("log_dt", log_dt, lr)

        log_A_real = torch.log(0.5 * torch.ones(H, N // 2))
        A_imag = math.pi * repeat(torch.arange(N // 2), "n -> h n", h=H)
        self._register_param("log_A_real", log_A_real, lr)
        self._register_param("A_imag", A_imag, lr)  # A_imag is not learned typically

    def forward(self, L: int):  # L is sequence length
        dt = torch.exp(self.log_dt)  # (H)
        C = _r2c(self.C)  # (H, N/2) complex
        A = -torch.exp(self.log_A_real) + 1j * self.A_imag  # (H, N/2) complex

        dtA = A * dt.unsqueeze(-1)  # (H, N/2)

        # Kernel calculation (Convolution theorem part)
        # K_conv = C * (e^(dtA) - 1) / A
        # This is part of the HiPPO framework for state space models
        C_times_dt = C * dt.unsqueeze(
            -1
        )  # Element-wise, for HiPPO-LegS this might be different
        # For S4D, the formula using C * (exp(dtA)-1)/A is common.

        # Original S4D computes K using Vandermonde multiplication for HiPPO C_bar
        # For frequency domain kernel:
        # K_f = (C_bar * (omega - A)^-1 * B_bar) # This is for continuous case
        # Discretized version (bilinear transform or ZOH):
        # K_z = C_z * (zI - A_z)^-1 * B_z
        # The provided code uses an explicit time-domain kernel computation K

        # This K seems to be a direct computation of the impulse response
        coeffs = dtA.unsqueeze(-1) * torch.arange(L, device=A.device)  # (H, N/2, L)
        K_complex = torch.einsum(
            "hn,hnl->hl", C * (torch.exp(dtA) - 1.0) / A, torch.exp(coeffs)
        )
        K = 2 * K_complex.real
        return K

    def _register_param(self, name: str, tensor: torch.Tensor, lr: float = None):
        if lr == 0.0:  # Treat as frozen buffer
            self.register_buffer(name, tensor)
        else:
            param = nn.Parameter(tensor)
            self.register_parameter(name, param)
            if lr is not None:  # S4-specific learning rate
                setattr(param, "_optim", {"lr": lr, "weight_decay": 0.0})


class S4D(nn.Module):
    def __init__(
        self,
        d_model: int,
        d_state: int = 64,
        dropout: float = 0.0,
        transposed: bool = True,
        **kernel_args,
    ):
        super().__init__()

        self.h_model = d_model  # H in S4 paper
        self.d_state = d_state  # N in S4 paper
        self.d_output = self.h_model  # Output dim is same as input model dim
        self.transposed = transposed  # If true, input is (B, H, L), else (B, L, H)

        self.D_skip_connection = nn.Parameter(torch.randn(self.h_model))

        self.kernel = S4DKernel(self.h_model, N=self.d_state, **kernel_args)

        # Activation and dropout after convolution and skip connection
        self.activation = nn.GELU()  # Common in S4
        self.dropout_layer = (
            DropoutNd(dropout, transposed=self.transposed)
            if dropout > 0.0
            else nn.Identity()
        )

        # Output linear GLU layer
        self.output_linear = nn.Sequential(
            nn.Conv1d(self.h_model, 2 * self.h_model, kernel_size=1),  # Project to 2*H
            nn.GLU(
                dim=1
            ),  # GLU halves the channel dimension back to H at dim=1 (channel dim for Conv1d)
        )

    def forward(self, u: torch.Tensor, **kwargs):  # u is input sequence
        if not self.transposed:  # Expects (B, H, L)
            u = u.transpose(-1, -2)

        seq_len = u.size(-1)
        k = self.kernel(L=seq_len)  # (H, L)

        # Convolution via FFT
        k_f = torch.fft.rfft(k, n=2 * seq_len)  # (H, L_fft)
        u_f = torch.fft.rfft(u.to(torch.float32), n=2 * seq_len)  # (B, H, L_fft)

        # Element-wise product in frequency domain
        y_conv_f = u_f * k_f.unsqueeze(0)  # Add batch dim to kernel_f
        y_conv = torch.fft.irfft(y_conv_f, n=2 * seq_len)[..., :seq_len]  # (B, H, L)

        # Add D skip connection (direct path for input u)
        y_skip = y_conv + u * self.D_skip_connection.unsqueeze(0).unsqueeze(
            -1
        )  # (B,H,1) broadcast

        # Activation and dropout
        y_activated = self.dropout_layer(self.activation(y_skip))

        # Output linear layer
        y_output = self.output_linear(y_activated)  # (B, H, L)

        if not self.transposed:
            y_output = y_output.transpose(-1, -2)  # (B, L, H)
        return y_output


class S4Model(nn.Module):  # S4MIL Wrapper
    def __init__(
        self,
        in_dim: int,
        n_classes: int,
        embed_dim: int = 512,
        s4_d_state: int = 64,  # S4 N param
        dropout_rate: float = 0.0,  # Dropout for FC and S4D
        activation: str = "gelu",  # Activation for FC
        is_survival: bool = False,
    ):
        super().__init__()
        self.is_survival = is_survival
        self.n_classes = n_classes

        fc1_layers = [nn.Linear(in_dim, embed_dim)]
        fc1_layers.append(get_activation_fn(activation))
        if dropout_rate > 0:  # Dropout after first FC's activation
            fc1_layers.append(nn.Dropout(dropout_rate))
        self.feature_extractor_fc = nn.Sequential(*fc1_layers)

        # S4 block expects input (Batch, SeqLen, Features) if transposed=False
        # or (Batch, Features, SeqLen) if transposed=True.
        # Here, Features = embed_dim, SeqLen = num_instances.
        self.s4_block = nn.Sequential(
            nn.LayerNorm(embed_dim),  # LayerNorm before S4, applied on feature dim
            S4D(
                d_model=embed_dim,
                d_state=s4_d_state,
                dropout=dropout_rate,
                transposed=False,
            ),
        )
        self.classifier = nn.Linear(embed_dim, n_classes)
        self.apply(initialize_weights)

    def forward(self, x: torch.Tensor):
        # x: (batch_size, num_instances, in_dim)
        if x.ndim == 2:  # If single bag (num_instances, in_dim)
            x = x.unsqueeze(0)  # (1, num_instances, in_dim)

        # Instance feature extraction
        instance_features = self.feature_extractor_fc(
            x
        )  # (batch_size, num_instances, embed_dim)

        # S4 processing: input (batch, seq_len=num_instances, features=embed_dim)
        s4_output = self.s4_block(
            instance_features
        )  # (batch_size, num_instances, embed_dim)

        # Max pooling over instances (sequence dimension)
        bag_representation, _ = torch.max(s4_output, dim=1)  # (batch_size, embed_dim)

        logits = self.classifier(bag_representation)  # (batch_size, n_classes)

        # Predictions (highest logit index)
        predictions = torch.topk(logits, 1, dim=1)[1]

        if self.is_survival:
            hazards = torch.sigmoid(logits)
            survival_curves = torch.cumprod(1 - hazards, dim=1)
            return hazards, survival_curves, predictions, None, {}
        else:
            probabilities = F.softmax(logits, dim=1)
            return logits, probabilities, predictions, None, {}



================================================
FILE: sauron/mil_models/TransMIL.py
================================================
from math import ceil

import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
from einops import rearrange, reduce
from torch import einsum

from sauron.utils.generic_utils import initialize_weights  # Keep if used elsewhere

from .activations import get_activation_fn


# --- Nystrom Attention and Transformer Components (largely kept as is) ---
def exists(val):
    return val is not None


def moore_penrose_iter_pinv(x, iters=6):  # Moore-Penrose pseudo-inverse
    device = x.device
    abs_x = torch.abs(x)
    col = abs_x.sum(dim=-1)
    row = abs_x.sum(dim=-2)
    # Ensure denominators are not zero, add small epsilon if necessary
    max_col = torch.max(col, dim=-1, keepdim=True)[0]
    max_row = torch.max(row, dim=-1, keepdim=True)[0]

    # Handle potential division by zero if max_col or max_row is 0
    # This can happen if a landmark becomes all zeros
    z_denom = max_col * max_row
    z_denom = torch.where(z_denom == 0, torch.tensor(1e-8, device=device), z_denom)

    z = x.transpose(-2, -1) / z_denom

    I = torch.eye(x.shape[-1], device=device).unsqueeze(
        0
    )  # Add batch dim for broadcasting

    for _ in range(iters):
        xz = x @ z
        z = 0.25 * z @ (13 * I - (xz @ (15 * I - (xz @ (7 * I - xz)))))
    return z


class NystromAttention(nn.Module):
    def __init__(
        self,
        dim,
        dim_head=64,
        heads=8,
        num_landmarks=256,
        pinv_iterations=6,
        residual=True,
        residual_conv_kernel=33,
        eps=1e-8,
        dropout=0.0,
    ):
        super().__init__()
        self.eps = eps
        inner_dim = heads * dim_head

        self.num_landmarks = num_landmarks
        self.pinv_iterations = pinv_iterations

        self.heads = heads
        self.scale = dim_head**-0.5
        self.to_qkv = nn.Linear(dim, inner_dim * 3, bias=False)

        self.to_out = nn.Sequential(nn.Linear(inner_dim, dim), nn.Dropout(dropout))

        self.residual = residual
        if residual:
            kernel_size = residual_conv_kernel
            padding = residual_conv_kernel // 2
            # Conv2d for (B, H, N, D) type data, applying kernel over N (sequence) dim
            # For residual connection on V, V is (B,H,N,D_head). Conv over N.
            # So Conv1d on (B*H, D_head, N) or Conv2d (B,H,N,1) applied to D_head features over N seq.
            # Original uses Conv2d (heads, heads, (kernel,1)). This implies groups=heads, operating per head.
            # Let's assume it operates on V's sequence dimension.
            # If V is (B, H, N, D_head), transpose to (B, H, D_head, N) for Conv1d
            # Or (B, H*D_head, N) if not grouped by head
            # The original Conv2d (H,H,(K,1)) groups=H, is like H separate Conv1ds on (B,1,N,D_h)
            # This is complex. If `v` is (B,H,N,D_head), want (B,H,N,D_head) out.
            # A depthwise Conv1d for each head might be:
            # Reshape v to (B*H, N, D_head), permute to (B*H, D_head, N), Conv1d, permute back.
            # The current res_conv is (H, H, (K,1)) groups=H. Input (B, H, N, D).
            # This means it expects V to be (B,H,N,1) and D_head=1, which seems unlikely.
            # Or, it's applied to a reshaped V.
            # For now, I will keep the original res_conv structure, but it's a point of caution.
            self.res_conv = nn.Conv2d(
                heads,
                heads,
                (kernel_size, 1),
                padding=(padding, 0),
                groups=heads,
                bias=False,
            )

    def forward(self, x, mask=None, return_attn=False):
        batch_size, num_instances, _ = x.shape
        h = self.heads
        m = self.num_landmarks  # num_landmarks

        # Padding for landmarks
        remainder = num_instances % m
        if remainder > 0:
            padding = m - remainder
            x = F.pad(x, (0, 0, 0, padding), value=0)  # Pad sequence dim (N)
            if exists(mask):  # mask is (B, N)
                mask = F.pad(mask, (0, padding), value=False)

        padded_n = x.shape[1]

        q, k, v = self.to_qkv(x).chunk(3, dim=-1)
        # (B, N, H*D_h) -> (B, H, N, D_h)
        q, k, v = map(lambda t: rearrange(t, "b n (h d) -> b h n d", h=h), (q, k, v))

        if exists(mask):  # mask (B, N_padded) -> (B, 1, N_padded, 1) for broadcasting
            mask_expanded = mask.unsqueeze(1).unsqueeze(-1)
            q = q.masked_fill(~mask_expanded, 0.0)
            k = k.masked_fill(~mask_expanded, 0.0)
            v = v.masked_fill(~mask_expanded, 0.0)

        q = q * self.scale  # Apply scaling factor

        # Landmarks: average pooling to get landmarks
        # q_landmarks: (B, H, M, D_h)
        # reduce from (B, H, N_padded, D_h) to (B, H, M, D_h) by averaging N_padded/M instances
        l_num_groups = padded_n // m
        q_landmarks = reduce(q, "b h (l m) d -> b h l d", "mean", m=m)
        k_landmarks = reduce(k, "b h (l m) d -> b h l d", "mean", m=m)
        # Note: Original uses 'sum' and then divides by 'l' or masked sum. 'mean' is more direct.

        einops_eq = "b h i d, b h j d -> b h i j"  # einsum equation for dot products
        sim1 = einsum(einops_eq, q, k_landmarks)  # (B, H, N_padded, M)
        sim2 = einsum(einops_eq, q_landmarks, k_landmarks)  # (B, H, M, M)
        sim3 = einsum(einops_eq, q_landmarks, k)  # (B, H, M, N_padded)

        # Masking attention scores
        if exists(mask):
            # mask is (B, N_padded)
            # landmark_mask is (B, M)
            landmark_mask = (
                reduce(mask.float(), "b (l m) -> b l", "sum", m=m) > 0
            )  # (B,M) true if any instance in landmark group is not masked

            # Apply masks (mask_value is usually a large negative number)
            mask_value = -torch.finfo(q.dtype).max
            # sim1: q vs k_landmarks. Mask based on q's mask and k_landmarks' mask
            # (B,1,N_pad,1) * (B,1,1,M) -> (B,1,N_pad,M)
            sim1_mask = mask.unsqueeze(1).unsqueeze(-1) * landmark_mask.unsqueeze(
                1
            ).unsqueeze(1)
            sim1.masked_fill_(~sim1_mask, mask_value)

            # sim2: q_landmarks vs k_landmarks
            # (B,1,M,1) * (B,1,1,M) -> (B,1,M,M)
            sim2_mask = landmark_mask.unsqueeze(1).unsqueeze(
                -1
            ) * landmark_mask.unsqueeze(1).unsqueeze(1)
            sim2.masked_fill_(~sim2_mask, mask_value)

            # sim3: q_landmarks vs k
            # (B,1,M,1) * (B,1,1,N_pad) -> (B,1,M,N_pad)
            sim3_mask = landmark_mask.unsqueeze(1).unsqueeze(-1) * mask.unsqueeze(
                1
            ).unsqueeze(1)
            sim3.masked_fill_(~sim3_mask, mask_value)

        attn1 = sim1.softmax(dim=-1)  # (B, H, N_padded, M)
        attn2 = sim2.softmax(dim=-1)  # (B, H, M, M)
        attn3 = sim3.softmax(dim=-1)  # (B, H, M, N_padded)

        attn2_inv = moore_penrose_iter_pinv(attn2, self.pinv_iterations)  # (B, H, M, M)

        # (B,H,N_padded,M) @ (B,H,M,M) @ (B,H,M,N_padded) @ (B,H,N_padded,D_h)
        out = (attn1 @ attn2_inv) @ (attn3 @ v)  # (B, H, N_padded, D_h)

        if self.residual:
            # V is (B,H,N_padded, D_h). res_conv expects (B,H,N,1) effectively or similar.
            # This residual part is tricky. If res_conv is Conv2d(H,H,(K,1),groups=H),
            # it expects input like (B, H, N, D_some_feature=1).
            # A common way for residual in transformers for `v` is just `v` itself or a linear projection of `v`.
            # Assuming the original res_conv structure expects v with D_h=1 or similar.
            # A simple additive residual of v is more standard if res_conv is problematic:
            # out = out + v
            # For now, trying to make original structure work by permuting and squeezing.
            # This part may need careful review based on expected shapes of res_conv.
            # If v is (B, H, N, D_h), res_conv needs to map this to (B, H, N, D_h)
            # For Conv2d(H,H,(K,1),groups=H), input (B,H,N,D_h), permute to (B,H,D_h,N)
            # If we want to convolve over N, maybe (B*D_h, H, N, 1) then sum over D_h?
            # Sticking to simplest interpretation: it applies a conv per head over sequence dimension.
            # Input to Conv2d: (Batch, Channels_in, H_in, W_in)
            # V: (B, H, N, D_h). Treat H as channels, N as height, D_h as width.
            # res_conv Conv2d(H, H, (kernel,1), groups=H) operates on (B, H, N, D_h_as_W).
            # So D_h must be 1 if kernel is (K,1).
            # Let's assume the residual is on a projection of v if D_h > 1, or the original res_conv is intended differently.
            # A simple residual for now for compatibility:
            out = out + v  # Direct residual from v
            # Original: out += self.res_conv(v) # This line is problematic with typical v shapes.
            # If self.res_conv is for (B, H, N, 1) and D_h > 1, this won't work.
            # If v is (B,H,N,D) and res_conv is Conv2d(H,H,(K,1),groups=H), this is applying filter of size (K,1)
            # on a (N,D) feature map, per head. Requires D=1 for kernel W=1.
            # If it's depthwise conv over N: (B*H, D_head, N), apply Conv1d, then reshape.
            # Reverting to original in case it has a specific meaning, but with a warning.
            # This part of NystromAttention is often a simple `out = out + v` or `out = out + self.res_linear(v)`
            # if res_conv (B,H,N,1) expects D_head=1.
            # If D_head > 1, this is likely an issue.
            # For now, let's assume a simple additive residual as it's safer.
            # out = out + v # Safer residual
            # If keeping original:
            # Must ensure v is shaped like (B, H, N, 1) for res_conv to work as written.
            # If v is (B,H,N,D_h) and we want residual, typically a linear layer or direct add.
            # For now, I'll comment out the complex residual. A simple one is better.
            # out_residual = v
            # if self.residual:
            #     # This requires careful shape management for self.res_conv
            #     # A simple solution is a linear layer or direct addition of v
            #     out = out + out_residual # Example: direct add

            # The original paper might use a Conv1D applied channel-wise (depth-wise) on sequence.
            # If v is (B,H,N,D_h), then v_permuted = v.permute(0,1,3,2) is (B,H,D_h,N)
            # Then apply Conv1d to (B*H, D_h, N). For this, res_conv should be Conv1d.
            # Given it's Conv2d, it is unusual. A simple `out = out + v` is common.

        out = rearrange(out, "b h n d -> b n (h d)")  # (B, N_padded, H*D_h)
        out = self.to_out(out)  # (B, N_padded, Dim)

        # Remove padding
        out = out[:, :num_instances, :]

        if return_attn:  # Not used by TransMIL wrapper, but good for analysis
            attn = (attn1 @ attn2_inv) @ attn3
            return out, attn
        return out


class PreNorm(nn.Module):
    def __init__(self, dim, fn):
        super().__init__()
        self.norm = nn.LayerNorm(dim)
        self.fn = fn

    def forward(self, x, **kwargs):
        return self.fn(self.norm(x), **kwargs) + x  # Residual connection


class FeedForward(nn.Module):
    def __init__(self, dim, hidden_dim_mult=4, dropout=0.0):
        super().__init__()
        self.net = nn.Sequential(
            nn.Linear(dim, dim * hidden_dim_mult),
            nn.GELU(),
            nn.Dropout(dropout),
            nn.Linear(dim * hidden_dim_mult, dim),
            nn.Dropout(dropout),  # Dropout after final linear layer in FFN
        )

    def forward(self, x):
        return self.net(x)


class Nystromformer(nn.Module):  # Replaces TransLayer for clarity
    def __init__(
        self,
        dim,
        num_heads,
        dim_head,
        num_landmarks,
        mlp_mult=4,
        dropout_attn=0.1,
        dropout_ff=0.1,
    ):
        super().__init__()
        self.attention_block = PreNorm(
            dim,
            NystromAttention(
                dim=dim,
                heads=num_heads,
                dim_head=dim_head,
                num_landmarks=num_landmarks,
                dropout=dropout_attn,
                residual=True,
            ),
        )
        self.feed_forward_block = PreNorm(
            dim, FeedForward(dim=dim, mult=mlp_mult, dropout=dropout_ff)
        )

    def forward(self, x, mask=None):
        x = self.attention_block(x, mask=mask)  # Includes residual from PreNorm
        x = self.feed_forward_block(x)  # Includes residual from PreNorm
        return x


class PPEG(nn.Module):  # Positional Pixel Embedding Generator
    def __init__(
        self, dim=512, kernel_size=7, groups_factor=1
    ):  # groups_factor to control groups=dim or groups=1
        super().__init__()
        # Using groups=dim makes them depthwise convolutions
        # Using groups=1 makes them standard convolutions mixing channels
        # Original TransMIL paper implies depthwise-like structure for PPEG
        groups = dim // groups_factor if groups_factor > 0 else 1

        self.proj = nn.Conv2d(dim, dim, kernel_size, 1, kernel_size // 2, groups=groups)
        self.proj1 = nn.Conv2d(
            dim, dim, kernel_size - 2, 1, (kernel_size - 2) // 2, groups=groups
        )
        self.proj2 = nn.Conv2d(
            dim, dim, kernel_size - 4, 1, (kernel_size - 4) // 2, groups=groups
        )

    def forward(self, x: torch.Tensor, H: int, W: int):
        # x: (batch_size, num_tokens, dim) where num_tokens = 1 (cls) + H*W (patches)
        batch_size, _, C = x.shape
        cls_token, feat_tokens = (
            x[:, :1],
            x[:, 1:],
        )  # Split CLS token and feature tokens

        # Reshape feature tokens to 2D grid: (B, H*W, C) -> (B, C, H, W)
        cnn_feat = feat_tokens.transpose(1, 2).view(batch_size, C, H, W)

        # Apply convolutions
        x_conv = (
            self.proj(cnn_feat) + self.proj1(cnn_feat) + self.proj2(cnn_feat) + cnn_feat
        )

        # Flatten back: (B, C, H, W) -> (B, H*W, C)
        x_processed_feats = x_conv.flatten(2).transpose(1, 2)

        # Concatenate CLS token back
        x_out = torch.cat((cls_token, x_processed_feats), dim=1)
        return x_out


class TransMIL(nn.Module):
    def __init__(
        self,
        in_dim: int,
        n_classes: int,
        embed_dim: int = 512,
        num_transformer_layers: int = 2,
        num_attn_heads: int = 8,  # num_landmarks usually embed_dim // 2
        dropout_rate: float = 0.1,  # General dropout for FC, attention, FF
        activation: str = "gelu",
        is_survival: bool = False,
    ):
        super().__init__()
        self.is_survival = is_survival
        self.n_classes = n_classes

        fc1_layers = [nn.Linear(in_dim, embed_dim)]
        fc1_layers.append(get_activation_fn(activation))
        if dropout_rate > 0:
            fc1_layers.append(nn.Dropout(dropout_rate))
        self.feature_extractor_fc = nn.Sequential(*fc1_layers)

        self.pos_layer_generator = PPEG(dim=embed_dim)
        self.cls_token = nn.Parameter(torch.randn(1, 1, embed_dim))
        nn.init.normal_(self.cls_token, std=1e-6)  # Initialize CLS token

        # Transformer layers
        self.transformer_layers = nn.ModuleList([])
        dim_head = embed_dim // num_attn_heads
        num_landmarks = embed_dim // 2  # As per original TransMIL settings
        for _ in range(num_transformer_layers):
            self.transformer_layers.append(
                TransformerLayer(
                    dim=embed_dim,
                    num_heads=num_attn_heads,
                    dim_head=dim_head,
                    num_landmarks=num_landmarks,
                    dropout_attn=dropout_rate,
                    dropout_ff=dropout_rate,
                )
            )

        self.final_norm = nn.LayerNorm(embed_dim)
        self.classifier = nn.Linear(embed_dim, n_classes)
        self.apply(initialize_weights)  # Assuming custom weight init

    def forward(self, x: torch.Tensor):
        # x: (batch_size, num_instances, in_dim)
        if x.ndim == 2:  # Single bag
            x = x.unsqueeze(0)
        batch_size = x.shape[0]

        instance_features = self.feature_extractor_fc(x.float())  # (B, N, embed_dim)

        # Prepare for PPEG: pad to make it squarish for H, W calculation
        num_instances = instance_features.shape[1]
        H_approx = W_approx = int(np.ceil(np.sqrt(num_instances)))
        padded_num_features = H_approx * W_approx

        if num_instances < padded_num_features:
            padding_size = padded_num_features - num_instances
            # Pad by repeating last few elements or zero padding
            # Using repeat of initial elements for padding (as in some implementations)
            padding_tensor = instance_features[:, :padding_size, :]
            # Or zero padding: torch.zeros(batch_size, padding_size, instance_features.shape[2], device=x.device)
            h_padded = torch.cat([instance_features, padding_tensor], dim=1)
        else:
            h_padded = instance_features[
                :, :padded_num_features, :
            ]  # Truncate if too many, or ensure N <= H_approx*W_approx

        # Add CLS token
        cls_tokens = self.cls_token.expand(batch_size, -1, -1)  # (B, 1, embed_dim)
        h_with_cls = torch.cat(
            (cls_tokens, h_padded), dim=1
        )  # (B, 1+padded_N, embed_dim)

        # First Transformer Layer (without PPEG)
        if len(self.transformer_layers) > 0:
            h_transformed = self.transformer_layers[0](h_with_cls)
        else:  # Should have at least one layer
            h_transformed = h_with_cls

        # PPEG positional encoding
        h_pos_encoded = self.pos_layer_generator(h_transformed, H_approx, W_approx)

        # Remaining Transformer Layers (if any, after PPEG)
        h_final_transformed = h_pos_encoded
        if len(self.transformer_layers) > 1:
            for i in range(1, len(self.transformer_layers)):
                h_final_transformed = self.transformer_layers[i](h_final_transformed)

        # Get CLS token representation
        cls_representation = self.final_norm(
            h_final_transformed[:, 0]
        )  # (B, embed_dim)

        logits = self.classifier(cls_representation)  # (B, n_classes)

        # Predictions
        predictions = torch.topk(logits, 1, dim=1)[1]

        if self.is_survival:
            hazards = torch.sigmoid(logits)
            survival_curves = torch.cumprod(1 - hazards, dim=1)
            return hazards, survival_curves, predictions, None, {}
        else:
            probabilities = F.softmax(logits, dim=1)
            return logits, probabilities, predictions, None, {}



================================================
FILE: sauron/mil_models/WIKGMIL.py
================================================
from typing import Dict, Union

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch_geometric.nn import GlobalAttention, global_max_pool, global_mean_pool

from sauron.utils.generic_utils import initialize_weights

from .activations import get_activation_fn  # Assuming it's in mil_models/

# torch.autograd.set_detect_anomaly(True) # Should be for debugging, not in library code


class WiKG(nn.Module):
    def __init__(
        self,
        in_dim: int,
        n_classes: int,
        hidden_dim: int = 512,
        top_k_neighbors: int = 6,
        agg_type: str = "bi-interaction",  # "gcn", "sage", "bi-interaction"
        pool_type: str = "attn",  # "mean", "max", "attn"
        dropout_rate: float = 0.3,
        activation: str = "leaky_relu",  # Original used LeakyReLU
        is_survival: bool = False,
    ):
        super().__init__()
        self.is_survival = is_survival
        self.n_classes = n_classes

        # Using LeakyReLU as per original if specified, else map via get_activation_fn
        if activation.lower() == "leaky_relu":
            self.activation_fn = nn.LeakyReLU()
        else:
            self.activation_fn = get_activation_fn(activation)

        self.feature_extractor_fc = nn.Sequential(
            nn.Linear(in_dim, hidden_dim),
            self.activation_fn,  # LeakyReLU or other
        )

        self.W_head = nn.Linear(hidden_dim, hidden_dim)
        self.W_tail = nn.Linear(hidden_dim, hidden_dim)

        self.scale = hidden_dim**-0.5
        self.top_k_neighbors = top_k_neighbors
        self.agg_type = agg_type

        # Gated Knowledge Attention components (Original seems to miss these, but they are usually part of WiKG)
        # Based on typical Gated Attention Units or similar structures
        # For simplicity, I'll follow the provided structure for neighbor aggregation
        # and assume KA (Knowledge Attention) weights are implicitly handled or simplified.
        # If a more complex KA is needed, it would involve more layers.

        # Aggregation layers
        if self.agg_type == "gcn":
            self.agg_linear = nn.Linear(hidden_dim, hidden_dim)
        elif self.agg_type == "sage":
            self.agg_linear = nn.Linear(hidden_dim * 2, hidden_dim)
        elif self.agg_type == "bi-interaction":
            self.agg_linear1 = nn.Linear(hidden_dim, hidden_dim)
            self.agg_linear2 = nn.Linear(hidden_dim, hidden_dim)
        else:
            raise NotImplementedError(f"Aggregation type '{agg_type}' not implemented.")

        self.message_dropout = (
            nn.Dropout(dropout_rate) if dropout_rate > 0 else nn.Identity()
        )
        self.output_norm = nn.LayerNorm(hidden_dim)
        self.classifier_fc = nn.Linear(hidden_dim, n_classes)

        # Readout (Pooling) layer
        if pool_type == "mean":
            self.readout = global_mean_pool
        elif pool_type == "max":
            self.readout = global_max_pool
        elif pool_type == "attn":
            att_net = nn.Sequential(
                nn.Linear(hidden_dim, hidden_dim // 2),
                self.activation_fn,  # LeakyReLU or other
                nn.Linear(hidden_dim // 2, 1),
            )
            self.readout = GlobalAttention(att_net)
        else:
            raise NotImplementedError(f"Pooling type '{pool_type}' not implemented.")

        self.apply(initialize_weights)

    def forward(self, x: Union[torch.Tensor, Dict[str, torch.Tensor]]):
        # x: (batch_size, num_instances, in_dim) or dict {'feature': tensor}
        if isinstance(x, dict):
            x = x.get("feature")
            if x is None:
                raise ValueError("Input dictionary must contain 'feature' key.")

        if x.ndim == 2:  # Single bag
            x = x.unsqueeze(0)

        batch_size, num_instances, _ = x.shape

        instance_features = self.feature_extractor_fc(x)  # (B, N, C_hidden)

        # Instance graph construction / feature smoothing
        # (Original: (x + x.mean(dim=1, keepdim=True)) * 0.5).
        # This averages each instance feature with the mean feature of the bag.
        instance_features = (
            instance_features + instance_features.mean(dim=1, keepdim=True)
        ) * 0.5

        e_h = self.W_head(instance_features)  # (B, N, C_h) "head" features for query
        e_t = self.W_tail(
            instance_features
        )  # (B, N, C_h) "tail" features for key/value

        # Construct neighborhood via attention
        # attn_logit: (B, N, N) - similarity between each pair of instances in a bag
        attn_logit = torch.bmm(e_h * self.scale, e_t.transpose(1, 2))

        # Select top-k neighbors for each instance
        # topk_weights: (B, N, K_neighbors), topk_indices: (B, N, K_neighbors)
        topk_weights, topk_indices = torch.topk(
            attn_logit, k=self.top_k_neighbors, dim=-1
        )

        # Gather features of top-k neighbors
        # e_t is (B, N, C_h). topk_indices is (B, N, K_neigh).
        # We need to gather along dim 1 (instance dimension) for each batch element.
        # expanded_indices = topk_indices.unsqueeze(-1).expand(-1, -1, -1, e_t.shape[-1])
        # neighbor_features = torch.gather(e_t.unsqueeze(2).expand(-1, -1, num_instances, -1), 2, expanded_indices)
        # More direct way using advanced indexing:
        batch_idx_for_gather = torch.arange(batch_size, device=x.device).view(-1, 1, 1)
        neighbor_features = e_t[
            batch_idx_for_gather, topk_indices
        ]  # (B, N, K_neigh, C_h)

        # Softmax for attention probabilities over neighbors
        topk_attention_probs = F.softmax(topk_weights, dim=2)  # (B, N, K_neigh)

        # Weighted sum of neighbor features (eh_r in original)
        # This part from original "eh_r = torch.mul(topk_prob.unsqueeze(-1), Nb_h) + torch.matmul((1 - topk_prob).unsqueeze(-1), e_h.unsqueeze(2))"
        # is unusual for standard attention. A simple weighted sum is more common.
        # eh_r seems like a mix of neighbor features and self features.
        # For now, let's compute aggregated neighbor representation:
        aggregated_neighbors = torch.einsum(
            "bnk,bnkc->bnc", topk_attention_probs, neighbor_features
        )  # (B, N, C_h)

        # The "gated knowledge attention" part (ka_weight, ka_prob, e_Nh) from original
        # seems to be another layer of attention on these neighbors.
        # For simplicity and robustness, using the aggregated_neighbors directly or a simplified KA.
        # Original KA:
        # e_h_expand = e_h.unsqueeze(2).expand(-1, -1, self.top_k_neighbors, -1) # (B, N, K_neigh, C_h)
        # gate_input = torch.tanh(e_h_expand + neighbor_features) # Combine self with neighbors
        # ka_logit = torch.einsum("bnkc,bnkc->bnk", neighbor_features, gate_input) # Dot product
        # ka_attention_probs = F.softmax(ka_logit, dim=2) # (B, N, K_neigh)
        # e_Nh_refined_neighbors = torch.einsum('bnk,bnkc->bnc', ka_attention_probs, neighbor_features) # (B,N,C_h)
        # This e_Nh_refined_neighbors is the `e_Nh` from original. Let's use this refined version.
        e_h_expanded = e_h.unsqueeze(2).expand_as(neighbor_features)
        gate_val = torch.tanh(e_h_expanded + neighbor_features)  # (B,N,K,C)
        ka_weights = torch.sum(neighbor_features * gate_val, dim=-1)  # (B,N,K)
        ka_probs = F.softmax(ka_weights, dim=-1)  # (B,N,K)
        e_Nh = torch.sum(ka_probs.unsqueeze(-1) * neighbor_features, dim=2)  # (B,N,C)

        # Node feature aggregation (GCN, SAGE, Bi-interaction style)
        if self.agg_type == "gcn":
            aggregated_embedding = self.activation_fn(self.agg_linear(e_h + e_Nh))
        elif self.agg_type == "sage":
            concat_embedding = torch.cat([e_h, e_Nh], dim=2)
            aggregated_embedding = self.activation_fn(self.agg_linear(concat_embedding))
        elif self.agg_type == "bi-interaction":
            sum_embedding = self.activation_fn(self.agg_linear1(e_h + e_Nh))
            bi_embedding = self.activation_fn(self.agg_linear2(e_h * e_Nh))
            aggregated_embedding = sum_embedding + bi_embedding
        else:  # Should not happen due to init check
            aggregated_embedding = e_h

        h_messages = self.message_dropout(aggregated_embedding)  # (B, N, C_h)

        # Readout to get bag-level representation
        # Reshape for torch_geometric global pooling: (TotalNodes, Features)
        h_reshaped = h_messages.contiguous().view(-1, h_messages.size(-1))
        # Create batch vector for pooling: [0,0..0, 1,1..1, ..., B-1..B-1]
        batch_vector = torch.arange(batch_size, device=x.device).repeat_interleave(
            num_instances
        )

        bag_representation = self.readout(h_reshaped, batch=batch_vector)  # (B, C_h)
        bag_representation = self.output_norm(bag_representation)
        logits = self.classifier_fc(bag_representation)  # (B, n_classes)

        # Predictions
        predictions = torch.topk(logits, 1, dim=1)[1]

        # A_raw could be topk_attention_probs or ka_probs. Let's use KA.
        # Need to decide which attention score to return, ka_probs is instance-neighbor level.
        # For bag level, maybe average instance attention? For now, None.
        attention_scores_raw = None

        if self.is_survival:
            hazards = torch.sigmoid(logits)
            survival_curves = torch.cumprod(1 - hazards, dim=1)
            return hazards, survival_curves, predictions, attention_scores_raw, {}
        else:
            probabilities = F.softmax(logits, dim=1)
            return logits, probabilities, predictions, attention_scores_raw, {}



================================================
FILE: sauron/mil_models/mamba_ssm/__init__.py
================================================
__version__ = "1.1.2"

from mil_models.mamba_ssm.models.mixer_seq_simple import MambaLMHeadModel
from mil_models.mamba_ssm.modules.bimamba import BiMamba
from mil_models.mamba_ssm.modules.mamba_simple import Mamba
from mil_models.mamba_ssm.modules.srmamba import SRMamba
from mil_models.mamba_ssm.ops.selective_scan_interface import (
    mamba_inner_fn,
    selective_scan_fn,
)



================================================
FILE: sauron/mil_models/mamba_ssm/models/__init__.py
================================================



================================================
FILE: sauron/mil_models/mamba_ssm/models/config_mamba.py
================================================
from dataclasses import dataclass, field


@dataclass
class MambaConfig:

    d_model: int = 2560
    n_layer: int = 64
    vocab_size: int = 50277
    ssm_cfg: dict = field(default_factory=dict)
    rms_norm: bool = True
    residual_in_fp32: bool = True
    fused_add_norm: bool = True
    pad_vocab_size_multiple: int = 8



================================================
FILE: sauron/mil_models/mamba_ssm/models/mixer_seq_simple.py
================================================
# Copyright (c) 2023, Albert Gu, Tri Dao.

import json
import math
import os
from collections import namedtuple
from functools import partial

import torch
import torch.nn as nn

from mamba_ssm.models.config_mamba import MambaConfig
from mamba_ssm.modules.mamba_simple import Block, Mamba
from mamba_ssm.utils.generation import GenerationMixin
from mamba_ssm.utils.hf import load_config_hf, load_state_dict_hf

try:
    from mamba_ssm.ops.triton.layernorm import RMSNorm, layer_norm_fn, rms_norm_fn
except ImportError:
    RMSNorm, layer_norm_fn, rms_norm_fn = None, None, None


def create_block(
    d_model,
    ssm_cfg=None,
    norm_epsilon=1e-5,
    rms_norm=False,
    residual_in_fp32=False,
    fused_add_norm=False,
    layer_idx=None,
    device=None,
    dtype=None,
):
    if ssm_cfg is None:
        ssm_cfg = {}
    factory_kwargs = {"device": device, "dtype": dtype}
    mixer_cls = partial(Mamba, layer_idx=layer_idx, **ssm_cfg, **factory_kwargs)
    norm_cls = partial(
        nn.LayerNorm if not rms_norm else RMSNorm, eps=norm_epsilon, **factory_kwargs
    )
    block = Block(
        d_model,
        mixer_cls,
        norm_cls=norm_cls,
        fused_add_norm=fused_add_norm,
        residual_in_fp32=residual_in_fp32,
    )
    block.layer_idx = layer_idx
    return block


# https://github.com/huggingface/transformers/blob/c28d04e9e252a1a099944e325685f14d242ecdcd/src/transformers/models/gpt2/modeling_gpt2.py#L454
def _init_weights(
    module,
    n_layer,
    initializer_range=0.02,  # Now only used for embedding layer.
    rescale_prenorm_residual=True,
    n_residuals_per_layer=1,  # Change to 2 if we have MLP
):
    if isinstance(module, nn.Linear):
        if module.bias is not None:
            if not getattr(module.bias, "_no_reinit", False):
                nn.init.zeros_(module.bias)
    elif isinstance(module, nn.Embedding):
        nn.init.normal_(module.weight, std=initializer_range)

    if rescale_prenorm_residual:
        # Reinitialize selected weights subject to the OpenAI GPT-2 Paper Scheme:
        #   > A modified initialization which accounts for the accumulation on the residual path with model depth. Scale
        #   > the weights of residual layers at initialization by a factor of 1/√N where N is the # of residual layers.
        #   >   -- GPT-2 :: https://openai.com/blog/better-language-models/
        #
        # Reference (Megatron-LM): https://github.com/NVIDIA/Megatron-LM/blob/main/megatron/model/gpt_model.py
        for name, p in module.named_parameters():
            if name in ["out_proj.weight", "fc2.weight"]:
                # Special Scaled Initialization --> There are 2 Layer Norms per Transformer Block
                # Following Pytorch init, except scale by 1/sqrt(2 * n_layer)
                # We need to reinit p since this code could be called multiple times
                # Having just p *= scale would repeatedly scale it down
                nn.init.kaiming_uniform_(p, a=math.sqrt(5))
                with torch.no_grad():
                    p /= math.sqrt(n_residuals_per_layer * n_layer)


class MixerModel(nn.Module):
    def __init__(
        self,
        d_model: int,
        n_layer: int,
        vocab_size: int,
        ssm_cfg=None,
        norm_epsilon: float = 1e-5,
        rms_norm: bool = False,
        initializer_cfg=None,
        fused_add_norm=False,
        residual_in_fp32=False,
        device=None,
        dtype=None,
    ) -> None:
        factory_kwargs = {"device": device, "dtype": dtype}
        super().__init__()
        self.residual_in_fp32 = residual_in_fp32

        self.embedding = nn.Embedding(vocab_size, d_model, **factory_kwargs)

        # We change the order of residual and layer norm:
        # Instead of LN -> Attn / MLP -> Add, we do:
        # Add -> LN -> Attn / MLP / Mixer, returning both the residual branch (output of Add) and
        # the main branch (output of MLP / Mixer). The model definition is unchanged.
        # This is for performance reason: we can fuse add + layer_norm.
        self.fused_add_norm = fused_add_norm
        if self.fused_add_norm:
            if layer_norm_fn is None or rms_norm_fn is None:
                raise ImportError("Failed to import Triton LayerNorm / RMSNorm kernels")

        self.layers = nn.ModuleList(
            [
                create_block(
                    d_model,
                    ssm_cfg=ssm_cfg,
                    norm_epsilon=norm_epsilon,
                    rms_norm=rms_norm,
                    residual_in_fp32=residual_in_fp32,
                    fused_add_norm=fused_add_norm,
                    layer_idx=i,
                    **factory_kwargs,
                )
                for i in range(n_layer)
            ]
        )

        self.norm_f = (nn.LayerNorm if not rms_norm else RMSNorm)(
            d_model, eps=norm_epsilon, **factory_kwargs
        )

        self.apply(
            partial(
                _init_weights,
                n_layer=n_layer,
                **(initializer_cfg if initializer_cfg is not None else {}),
            )
        )

    def allocate_inference_cache(self, batch_size, max_seqlen, dtype=None, **kwargs):
        return {
            i: layer.allocate_inference_cache(
                batch_size, max_seqlen, dtype=dtype, **kwargs
            )
            for i, layer in enumerate(self.layers)
        }

    def forward(self, input_ids, inference_params=None):
        hidden_states = self.embedding(input_ids)
        residual = None
        for layer in self.layers:
            hidden_states, residual = layer(
                hidden_states, residual, inference_params=inference_params
            )
        if not self.fused_add_norm:
            residual = (
                (hidden_states + residual) if residual is not None else hidden_states
            )
            hidden_states = self.norm_f(residual.to(dtype=self.norm_f.weight.dtype))
        else:
            # Set prenorm=False here since we don't need the residual
            fused_add_norm_fn = (
                rms_norm_fn if isinstance(self.norm_f, RMSNorm) else layer_norm_fn
            )
            hidden_states = fused_add_norm_fn(
                hidden_states,
                self.norm_f.weight,
                self.norm_f.bias,
                eps=self.norm_f.eps,
                residual=residual,
                prenorm=False,
                residual_in_fp32=self.residual_in_fp32,
            )
        return hidden_states


class MambaLMHeadModel(nn.Module, GenerationMixin):
    def __init__(
        self,
        config: MambaConfig,
        initializer_cfg=None,
        device=None,
        dtype=None,
    ) -> None:
        self.config = config
        d_model = config.d_model
        n_layer = config.n_layer
        vocab_size = config.vocab_size
        ssm_cfg = config.ssm_cfg
        rms_norm = config.rms_norm
        residual_in_fp32 = config.residual_in_fp32
        fused_add_norm = config.fused_add_norm
        pad_vocab_size_multiple = config.pad_vocab_size_multiple
        factory_kwargs = {"device": device, "dtype": dtype}

        super().__init__()
        if vocab_size % pad_vocab_size_multiple != 0:
            vocab_size += pad_vocab_size_multiple - (
                vocab_size % pad_vocab_size_multiple
            )
        self.backbone = MixerModel(
            d_model=d_model,
            n_layer=n_layer,
            vocab_size=vocab_size,
            ssm_cfg=ssm_cfg,
            rms_norm=rms_norm,
            initializer_cfg=initializer_cfg,
            fused_add_norm=fused_add_norm,
            residual_in_fp32=residual_in_fp32,
            **factory_kwargs,
        )
        self.lm_head = nn.Linear(d_model, vocab_size, bias=False, **factory_kwargs)

        # Initialize weights and apply final processing
        self.apply(
            partial(
                _init_weights,
                n_layer=n_layer,
                **(initializer_cfg if initializer_cfg is not None else {}),
            )
        )
        self.tie_weights()

    def tie_weights(self):
        self.lm_head.weight = self.backbone.embedding.weight

    def allocate_inference_cache(self, batch_size, max_seqlen, dtype=None, **kwargs):
        return self.backbone.allocate_inference_cache(
            batch_size, max_seqlen, dtype=dtype, **kwargs
        )

    def forward(
        self, input_ids, position_ids=None, inference_params=None, num_last_tokens=0
    ):
        """
        "position_ids" is just to be compatible with Transformer generation. We don't use it.
        num_last_tokens: if > 0, only return the logits for the last n tokens
        """
        hidden_states = self.backbone(input_ids, inference_params=inference_params)
        if num_last_tokens > 0:
            hidden_states = hidden_states[:, -num_last_tokens:]
        lm_logits = self.lm_head(hidden_states)
        CausalLMOutput = namedtuple("CausalLMOutput", ["logits"])
        return CausalLMOutput(logits=lm_logits)

    @classmethod
    def from_pretrained(cls, pretrained_model_name, device=None, dtype=None, **kwargs):
        config_data = load_config_hf(pretrained_model_name)
        config = MambaConfig(**config_data)
        model = cls(config, device=device, dtype=dtype, **kwargs)
        model.load_state_dict(
            load_state_dict_hf(pretrained_model_name, device=device, dtype=dtype)
        )
        return model

    def save_pretrained(self, save_directory):
        """
        Minimal implementation of save_pretrained for MambaLMHeadModel.
        Save the model and its configuration file to a directory.
        """
        # Ensure save_directory exists
        if not os.path.exists(save_directory):
            os.makedirs(save_directory)

        # Save the model's state_dict
        model_path = os.path.join(save_directory, "pytorch_model.bin")
        torch.save(self.state_dict(), model_path)

        # Save the configuration of the model
        config_path = os.path.join(save_directory, "config.json")
        with open(config_path, "w") as f:
            json.dump(self.config.__dict__, f)



================================================
FILE: sauron/mil_models/mamba_ssm/modules/__init__.py
================================================



================================================
FILE: sauron/mil_models/mamba_ssm/modules/bimamba.py
================================================
# Copyright (c) 2023, Tri Dao, Albert Gu.

import math
from typing import Optional

import torch
import torch.nn as nn
import torch.nn.functional as F
from einops import rearrange, repeat
from torch import Tensor

from mamba.mamba_ssm.ops.selective_scan_interface import (
    mamba_inner_fn_no_out_proj,
    selective_scan_fn,
)

try:
    from causal_conv1d import causal_conv1d_fn, causal_conv1d_update
except ImportError:
    causal_conv1d_fn, causal_conv1d_update = None

try:
    from mamba_ssm.ops.triton.selective_state_update import selective_state_update
except ImportError:
    selective_state_update = None

try:
    from mamba_ssm.ops.triton.layernorm import RMSNorm, layer_norm_fn, rms_norm_fn
except ImportError:
    RMSNorm, layer_norm_fn, rms_norm_fn = None, None, None


class TransposeTokenReEmbedding:
    @staticmethod
    def transpose_normal_padding(x, rate):
        x = rearrange(x, "b c l -> b l c")
        B, N, C = x.shape
        value = N // rate
        if N % rate != 0:
            padding_length = (value + 1) * rate - N
            padded_x = torch.nn.functional.pad(x, (0, 0, 0, padding_length))
        else:
            padded_x = x
        x_ = rearrange(padded_x, "b (k w) d -> b (w k) d", w=rate)
        x_ = rearrange(x_, "b l c -> b c l")
        return x_

    @staticmethod
    def transpose_remove_padding(x, rate, length):
        x = rearrange(x, "b c l -> b l c")
        x = rearrange(x, "b (w k) d -> b (k w) d", w=rate)
        x = x[:, :length, :]
        x = rearrange(x, "b l c -> b c l")
        return x


class BiMamba(nn.Module):
    def __init__(
        self,
        d_model,
        d_state=16,
        d_conv=4,
        expand=2,
        dt_rank="auto",
        dt_min=0.001,
        dt_max=0.1,
        dt_init="random",
        dt_scale=1.0,
        dt_init_floor=1e-4,
        conv_bias=True,
        bias=False,
        use_fast_path=True,  # Fused kernel options
        layer_idx=None,
        device=None,
        dtype=None,
    ):
        factory_kwargs = {"device": device, "dtype": dtype}
        super().__init__()
        self.d_model = d_model
        self.d_state = d_state
        self.d_conv = d_conv
        self.expand = expand
        self.d_inner = int(self.expand * self.d_model)
        self.dt_rank = math.ceil(self.d_model / 16) if dt_rank == "auto" else dt_rank
        self.use_fast_path = use_fast_path
        self.layer_idx = layer_idx

        self.in_proj = nn.Linear(
            self.d_model, self.d_inner * 2, bias=bias, **factory_kwargs
        )

        self.conv1d = nn.Conv1d(
            in_channels=self.d_inner,
            out_channels=self.d_inner,
            bias=conv_bias,
            kernel_size=d_conv,
            groups=self.d_inner,
            padding=d_conv - 1,
            **factory_kwargs,
        )
        self.conv1d_b = nn.Conv1d(
            in_channels=self.d_inner,
            out_channels=self.d_inner,
            bias=conv_bias,
            kernel_size=d_conv,
            groups=self.d_inner,
            padding=d_conv - 1,
            **factory_kwargs,
        )

        self.activation = "silu"
        self.act = nn.SiLU()

        self.x_proj = nn.Linear(
            self.d_inner, self.dt_rank + self.d_state * 2, bias=False, **factory_kwargs
        )
        self.x_proj_b = nn.Linear(
            self.d_inner, self.dt_rank + self.d_state * 2, bias=False, **factory_kwargs
        )

        self.dt_proj = nn.Linear(
            self.dt_rank, self.d_inner, bias=True, **factory_kwargs
        )
        self.dt_proj_b = nn.Linear(
            self.dt_rank, self.d_inner, bias=True, **factory_kwargs
        )

        # Initialize special dt projection to preserve variance at initialization
        dt_init_std = self.dt_rank**-0.5 * dt_scale
        if dt_init == "constant":
            nn.init.constant_(self.dt_proj.weight, dt_init_std)
            nn.init.constant_(self.dt_proj_b.weight, dt_init_std)  # ?
        elif dt_init == "random":
            nn.init.uniform_(self.dt_proj.weight, -dt_init_std, dt_init_std)
            nn.init.uniform_(self.dt_proj_b.weight, -dt_init_std, dt_init_std)  # ?
        else:
            raise NotImplementedError

        # Initialize dt bias so that F.softplus(dt_bias) is between dt_min and dt_max
        dt = torch.exp(
            torch.rand(self.d_inner, **factory_kwargs)
            * (math.log(dt_max) - math.log(dt_min))
            + math.log(dt_min)
        ).clamp(min=dt_init_floor)
        # Inverse of softplus: https://github.com/pytorch/pytorch/issues/72759
        inv_dt = dt + torch.log(-torch.expm1(-dt))
        with torch.no_grad():
            self.dt_proj.bias.copy_(inv_dt)
            self.dt_proj_b.bias.copy_(inv_dt)  # ?
        # Our initialization would set all Linear.bias to zero, need to mark this one as _no_reinit
        self.dt_proj.bias._no_reinit = True
        self.dt_proj_b.bias._no_reinit = True  # ?

        # S4D real initialization
        A = repeat(
            torch.arange(1, self.d_state + 1, dtype=torch.float32, device=device),
            "n -> d n",
            d=self.d_inner,
        ).contiguous()
        A_log = torch.log(A)  # Keep A_log in fp32
        self.A_log = nn.Parameter(A_log)
        self.A_log._no_weight_decay = True

        A_b = repeat(
            torch.arange(1, self.d_state + 1, dtype=torch.float32, device=device),
            "n -> d n",
            d=self.d_inner,
        ).contiguous()
        A_b_log = torch.log(A_b)  # Keep A_b_log in fp32
        self.A_b_log = nn.Parameter(A_b_log)
        self.A_b_log._no_weight_decay = True

        # D "skip" parameter
        self.D = nn.Parameter(torch.ones(self.d_inner, device=device))  # Keep in fp32
        self.D._no_weight_decay = True
        self.D_b = nn.Parameter(torch.ones(self.d_inner, device=device))  # Keep in fp32
        self.D_b._no_weight_decay = True

        self.out_proj = nn.Linear(
            self.d_inner, self.d_model, bias=bias, **factory_kwargs
        )

    def forward(self, hidden_states, inference_params=None, rate=10):
        """
        hidden_states: (B, L, D)
        Returns: same shape as hidden_states
        """
        batch, seqlen, dim = hidden_states.shape

        conv_state, ssm_state = None, None
        if inference_params is not None:
            conv_state, ssm_state = self._get_states_from_cache(inference_params, batch)
            if inference_params.seqlen_offset > 0:
                # The states are updated inplace
                out, _, _ = self.step(hidden_states, conv_state, ssm_state)
                return out

        # We do matmul and transpose BLH -> HBL at the same time
        xz = rearrange(
            self.in_proj.weight @ rearrange(hidden_states, "b l d -> d (b l)"),
            "d (b l) -> b d l",
            l=seqlen,
        )
        if self.in_proj.bias is not None:
            xz = xz + rearrange(self.in_proj.bias.to(dtype=xz.dtype), "d -> d 1")

        A = -torch.exp(self.A_log.float())  # (d_inner, d_state)
        A_b = -torch.exp(self.A_b_log.float())
        # In the backward pass we write dx and dz next to each other to avoid torch.cat
        if (
            self.use_fast_path and inference_params is None
        ):  # Doesn't support outputting the states
            out = mamba_inner_fn_no_out_proj(
                xz,
                self.conv1d.weight,
                self.conv1d.bias,
                self.x_proj.weight,
                self.dt_proj.weight,
                A,
                None,  # input-dependent B
                None,  # input-dependent C
                self.D.float(),
                delta_bias=self.dt_proj.bias.float(),
                delta_softplus=True,
            )
            xz_b = xz.flip([-1])
            out_b = mamba_inner_fn_no_out_proj(
                xz_b,
                self.conv1d_b.weight,
                self.conv1d_b.bias,
                self.x_proj_b.weight,
                self.dt_proj_b.weight,
                A_b,
                None,
                None,
                self.D_b.float(),
                delta_bias=self.dt_proj_b.bias.float(),
                delta_softplus=True,
            )
            out_b = out_b.flip([-1])
            out = F.linear(
                rearrange(out + out_b, "b d l -> b l d"),
                self.out_proj.weight,
                self.out_proj.bias,
            )
        else:
            # x, z
            x, z = xz.chunk(2, dim=1)
            x_b = x.flip([-1])
            # Compute short convolution
            if conv_state is not None:
                # If we just take x[:, :, -self.d_conv :], it will error if seqlen < self.d_conv
                # Instead F.pad will pad with zeros if seqlen < self.d_conv, and truncate otherwise.
                conv_state.copy_(
                    F.pad(x, (self.d_conv - x.shape[-1], 0))
                )  # Update state (B D W)
            if causal_conv1d_fn is None:
                x = self.act(self.conv1d(x)[..., :seqlen])
                x_b = self.act(self.conv1d_b(x_b)[..., :seqlen])
            else:
                assert self.activation in ["silu", "swish"]
                x = causal_conv1d_fn(
                    x=x,
                    weight=rearrange(self.conv1d.weight, "d 1 w -> d w"),
                    bias=self.conv1d.bias,
                    activation=self.activation,
                )
                x_b = causal_conv1d_fn(
                    x=x_b,
                    weight=rearrange(self.conv1d_b.weight, "d 1 w -> d w"),
                    bias=self.conv1d_b.bias,
                    activation=self.activation,
                )

            # We're careful here about the layout, to avoid extra transposes.
            # We want dt to have d as the slowest moving dimension
            # and L as the fastest moving dimension, since those are what the ssm_scan kernel expects.
            x_dbl = self.x_proj(rearrange(x, "b d l -> (b l) d"))  # (bl d)
            dt, B, C = torch.split(
                x_dbl, [self.dt_rank, self.d_state, self.d_state], dim=-1
            )
            dt = self.dt_proj.weight @ dt.t()
            dt = rearrange(dt, "d (b l) -> b d l", l=seqlen)
            B = rearrange(B, "(b l) dstate -> b dstate l", l=seqlen).contiguous()
            C = rearrange(C, "(b l) dstate -> b dstate l", l=seqlen).contiguous()

            x_dbl_b = self.x_proj_b(rearrange(x_b, "b d l -> (b l) d"))  # (bl d)
            dt_b, B_b, C_b = torch.split(
                x_dbl_b, [self.dt_rank, self.d_state, self.d_state], dim=-1
            )
            dt_b = self.dt_proj_b.weight @ dt_b.t()
            dt_b = rearrange(dt_b, "d (b l) -> b d l", l=seqlen)
            B_b = rearrange(B_b, "(b l) dstate -> b dstate l", l=seqlen).contiguous()
            C_b = rearrange(C_b, "(b l) dstate -> b dstate l", l=seqlen).contiguous()
            assert self.activation in ["silu", "swish"]
            y = selective_scan_fn(
                x,
                dt,
                A,
                B,
                C,
                self.D.float(),
                z=z,
                delta_bias=self.dt_proj.bias.float(),
                delta_softplus=True,
                return_last_state=ssm_state is not None,
            )
            y_b = selective_scan_fn(
                x_b,
                dt_b,
                A_b,
                B_b,
                C_b,
                self.D_b.float(),
                z=z,
                delta_bias=self.dt_proj_b.bias.float(),
                delta_softplus=True,
                return_last_state=ssm_state is not None,
            )
            if ssm_state is not None:
                y, last_state = y
                ssm_state.copy_(last_state)
            y = rearrange(y, "b d l -> b l d")
            y_b = rearrange(y_b, "b d l -> b l d")
            out = self.out_proj(y_b + y)
        return out

    def step(self, hidden_states, conv_state, ssm_state):
        dtype = hidden_states.dtype
        assert (
            hidden_states.shape[1] == 1
        ), "Only support decoding with 1 token at a time for now"
        xz = self.in_proj(hidden_states.squeeze(1))  # (B 2D)
        x, z = xz.chunk(2, dim=-1)  # (B D)

        # Conv step
        if causal_conv1d_update is None:
            conv_state.copy_(
                torch.roll(conv_state, shifts=-1, dims=-1)
            )  # Update state (B D W)
            conv_state[:, :, -1] = x
            x = torch.sum(
                conv_state * rearrange(self.conv1d.weight, "d 1 w -> d w"), dim=-1
            )  # (B D)
            if self.conv1d.bias is not None:
                x = x + self.conv1d.bias
            x = self.act(x).to(dtype=dtype)
        else:
            x = causal_conv1d_update(
                x,
                conv_state,
                rearrange(self.conv1d.weight, "d 1 w -> d w"),
                self.conv1d.bias,
                self.activation,
            )

        x_db = self.x_proj(x)  # (B dt_rank+2*d_state)
        dt, B, C = torch.split(x_db, [self.dt_rank, self.d_state, self.d_state], dim=-1)
        # Don't add dt_bias here
        dt = F.linear(dt, self.dt_proj.weight)  # (B d_inner)
        A = -torch.exp(self.A_log.float())  # (d_inner, d_state)

        # SSM step
        if selective_state_update is None:
            # Discretize A and B
            dt = F.softplus(dt + self.dt_proj.bias.to(dtype=dt.dtype))
            dA = torch.exp(torch.einsum("bd,dn->bdn", dt, A))
            dB = torch.einsum("bd,bn->bdn", dt, B)
            ssm_state.copy_(ssm_state * dA + rearrange(x, "b d -> b d 1") * dB)
            y = torch.einsum("bdn,bn->bd", ssm_state.to(dtype), C)
            y = y + self.D.to(dtype) * x
            y = y * self.act(z)  # (B D)
        else:
            y = selective_state_update(
                ssm_state,
                x,
                dt,
                A,
                B,
                C,
                self.D,
                z=z,
                dt_bias=self.dt_proj.bias,
                dt_softplus=True,
            )

        out = self.out_proj(y)
        return out.unsqueeze(1), conv_state, ssm_state

    def allocate_inference_cache(self, batch_size, max_seqlen, dtype=None, **kwargs):
        device = self.out_proj.weight.device
        conv_dtype = self.conv1d.weight.dtype if dtype is None else dtype
        conv_state = torch.zeros(
            batch_size,
            self.d_model * self.expand,
            self.d_conv,
            device=device,
            dtype=conv_dtype,
        )
        ssm_dtype = self.dt_proj.weight.dtype if dtype is None else dtype
        # ssm_dtype = torch.float32
        ssm_state = torch.zeros(
            batch_size,
            self.d_model * self.expand,
            self.d_state,
            device=device,
            dtype=ssm_dtype,
        )
        return conv_state, ssm_state

    def _get_states_from_cache(
        self, inference_params, batch_size, initialize_states=False
    ):
        assert self.layer_idx is not None
        if self.layer_idx not in inference_params.key_value_memory_dict:
            batch_shape = (batch_size,)
            conv_state = torch.zeros(
                batch_size,
                self.d_model * self.expand,
                self.d_conv,
                device=self.conv1d.weight.device,
                dtype=self.conv1d.weight.dtype,
            )
            ssm_state = torch.zeros(
                batch_size,
                self.d_model * self.expand,
                self.d_state,
                device=self.dt_proj.weight.device,
                dtype=self.dt_proj.weight.dtype,
                # dtype=torch.float32,
            )
            inference_params.key_value_memory_dict[self.layer_idx] = (
                conv_state,
                ssm_state,
            )
        else:
            conv_state, ssm_state = inference_params.key_value_memory_dict[
                self.layer_idx
            ]
            # TODO: What if batch size changes between generation, and we reuse the same states?
            if initialize_states:
                conv_state.zero_()
                ssm_state.zero_()
        return conv_state, ssm_state


class Block(nn.Module):
    def __init__(
        self,
        dim,
        mixer_cls,
        norm_cls=nn.LayerNorm,
        fused_add_norm=False,
        residual_in_fp32=False,
    ):
        """
        Simple block wrapping a mixer class with LayerNorm/RMSNorm and residual connection"

        This Block has a slightly different structure compared to a regular
        prenorm Transformer block.
        The standard block is: LN -> MHA/MLP -> Add.
        [Ref: https://arxiv.org/abs/2002.04745]
        Here we have: Add -> LN -> Mixer, returning both
        the hidden_states (output of the mixer) and the residual.
        This is purely for performance reasons, as we can fuse add and LayerNorm.
        The residual needs to be provided (except for the very first block).
        """
        super().__init__()
        self.residual_in_fp32 = residual_in_fp32
        self.fused_add_norm = fused_add_norm
        self.mixer = mixer_cls(dim)
        self.norm = norm_cls(dim)
        if self.fused_add_norm:
            assert RMSNorm is not None, "RMSNorm import fails"
            assert isinstance(
                self.norm, (nn.LayerNorm, RMSNorm)
            ), "Only LayerNorm and RMSNorm are supported for fused_add_norm"

    def forward(
        self,
        hidden_states: Tensor,
        residual: Optional[Tensor] = None,
        inference_params=None,
    ):
        r"""Pass the input through the encoder layer.

        Args:
            hidden_states: the sequence to the encoder layer (required).
            residual: hidden_states = Mixer(LN(residual))
        """
        if not self.fused_add_norm:
            residual = (
                (hidden_states + residual) if residual is not None else hidden_states
            )
            hidden_states = self.norm(residual.to(dtype=self.norm.weight.dtype))
            if self.residual_in_fp32:
                residual = residual.to(torch.float32)
        else:
            fused_add_norm_fn = (
                rms_norm_fn if isinstance(self.norm, RMSNorm) else layer_norm_fn
            )
            hidden_states, residual = fused_add_norm_fn(
                hidden_states,
                self.norm.weight,
                self.norm.bias,
                residual=residual,
                prenorm=True,
                residual_in_fp32=self.residual_in_fp32,
                eps=self.norm.eps,
            )
        hidden_states = self.mixer(hidden_states, inference_params=inference_params)
        return hidden_states, residual

    def allocate_inference_cache(self, batch_size, max_seqlen, dtype=None, **kwargs):
        return self.mixer.allocate_inference_cache(
            batch_size, max_seqlen, dtype=dtype, **kwargs
        )



================================================
FILE: sauron/mil_models/mamba_ssm/modules/mamba_simple.py
================================================
# Copyright (c) 2023, Tri Dao, Albert Gu.

import math
from typing import Optional

import torch
import torch.nn as nn
import torch.nn.functional as F
from einops import rearrange, repeat
from torch import Tensor

from mamba_ssm.ops.selective_scan_interface import mamba_inner_fn, selective_scan_fn

try:
    from causal_conv1d import causal_conv1d_fn, causal_conv1d_update
except ImportError:
    causal_conv1d_fn, causal_conv1d_update = None

try:
    from mamba_ssm.ops.triton.selective_state_update import selective_state_update
except ImportError:
    selective_state_update = None

try:
    from mamba_ssm.ops.triton.layernorm import RMSNorm, layer_norm_fn, rms_norm_fn
except ImportError:
    RMSNorm, layer_norm_fn, rms_norm_fn = None, None, None


class Mamba(nn.Module):
    def __init__(
        self,
        d_model,
        d_state=16,
        d_conv=4,
        expand=2,
        dt_rank="auto",
        dt_min=0.001,
        dt_max=0.1,
        dt_init="random",
        dt_scale=1.0,
        dt_init_floor=1e-4,
        conv_bias=True,
        bias=False,
        use_fast_path=True,  # Fused kernel options
        layer_idx=None,
        device=None,
        dtype=None,
    ):
        factory_kwargs = {"device": device, "dtype": dtype}
        super().__init__()
        self.d_model = d_model
        self.d_state = d_state
        self.d_conv = d_conv
        self.expand = expand
        self.d_inner = int(self.expand * self.d_model)
        self.dt_rank = math.ceil(self.d_model / 16) if dt_rank == "auto" else dt_rank
        self.use_fast_path = use_fast_path
        self.layer_idx = layer_idx

        self.in_proj = nn.Linear(
            self.d_model, self.d_inner * 2, bias=bias, **factory_kwargs
        )

        self.conv1d = nn.Conv1d(
            in_channels=self.d_inner,
            out_channels=self.d_inner,
            bias=conv_bias,
            kernel_size=d_conv,
            groups=self.d_inner,
            padding=d_conv - 1,
            **factory_kwargs,
        )

        self.activation = "silu"
        self.act = nn.SiLU()

        self.x_proj = nn.Linear(
            self.d_inner, self.dt_rank + self.d_state * 2, bias=False, **factory_kwargs
        )
        self.dt_proj = nn.Linear(
            self.dt_rank, self.d_inner, bias=True, **factory_kwargs
        )

        # Initialize special dt projection to preserve variance at initialization
        dt_init_std = self.dt_rank**-0.5 * dt_scale
        if dt_init == "constant":
            nn.init.constant_(self.dt_proj.weight, dt_init_std)
        elif dt_init == "random":
            nn.init.uniform_(self.dt_proj.weight, -dt_init_std, dt_init_std)
        else:
            raise NotImplementedError

        # Initialize dt bias so that F.softplus(dt_bias) is between dt_min and dt_max
        dt = torch.exp(
            torch.rand(self.d_inner, **factory_kwargs)
            * (math.log(dt_max) - math.log(dt_min))
            + math.log(dt_min)
        ).clamp(min=dt_init_floor)
        # Inverse of softplus: https://github.com/pytorch/pytorch/issues/72759
        inv_dt = dt + torch.log(-torch.expm1(-dt))
        with torch.no_grad():
            self.dt_proj.bias.copy_(inv_dt)
        # Our initialization would set all Linear.bias to zero, need to mark this one as _no_reinit
        self.dt_proj.bias._no_reinit = True

        # S4D real initialization
        A = repeat(
            torch.arange(1, self.d_state + 1, dtype=torch.float32, device=device),
            "n -> d n",
            d=self.d_inner,
        ).contiguous()
        A_log = torch.log(A)  # Keep A_log in fp32
        self.A_log = nn.Parameter(A_log)
        self.A_log._no_weight_decay = True

        # D "skip" parameter
        self.D = nn.Parameter(torch.ones(self.d_inner, device=device))  # Keep in fp32
        self.D._no_weight_decay = True

        self.out_proj = nn.Linear(
            self.d_inner, self.d_model, bias=bias, **factory_kwargs
        )

    def forward(self, hidden_states, inference_params=None):
        """
        hidden_states: (B, L, D)
        Returns: same shape as hidden_states
        """
        batch, seqlen, dim = hidden_states.shape

        conv_state, ssm_state = None, None
        if inference_params is not None:
            conv_state, ssm_state = self._get_states_from_cache(inference_params, batch)
            if inference_params.seqlen_offset > 0:
                # The states are updated inplace
                out, _, _ = self.step(hidden_states, conv_state, ssm_state)
                return out

        # We do matmul and transpose BLH -> HBL at the same time
        xz = rearrange(
            self.in_proj.weight @ rearrange(hidden_states, "b l d -> d (b l)"),
            "d (b l) -> b d l",
            l=seqlen,
        )
        if self.in_proj.bias is not None:
            xz = xz + rearrange(self.in_proj.bias.to(dtype=xz.dtype), "d -> d 1")

        A = -torch.exp(self.A_log.float())  # (d_inner, d_state)
        # In the backward pass we write dx and dz next to each other to avoid torch.cat
        if (
            self.use_fast_path and inference_params is None
        ):  # Doesn't support outputting the states
            out = mamba_inner_fn(
                xz,
                self.conv1d.weight,
                self.conv1d.bias,
                self.x_proj.weight,
                self.dt_proj.weight,
                self.out_proj.weight,
                self.out_proj.bias,
                A,
                None,  # input-dependent B
                None,  # input-dependent C
                self.D.float(),
                delta_bias=self.dt_proj.bias.float(),
                delta_softplus=True,
            )
        else:
            x, z = xz.chunk(2, dim=1)
            # Compute short convolution
            if conv_state is not None:
                # If we just take x[:, :, -self.d_conv :], it will error if seqlen < self.d_conv
                # Instead F.pad will pad with zeros if seqlen < self.d_conv, and truncate otherwise.
                conv_state.copy_(
                    F.pad(x, (self.d_conv - x.shape[-1], 0))
                )  # Update state (B D W)
            if causal_conv1d_fn is None:
                x = self.act(self.conv1d(x)[..., :seqlen])
            else:
                assert self.activation in ["silu", "swish"]
                x = causal_conv1d_fn(
                    x=x,
                    weight=rearrange(self.conv1d.weight, "d 1 w -> d w"),
                    bias=self.conv1d.bias,
                    activation=self.activation,
                )

            # We're careful here about the layout, to avoid extra transposes.
            # We want dt to have d as the slowest moving dimension
            # and L as the fastest moving dimension, since those are what the ssm_scan kernel expects.
            x_dbl = self.x_proj(rearrange(x, "b d l -> (b l) d"))  # (bl d)
            dt, B, C = torch.split(
                x_dbl, [self.dt_rank, self.d_state, self.d_state], dim=-1
            )
            dt = self.dt_proj.weight @ dt.t()
            dt = rearrange(dt, "d (b l) -> b d l", l=seqlen)
            B = rearrange(B, "(b l) dstate -> b dstate l", l=seqlen).contiguous()
            C = rearrange(C, "(b l) dstate -> b dstate l", l=seqlen).contiguous()
            assert self.activation in ["silu", "swish"]
            y = selective_scan_fn(
                x,
                dt,
                A,
                B,
                C,
                self.D.float(),
                z=z,
                delta_bias=self.dt_proj.bias.float(),
                delta_softplus=True,
                return_last_state=ssm_state is not None,
            )
            if ssm_state is not None:
                y, last_state = y
                ssm_state.copy_(last_state)
            y = rearrange(y, "b d l -> b l d")
            out = self.out_proj(y)
        return out

    def step(self, hidden_states, conv_state, ssm_state):
        dtype = hidden_states.dtype
        assert (
            hidden_states.shape[1] == 1
        ), "Only support decoding with 1 token at a time for now"
        xz = self.in_proj(hidden_states.squeeze(1))  # (B 2D)
        x, z = xz.chunk(2, dim=-1)  # (B D)

        # Conv step
        if causal_conv1d_update is None:
            conv_state.copy_(
                torch.roll(conv_state, shifts=-1, dims=-1)
            )  # Update state (B D W)
            conv_state[:, :, -1] = x
            x = torch.sum(
                conv_state * rearrange(self.conv1d.weight, "d 1 w -> d w"), dim=-1
            )  # (B D)
            if self.conv1d.bias is not None:
                x = x + self.conv1d.bias
            x = self.act(x).to(dtype=dtype)
        else:
            x = causal_conv1d_update(
                x,
                conv_state,
                rearrange(self.conv1d.weight, "d 1 w -> d w"),
                self.conv1d.bias,
                self.activation,
            )

        x_db = self.x_proj(x)  # (B dt_rank+2*d_state)
        dt, B, C = torch.split(x_db, [self.dt_rank, self.d_state, self.d_state], dim=-1)
        # Don't add dt_bias here
        dt = F.linear(dt, self.dt_proj.weight)  # (B d_inner)
        A = -torch.exp(self.A_log.float())  # (d_inner, d_state)

        # SSM step
        if selective_state_update is None:
            # Discretize A and B
            dt = F.softplus(dt + self.dt_proj.bias.to(dtype=dt.dtype))
            dA = torch.exp(torch.einsum("bd,dn->bdn", dt, A))
            dB = torch.einsum("bd,bn->bdn", dt, B)
            ssm_state.copy_(ssm_state * dA + rearrange(x, "b d -> b d 1") * dB)
            y = torch.einsum("bdn,bn->bd", ssm_state.to(dtype), C)
            y = y + self.D.to(dtype) * x
            y = y * self.act(z)  # (B D)
        else:
            y = selective_state_update(
                ssm_state,
                x,
                dt,
                A,
                B,
                C,
                self.D,
                z=z,
                dt_bias=self.dt_proj.bias,
                dt_softplus=True,
            )

        out = self.out_proj(y)
        return out.unsqueeze(1), conv_state, ssm_state

    def allocate_inference_cache(self, batch_size, max_seqlen, dtype=None, **kwargs):
        device = self.out_proj.weight.device
        conv_dtype = self.conv1d.weight.dtype if dtype is None else dtype
        conv_state = torch.zeros(
            batch_size,
            self.d_model * self.expand,
            self.d_conv,
            device=device,
            dtype=conv_dtype,
        )
        ssm_dtype = self.dt_proj.weight.dtype if dtype is None else dtype
        # ssm_dtype = torch.float32
        ssm_state = torch.zeros(
            batch_size,
            self.d_model * self.expand,
            self.d_state,
            device=device,
            dtype=ssm_dtype,
        )
        return conv_state, ssm_state

    def _get_states_from_cache(
        self, inference_params, batch_size, initialize_states=False
    ):
        assert self.layer_idx is not None
        if self.layer_idx not in inference_params.key_value_memory_dict:
            batch_shape = (batch_size,)
            conv_state = torch.zeros(
                batch_size,
                self.d_model * self.expand,
                self.d_conv,
                device=self.conv1d.weight.device,
                dtype=self.conv1d.weight.dtype,
            )
            ssm_state = torch.zeros(
                batch_size,
                self.d_model * self.expand,
                self.d_state,
                device=self.dt_proj.weight.device,
                dtype=self.dt_proj.weight.dtype,
                # dtype=torch.float32,
            )
            inference_params.key_value_memory_dict[self.layer_idx] = (
                conv_state,
                ssm_state,
            )
        else:
            conv_state, ssm_state = inference_params.key_value_memory_dict[
                self.layer_idx
            ]
            # TODO: What if batch size changes between generation, and we reuse the same states?
            if initialize_states:
                conv_state.zero_()
                ssm_state.zero_()
        return conv_state, ssm_state


class Block(nn.Module):
    def __init__(
        self,
        dim,
        mixer_cls,
        norm_cls=nn.LayerNorm,
        fused_add_norm=False,
        residual_in_fp32=False,
    ):
        """
        Simple block wrapping a mixer class with LayerNorm/RMSNorm and residual connection"

        This Block has a slightly different structure compared to a regular
        prenorm Transformer block.
        The standard block is: LN -> MHA/MLP -> Add.
        [Ref: https://arxiv.org/abs/2002.04745]
        Here we have: Add -> LN -> Mixer, returning both
        the hidden_states (output of the mixer) and the residual.
        This is purely for performance reasons, as we can fuse add and LayerNorm.
        The residual needs to be provided (except for the very first block).
        """
        super().__init__()
        self.residual_in_fp32 = residual_in_fp32
        self.fused_add_norm = fused_add_norm
        self.mixer = mixer_cls(dim)
        self.norm = norm_cls(dim)
        if self.fused_add_norm:
            assert RMSNorm is not None, "RMSNorm import fails"
            assert isinstance(
                self.norm, (nn.LayerNorm, RMSNorm)
            ), "Only LayerNorm and RMSNorm are supported for fused_add_norm"

    def forward(
        self,
        hidden_states: Tensor,
        residual: Optional[Tensor] = None,
        inference_params=None,
    ):
        r"""Pass the input through the encoder layer.

        Args:
            hidden_states: the sequence to the encoder layer (required).
            residual: hidden_states = Mixer(LN(residual))
        """
        if not self.fused_add_norm:
            residual = (
                (hidden_states + residual) if residual is not None else hidden_states
            )
            hidden_states = self.norm(residual.to(dtype=self.norm.weight.dtype))
            if self.residual_in_fp32:
                residual = residual.to(torch.float32)
        else:
            fused_add_norm_fn = (
                rms_norm_fn if isinstance(self.norm, RMSNorm) else layer_norm_fn
            )
            hidden_states, residual = fused_add_norm_fn(
                hidden_states,
                self.norm.weight,
                self.norm.bias,
                residual=residual,
                prenorm=True,
                residual_in_fp32=self.residual_in_fp32,
                eps=self.norm.eps,
            )
        hidden_states = self.mixer(hidden_states, inference_params=inference_params)
        return hidden_states, residual

    def allocate_inference_cache(self, batch_size, max_seqlen, dtype=None, **kwargs):
        return self.mixer.allocate_inference_cache(
            batch_size, max_seqlen, dtype=dtype, **kwargs
        )



================================================
FILE: sauron/mil_models/mamba_ssm/modules/srmamba.py
================================================
# Copyright (c) 2023, Tri Dao, Albert Gu.

import math
from typing import Optional

import torch
import torch.nn as nn
import torch.nn.functional as F
from einops import rearrange, repeat
from torch import Tensor

from mamba.mamba_ssm.ops.selective_scan_interface import (
    mamba_inner_fn_no_out_proj,
    selective_scan_fn,
)

try:
    from causal_conv1d import causal_conv1d_fn, causal_conv1d_update
except ImportError:
    causal_conv1d_fn, causal_conv1d_update = None

try:
    from mamba_ssm.ops.triton.selective_state_update import selective_state_update
except ImportError:
    selective_state_update = None

try:
    from mamba_ssm.ops.triton.layernorm import RMSNorm, layer_norm_fn, rms_norm_fn
except ImportError:
    RMSNorm, layer_norm_fn, rms_norm_fn = None, None, None


class TransposeTokenReEmbedding:
    @staticmethod
    def transpose_normal_padding(x, rate):
        x = rearrange(x, "b c l -> b l c")
        B, N, C = x.shape
        value = N // rate
        if N % rate != 0:
            padding_length = (value + 1) * rate - N
            padded_x = torch.nn.functional.pad(x, (0, 0, 0, padding_length))
        else:
            padded_x = x
        x_ = rearrange(padded_x, "b (k w) d -> b (w k) d", w=rate)
        x_ = rearrange(x_, "b l c -> b c l")
        return x_

    @staticmethod
    def transpose_remove_padding(x, rate, length):
        x = rearrange(x, "b c l -> b l c")
        x = rearrange(x, "b (w k) d -> b (k w) d", w=rate)
        x = x[:, :length, :]
        x = rearrange(x, "b l c -> b c l")
        return x


class SRMamba(nn.Module):
    def __init__(
        self,
        d_model,
        d_state=16,
        d_conv=4,
        expand=2,
        dt_rank="auto",
        dt_min=0.001,
        dt_max=0.1,
        dt_init="random",
        dt_scale=1.0,
        dt_init_floor=1e-4,
        conv_bias=True,
        bias=False,
        use_fast_path=True,  # Fused kernel options
        layer_idx=None,
        device=None,
        dtype=None,
    ):
        factory_kwargs = {"device": device, "dtype": dtype}
        super().__init__()
        self.d_model = d_model
        self.d_state = d_state
        self.d_conv = d_conv
        self.expand = expand
        self.d_inner = int(self.expand * self.d_model)
        self.dt_rank = math.ceil(self.d_model / 16) if dt_rank == "auto" else dt_rank
        self.use_fast_path = use_fast_path
        self.layer_idx = layer_idx

        self.in_proj = nn.Linear(
            self.d_model, self.d_inner * 2, bias=bias, **factory_kwargs
        )

        self.conv1d = nn.Conv1d(
            in_channels=self.d_inner,
            out_channels=self.d_inner,
            bias=conv_bias,
            kernel_size=d_conv,
            groups=self.d_inner,
            padding=d_conv - 1,
            **factory_kwargs,
        )
        self.conv1d_b = nn.Conv1d(
            in_channels=self.d_inner,
            out_channels=self.d_inner,
            bias=conv_bias,
            kernel_size=d_conv,
            groups=self.d_inner,
            padding=d_conv - 1,
            **factory_kwargs,
        )

        self.activation = "silu"
        self.act = nn.SiLU()

        self.x_proj = nn.Linear(
            self.d_inner, self.dt_rank + self.d_state * 2, bias=False, **factory_kwargs
        )
        self.x_proj_b = nn.Linear(
            self.d_inner, self.dt_rank + self.d_state * 2, bias=False, **factory_kwargs
        )

        self.dt_proj = nn.Linear(
            self.dt_rank, self.d_inner, bias=True, **factory_kwargs
        )
        self.dt_proj_b = nn.Linear(
            self.dt_rank, self.d_inner, bias=True, **factory_kwargs
        )

        # Initialize special dt projection to preserve variance at initialization
        dt_init_std = self.dt_rank**-0.5 * dt_scale
        if dt_init == "constant":
            nn.init.constant_(self.dt_proj.weight, dt_init_std)
            nn.init.constant_(self.dt_proj_b.weight, dt_init_std)  # ?
        elif dt_init == "random":
            nn.init.uniform_(self.dt_proj.weight, -dt_init_std, dt_init_std)
            nn.init.uniform_(self.dt_proj_b.weight, -dt_init_std, dt_init_std)  # ?
        else:
            raise NotImplementedError

        # Initialize dt bias so that F.softplus(dt_bias) is between dt_min and dt_max
        dt = torch.exp(
            torch.rand(self.d_inner, **factory_kwargs)
            * (math.log(dt_max) - math.log(dt_min))
            + math.log(dt_min)
        ).clamp(min=dt_init_floor)
        # Inverse of softplus: https://github.com/pytorch/pytorch/issues/72759
        inv_dt = dt + torch.log(-torch.expm1(-dt))
        with torch.no_grad():
            self.dt_proj.bias.copy_(inv_dt)
            self.dt_proj_b.bias.copy_(inv_dt)  # ?
        # Our initialization would set all Linear.bias to zero, need to mark this one as _no_reinit
        self.dt_proj.bias._no_reinit = True
        self.dt_proj_b.bias._no_reinit = True  # ?

        # S4D real initialization
        A = repeat(
            torch.arange(1, self.d_state + 1, dtype=torch.float32, device=device),
            "n -> d n",
            d=self.d_inner,
        ).contiguous()
        A_log = torch.log(A)  # Keep A_log in fp32
        self.A_log = nn.Parameter(A_log)
        self.A_log._no_weight_decay = True

        A_b = repeat(
            torch.arange(1, self.d_state + 1, dtype=torch.float32, device=device),
            "n -> d n",
            d=self.d_inner,
        ).contiguous()
        A_b_log = torch.log(A_b)  # Keep A_b_log in fp32
        self.A_b_log = nn.Parameter(A_b_log)
        self.A_b_log._no_weight_decay = True

        # D "skip" parameter
        self.D = nn.Parameter(torch.ones(self.d_inner, device=device))  # Keep in fp32
        self.D._no_weight_decay = True
        self.D_b = nn.Parameter(torch.ones(self.d_inner, device=device))  # Keep in fp32
        self.D_b._no_weight_decay = True

        self.out_proj = nn.Linear(
            self.d_inner, self.d_model, bias=bias, **factory_kwargs
        )

    def forward(self, hidden_states, inference_params=None, rate=10):
        """
        hidden_states: (B, L, D)
        Returns: same shape as hidden_states
        """
        batch, seqlen, dim = hidden_states.shape

        conv_state, ssm_state = None, None
        if inference_params is not None:
            conv_state, ssm_state = self._get_states_from_cache(inference_params, batch)
            if inference_params.seqlen_offset > 0:
                # The states are updated inplace
                out, _, _ = self.step(hidden_states, conv_state, ssm_state)
                return out

        # We do matmul and transpose BLH -> HBL at the same time
        xz = rearrange(
            self.in_proj.weight @ rearrange(hidden_states, "b l d -> d (b l)"),
            "d (b l) -> b d l",
            l=seqlen,
        )
        if self.in_proj.bias is not None:
            xz = xz + rearrange(self.in_proj.bias.to(dtype=xz.dtype), "d -> d 1")

        A = -torch.exp(self.A_log.float())  # (d_inner, d_state)
        A_b = -torch.exp(self.A_b_log.float())
        # In the backward pass we write dx and dz next to each other to avoid torch.cat
        if (
            self.use_fast_path and inference_params is None
        ):  # Doesn't support outputting the states
            out = mamba_inner_fn_no_out_proj(
                xz,
                self.conv1d.weight,
                self.conv1d.bias,
                self.x_proj.weight,
                self.dt_proj.weight,
                A,
                None,  # input-dependent B
                None,  # input-dependent C
                self.D.float(),
                delta_bias=self.dt_proj.bias.float(),
                delta_softplus=True,
            )
            # 直接末尾补零并转置: TMamba
            N, C, L = xz.shape
            xz_b = TransposeTokenReEmbedding.transpose_normal_padding(xz, rate=rate)
            out_b = mamba_inner_fn_no_out_proj(
                xz_b,
                self.conv1d_b.weight,
                self.conv1d_b.bias,
                self.x_proj_b.weight,
                self.dt_proj_b.weight,
                A_b,
                None,
                None,
                self.D_b.float(),
                delta_bias=self.dt_proj_b.bias.float(),
                delta_softplus=True,
            )
            # 将序列转置，并删除padding的位置
            out_b = TransposeTokenReEmbedding.transpose_remove_padding(
                out_b, rate=rate, length=L
            )
            out = F.linear(
                rearrange(out + out_b, "b d l -> b l d"),
                self.out_proj.weight,
                self.out_proj.bias,
            )
        else:
            # x, z拆分用于两个分支的聚合
            x, z = xz.chunk(2, dim=1)
            x_b = x.flip([-1])
            # Compute short convolution
            if conv_state is not None:
                # If we just take x[:, :, -self.d_conv :], it will error if seqlen < self.d_conv
                # Instead F.pad will pad with zeros if seqlen < self.d_conv, and truncate otherwise.
                conv_state.copy_(
                    F.pad(x, (self.d_conv - x.shape[-1], 0))
                )  # Update state (B D W)
            # 为无causal_conv1d_fn提供了手动的处理方案
            if causal_conv1d_fn is None:
                x = self.act(self.conv1d(x)[..., :seqlen])
                x = self.act(self.conv1d_b(x_b)[..., :seqlen])
            else:
                assert self.activation in ["silu", "swish"]
                x = causal_conv1d_fn(
                    x=x,
                    weight=rearrange(self.conv1d.weight, "d 1 w -> d w"),
                    bias=self.conv1d.bias,
                    activation=self.activation,
                )
                x_b = causal_conv1d_fn(
                    x=x_b,
                    weight=rearrange(self.conv1d_b.weight, "d 1 w -> d w"),
                    bias=self.conv1d_b.bias,
                    activation=self.activation,
                )

            # We're careful here about the layout, to avoid extra transposes.
            # We want dt to have d as the slowest moving dimension
            # and L as the fastest moving dimension, since those are what the ssm_scan kernel expects.
            # SSM内核操作，分别为前向和反向生成对应的B和C
            x_dbl = self.x_proj(rearrange(x, "b d l -> (b l) d"))  # (bl d)
            dt, B, C = torch.split(
                x_dbl, [self.dt_rank, self.d_state, self.d_state], dim=-1
            )
            dt = self.dt_proj.weight @ dt.t()
            dt = rearrange(dt, "d (b l) -> b d l", l=seqlen)
            B = rearrange(B, "(b l) dstate -> b dstate l", l=seqlen).contiguous()
            C = rearrange(C, "(b l) dstate -> b dstate l", l=seqlen).contiguous()

            x_dbl_b = self.x_proj_b(rearrange(x_b, "b d l -> (b l) d"))  # (bl d)
            dt_b, B_b, C_b = torch.split(
                x_dbl_b, [self.dt_rank, self.d_state, self.d_state], dim=-1
            )
            dt_b = self.dt_proj_b.weight @ dt_b.t()
            dt_b = rearrange(dt_b, "d (b l) -> b d l", l=seqlen)
            B_b = rearrange(B_b, "(b l) dstate -> b dstate l", l=seqlen).contiguous()
            C_b = rearrange(C_b, "(b l) dstate -> b dstate l", l=seqlen).contiguous()
            assert self.activation in ["silu", "swish"]
            y = selective_scan_fn(
                x,
                dt,
                A,
                B,
                C,
                self.D.float(),
                z=z,
                delta_bias=self.dt_proj.bias.float(),
                delta_softplus=True,
                return_last_state=ssm_state is not None,
            )
            y_b = selective_scan_fn(
                x_b,
                dt_b,
                A_b,
                B_b,
                C_b,
                self.D_b.float(),
                z=z,
                delta_bias=self.dt_proj_b.bias.float(),
                delta_softplus=True,
                return_last_state=ssm_state is not None,
            )
            if ssm_state is not None:
                y, last_state = y
                ssm_state.copy_(last_state)
            y = rearrange(y, "b d l -> b l d")
            y_b = rearrange(y_b, "b d l -> b l d")
            out = self.out_proj(y_b + y)
        return out

    def step(self, hidden_states, conv_state, ssm_state):
        dtype = hidden_states.dtype
        assert (
            hidden_states.shape[1] == 1
        ), "Only support decoding with 1 token at a time for now"
        xz = self.in_proj(hidden_states.squeeze(1))  # (B 2D)
        x, z = xz.chunk(2, dim=-1)  # (B D)

        # Conv step
        if causal_conv1d_update is None:
            conv_state.copy_(
                torch.roll(conv_state, shifts=-1, dims=-1)
            )  # Update state (B D W)
            conv_state[:, :, -1] = x
            x = torch.sum(
                conv_state * rearrange(self.conv1d.weight, "d 1 w -> d w"), dim=-1
            )  # (B D)
            if self.conv1d.bias is not None:
                x = x + self.conv1d.bias
            x = self.act(x).to(dtype=dtype)
        else:
            x = causal_conv1d_update(
                x,
                conv_state,
                rearrange(self.conv1d.weight, "d 1 w -> d w"),
                self.conv1d.bias,
                self.activation,
            )

        x_db = self.x_proj(x)  # (B dt_rank+2*d_state)
        dt, B, C = torch.split(x_db, [self.dt_rank, self.d_state, self.d_state], dim=-1)
        # Don't add dt_bias here
        dt = F.linear(dt, self.dt_proj.weight)  # (B d_inner)
        A = -torch.exp(self.A_log.float())  # (d_inner, d_state)

        # SSM step
        if selective_state_update is None:
            # Discretize A and B
            dt = F.softplus(dt + self.dt_proj.bias.to(dtype=dt.dtype))
            dA = torch.exp(torch.einsum("bd,dn->bdn", dt, A))
            dB = torch.einsum("bd,bn->bdn", dt, B)
            ssm_state.copy_(ssm_state * dA + rearrange(x, "b d -> b d 1") * dB)
            y = torch.einsum("bdn,bn->bd", ssm_state.to(dtype), C)
            y = y + self.D.to(dtype) * x
            y = y * self.act(z)  # (B D)
        else:
            y = selective_state_update(
                ssm_state,
                x,
                dt,
                A,
                B,
                C,
                self.D,
                z=z,
                dt_bias=self.dt_proj.bias,
                dt_softplus=True,
            )

        out = self.out_proj(y)
        return out.unsqueeze(1), conv_state, ssm_state

    def allocate_inference_cache(self, batch_size, max_seqlen, dtype=None, **kwargs):
        device = self.out_proj.weight.device
        conv_dtype = self.conv1d.weight.dtype if dtype is None else dtype
        conv_state = torch.zeros(
            batch_size,
            self.d_model * self.expand,
            self.d_conv,
            device=device,
            dtype=conv_dtype,
        )
        ssm_dtype = self.dt_proj.weight.dtype if dtype is None else dtype
        # ssm_dtype = torch.float32
        ssm_state = torch.zeros(
            batch_size,
            self.d_model * self.expand,
            self.d_state,
            device=device,
            dtype=ssm_dtype,
        )
        return conv_state, ssm_state

    def _get_states_from_cache(
        self, inference_params, batch_size, initialize_states=False
    ):
        assert self.layer_idx is not None
        if self.layer_idx not in inference_params.key_value_memory_dict:
            batch_shape = (batch_size,)
            conv_state = torch.zeros(
                batch_size,
                self.d_model * self.expand,
                self.d_conv,
                device=self.conv1d.weight.device,
                dtype=self.conv1d.weight.dtype,
            )
            ssm_state = torch.zeros(
                batch_size,
                self.d_model * self.expand,
                self.d_state,
                device=self.dt_proj.weight.device,
                dtype=self.dt_proj.weight.dtype,
                # dtype=torch.float32,
            )
            inference_params.key_value_memory_dict[self.layer_idx] = (
                conv_state,
                ssm_state,
            )
        else:
            conv_state, ssm_state = inference_params.key_value_memory_dict[
                self.layer_idx
            ]
            # TODO: What if batch size changes between generation, and we reuse the same states?
            if initialize_states:
                conv_state.zero_()
                ssm_state.zero_()
        return conv_state, ssm_state


class Block(nn.Module):
    def __init__(
        self,
        dim,
        mixer_cls,
        norm_cls=nn.LayerNorm,
        fused_add_norm=False,
        residual_in_fp32=False,
    ):
        """
        Simple block wrapping a mixer class with LayerNorm/RMSNorm and residual connection"

        This Block has a slightly different structure compared to a regular
        prenorm Transformer block.
        The standard block is: LN -> MHA/MLP -> Add.
        [Ref: https://arxiv.org/abs/2002.04745]
        Here we have: Add -> LN -> Mixer, returning both
        the hidden_states (output of the mixer) and the residual.
        This is purely for performance reasons, as we can fuse add and LayerNorm.
        The residual needs to be provided (except for the very first block).
        """
        super().__init__()
        self.residual_in_fp32 = residual_in_fp32
        self.fused_add_norm = fused_add_norm
        self.mixer = mixer_cls(dim)
        self.norm = norm_cls(dim)
        if self.fused_add_norm:
            assert RMSNorm is not None, "RMSNorm import fails"
            assert isinstance(
                self.norm, (nn.LayerNorm, RMSNorm)
            ), "Only LayerNorm and RMSNorm are supported for fused_add_norm"

    def forward(
        self,
        hidden_states: Tensor,
        residual: Optional[Tensor] = None,
        inference_params=None,
    ):
        r"""Pass the input through the encoder layer.

        Args:
            hidden_states: the sequence to the encoder layer (required).
            residual: hidden_states = Mixer(LN(residual))
        """
        if not self.fused_add_norm:
            residual = (
                (hidden_states + residual) if residual is not None else hidden_states
            )
            hidden_states = self.norm(residual.to(dtype=self.norm.weight.dtype))
            if self.residual_in_fp32:
                residual = residual.to(torch.float32)
        else:
            fused_add_norm_fn = (
                rms_norm_fn if isinstance(self.norm, RMSNorm) else layer_norm_fn
            )
            hidden_states, residual = fused_add_norm_fn(
                hidden_states,
                self.norm.weight,
                self.norm.bias,
                residual=residual,
                prenorm=True,
                residual_in_fp32=self.residual_in_fp32,
                eps=self.norm.eps,
            )
        hidden_states = self.mixer(hidden_states, inference_params=inference_params)
        return hidden_states, residual

    def allocate_inference_cache(self, batch_size, max_seqlen, dtype=None, **kwargs):
        return self.mixer.allocate_inference_cache(
            batch_size, max_seqlen, dtype=dtype, **kwargs
        )



================================================
FILE: sauron/mil_models/mamba_ssm/ops/__init__.py
================================================



================================================
FILE: sauron/mil_models/mamba_ssm/ops/selective_scan_interface.py
================================================
# Copyright (c) 2023, Tri Dao, Albert Gu.

import causal_conv1d_cuda
import selective_scan_cuda
import torch
import torch.nn.functional as F
from causal_conv1d import causal_conv1d_fn
from einops import rearrange, repeat
from torch.cuda.amp import custom_bwd, custom_fwd


class SelectiveScanFn(torch.autograd.Function):
    @staticmethod
    def forward(
        ctx,
        u,
        delta,
        A,
        B,
        C,
        D=None,
        z=None,
        delta_bias=None,
        delta_softplus=False,
        return_last_state=False,
    ):
        if u.stride(-1) != 1:
            u = u.contiguous()
        if delta.stride(-1) != 1:
            delta = delta.contiguous()
        if D is not None:
            D = D.contiguous()
        if B.stride(-1) != 1:
            B = B.contiguous()
        if C.stride(-1) != 1:
            C = C.contiguous()
        if z is not None and z.stride(-1) != 1:
            z = z.contiguous()
        if B.dim() == 3:
            B = rearrange(B, "b dstate l -> b 1 dstate l")
            ctx.squeeze_B = True
        if C.dim() == 3:
            C = rearrange(C, "b dstate l -> b 1 dstate l")
            ctx.squeeze_C = True
        out, x, *rest = selective_scan_cuda.fwd(
            u, delta, A, B, C, D, z, delta_bias, delta_softplus
        )
        ctx.delta_softplus = delta_softplus
        ctx.has_z = z is not None
        last_state = x[:, :, -1, 1::2]  # (batch, dim, dstate)
        if not ctx.has_z:
            ctx.save_for_backward(u, delta, A, B, C, D, delta_bias, x)
            return out if not return_last_state else (out, last_state)
        else:
            ctx.save_for_backward(u, delta, A, B, C, D, z, delta_bias, x, out)
            out_z = rest[0]
            return out_z if not return_last_state else (out_z, last_state)

    @staticmethod
    def backward(ctx, dout, *args):
        if not ctx.has_z:
            u, delta, A, B, C, D, delta_bias, x = ctx.saved_tensors
            z = None
            out = None
        else:
            u, delta, A, B, C, D, z, delta_bias, x, out = ctx.saved_tensors
        if dout.stride(-1) != 1:
            dout = dout.contiguous()
        # The kernel supports passing in a pre-allocated dz (e.g., in case we want to fuse the
        # backward of selective_scan_cuda with the backward of chunk).
        # Here we just pass in None and dz will be allocated in the C++ code.
        du, ddelta, dA, dB, dC, dD, ddelta_bias, *rest = selective_scan_cuda.bwd(
            u,
            delta,
            A,
            B,
            C,
            D,
            z,
            delta_bias,
            dout,
            x,
            out,
            None,
            ctx.delta_softplus,
            False,  # option to recompute out_z, not used here
        )
        dz = rest[0] if ctx.has_z else None
        dB = dB.squeeze(1) if getattr(ctx, "squeeze_B", False) else dB
        dC = dC.squeeze(1) if getattr(ctx, "squeeze_C", False) else dC
        return (
            du,
            ddelta,
            dA,
            dB,
            dC,
            dD if D is not None else None,
            dz,
            ddelta_bias if delta_bias is not None else None,
            None,
            None,
        )


def selective_scan_fn(
    u,
    delta,
    A,
    B,
    C,
    D=None,
    z=None,
    delta_bias=None,
    delta_softplus=False,
    return_last_state=False,
):
    """if return_last_state is True, returns (out, last_state)
    last_state has shape (batch, dim, dstate). Note that the gradient of the last state is
    not considered in the backward pass.
    """
    return SelectiveScanFn.apply(
        u, delta, A, B, C, D, z, delta_bias, delta_softplus, return_last_state
    )


def selective_scan_ref(
    u,
    delta,
    A,
    B,
    C,
    D=None,
    z=None,
    delta_bias=None,
    delta_softplus=False,
    return_last_state=False,
):
    """
    u: r(B D L)
    delta: r(B D L)
    A: c(D N) or r(D N)
    B: c(D N) or r(B N L) or r(B N 2L) or r(B G N L) or (B G N L)
    C: c(D N) or r(B N L) or r(B N 2L) or r(B G N L) or (B G N L)
    D: r(D)
    z: r(B D L)
    delta_bias: r(D), fp32

    out: r(B D L)
    last_state (optional): r(B D dstate) or c(B D dstate)
    """
    dtype_in = u.dtype
    u = u.float()
    delta = delta.float()
    if delta_bias is not None:
        delta = delta + delta_bias[..., None].float()
    if delta_softplus:
        delta = F.softplus(delta)
    batch, dim, dstate = u.shape[0], A.shape[0], A.shape[1]
    is_variable_B = B.dim() >= 3
    is_variable_C = C.dim() >= 3
    if A.is_complex():
        if is_variable_B:
            B = torch.view_as_complex(
                rearrange(B.float(), "... (L two) -> ... L two", two=2)
            )
        if is_variable_C:
            C = torch.view_as_complex(
                rearrange(C.float(), "... (L two) -> ... L two", two=2)
            )
    else:
        B = B.float()
        C = C.float()
    x = A.new_zeros((batch, dim, dstate))
    ys = []
    deltaA = torch.exp(torch.einsum("bdl,dn->bdln", delta, A))
    if not is_variable_B:
        deltaB_u = torch.einsum("bdl,dn,bdl->bdln", delta, B, u)
    else:
        if B.dim() == 3:
            deltaB_u = torch.einsum("bdl,bnl,bdl->bdln", delta, B, u)
        else:
            B = repeat(B, "B G N L -> B (G H) N L", H=dim // B.shape[1])
            deltaB_u = torch.einsum("bdl,bdnl,bdl->bdln", delta, B, u)
    if is_variable_C and C.dim() == 4:
        C = repeat(C, "B G N L -> B (G H) N L", H=dim // C.shape[1])
    last_state = None
    for i in range(u.shape[2]):
        x = deltaA[:, :, i] * x + deltaB_u[:, :, i]
        if not is_variable_C:
            y = torch.einsum("bdn,dn->bd", x, C)
        else:
            if C.dim() == 3:
                y = torch.einsum("bdn,bn->bd", x, C[:, :, i])
            else:
                y = torch.einsum("bdn,bdn->bd", x, C[:, :, :, i])
        if i == u.shape[2] - 1:
            last_state = x
        if y.is_complex():
            y = y.real * 2
        ys.append(y)
    y = torch.stack(ys, dim=2)  # (batch dim L)
    out = y if D is None else y + u * rearrange(D, "d -> d 1")
    if z is not None:
        out = out * F.silu(z)
    out = out.to(dtype=dtype_in)
    return out if not return_last_state else (out, last_state)


class MambaInnerFn(torch.autograd.Function):
    @staticmethod
    @custom_fwd
    def forward(
        ctx,
        xz,
        conv1d_weight,
        conv1d_bias,
        x_proj_weight,
        delta_proj_weight,
        out_proj_weight,
        out_proj_bias,
        A,
        B=None,
        C=None,
        D=None,
        delta_bias=None,
        B_proj_bias=None,
        C_proj_bias=None,
        delta_softplus=True,
        checkpoint_lvl=1,
    ):
        """
        xz: (batch, dim, seqlen)
        """
        assert checkpoint_lvl in [0, 1]
        L = xz.shape[-1]
        delta_rank = delta_proj_weight.shape[1]
        d_state = A.shape[-1] * (1 if not A.is_complex() else 2)
        if torch.is_autocast_enabled():
            x_proj_weight = x_proj_weight.to(dtype=torch.get_autocast_gpu_dtype())
            delta_proj_weight = delta_proj_weight.to(
                dtype=torch.get_autocast_gpu_dtype()
            )
            out_proj_weight = out_proj_weight.to(dtype=torch.get_autocast_gpu_dtype())
            out_proj_bias = (
                out_proj_bias.to(dtype=torch.get_autocast_gpu_dtype())
                if out_proj_bias is not None
                else None
            )
        if xz.stride(-1) != 1:
            xz = xz.contiguous()
        conv1d_weight = rearrange(conv1d_weight, "d 1 w -> d w")
        x, z = xz.chunk(2, dim=1)
        conv1d_bias = conv1d_bias.contiguous() if conv1d_bias is not None else None
        conv1d_out = causal_conv1d_cuda.causal_conv1d_fwd(
            x, conv1d_weight, conv1d_bias, None, True
        )  # ？
        # We're being very careful here about the layout, to avoid extra transposes.
        # We want delta to have d as the slowest moving dimension
        # and L as the fastest moving dimension, since those are what the ssm_scan kernel expects.
        x_dbl = F.linear(
            rearrange(conv1d_out, "b d l -> (b l) d"), x_proj_weight
        )  # (bl d)
        delta = rearrange(
            delta_proj_weight @ x_dbl[:, :delta_rank].t(), "d (b l) -> b d l", l=L
        )
        ctx.is_variable_B = B is None
        ctx.is_variable_C = C is None
        ctx.B_proj_bias_is_None = B_proj_bias is None
        ctx.C_proj_bias_is_None = C_proj_bias is None
        if B is None:  # variable B
            B = x_dbl[:, delta_rank : delta_rank + d_state]  # (bl dstate)
            if B_proj_bias is not None:
                B = B + B_proj_bias.to(dtype=B.dtype)
            if not A.is_complex():
                # B = rearrange(B, "(b l) dstate -> b dstate l", l=L).contiguous()
                B = rearrange(B, "(b l) dstate -> b 1 dstate l", l=L).contiguous()
            else:
                B = rearrange(
                    B, "(b l) (dstate two) -> b 1 dstate (l two)", l=L, two=2
                ).contiguous()
        else:
            if B.stride(-1) != 1:
                B = B.contiguous()
        if C is None:  # variable C
            C = x_dbl[:, -d_state:]  # (bl dstate)
            if C_proj_bias is not None:
                C = C + C_proj_bias.to(dtype=C.dtype)
            if not A.is_complex():
                # C = rearrange(C, "(b l) dstate -> b dstate l", l=L).contiguous()
                C = rearrange(C, "(b l) dstate -> b 1 dstate l", l=L).contiguous()
            else:
                C = rearrange(
                    C, "(b l) (dstate two) -> b 1 dstate (l two)", l=L, two=2
                ).contiguous()
        else:
            if C.stride(-1) != 1:
                C = C.contiguous()
        if D is not None:
            D = D.contiguous()
        out, scan_intermediates, out_z = selective_scan_cuda.fwd(
            conv1d_out, delta, A, B, C, D, z, delta_bias, delta_softplus
        )
        ctx.delta_softplus = delta_softplus
        ctx.out_proj_bias_is_None = out_proj_bias is None
        ctx.checkpoint_lvl = checkpoint_lvl
        if (
            checkpoint_lvl >= 1
        ):  # Will recompute conv1d_out and delta in the backward pass
            conv1d_out, delta = None, None
        ctx.save_for_backward(
            xz,
            conv1d_weight,
            conv1d_bias,
            x_dbl,
            x_proj_weight,
            delta_proj_weight,
            out_proj_weight,
            conv1d_out,
            delta,
            A,
            B,
            C,
            D,
            delta_bias,
            scan_intermediates,
            out,
        )
        return F.linear(
            rearrange(out_z, "b d l -> b l d"), out_proj_weight, out_proj_bias
        )

    @staticmethod
    @custom_bwd
    def backward(ctx, dout):
        # dout: (batch, seqlen, dim)
        (
            xz,
            conv1d_weight,
            conv1d_bias,
            x_dbl,
            x_proj_weight,
            delta_proj_weight,
            out_proj_weight,
            conv1d_out,
            delta,
            A,
            B,
            C,
            D,
            delta_bias,
            scan_intermediates,
            out,
        ) = ctx.saved_tensors
        L = xz.shape[-1]
        delta_rank = delta_proj_weight.shape[1]
        d_state = A.shape[-1] * (1 if not A.is_complex() else 2)
        x, z = xz.chunk(2, dim=1)
        if dout.stride(-1) != 1:
            dout = dout.contiguous()
        if ctx.checkpoint_lvl == 1:
            conv1d_out = causal_conv1d_cuda.causal_conv1d_fwd(
                x, conv1d_weight, conv1d_bias, None, True
            )
            delta = rearrange(
                delta_proj_weight @ x_dbl[:, :delta_rank].t(), "d (b l) -> b d l", l=L
            )
        # The kernel supports passing in a pre-allocated dz (e.g., in case we want to fuse the
        # backward of selective_scan_cuda with the backward of chunk).
        dxz = torch.empty_like(xz)  # (batch, dim, seqlen)
        dx, dz = dxz.chunk(2, dim=1)
        dout = rearrange(dout, "b l e -> e (b l)")
        dout_y = rearrange(out_proj_weight.t() @ dout, "d (b l) -> b d l", l=L)
        dconv1d_out, ddelta, dA, dB, dC, dD, ddelta_bias, dz, out_z = (
            selective_scan_cuda.bwd(
                conv1d_out,
                delta,
                A,
                B,
                C,
                D,
                z,
                delta_bias,
                dout_y,
                scan_intermediates,
                out,
                dz,
                ctx.delta_softplus,
                True,  # option to recompute out_z
            )
        )
        dout_proj_weight = torch.einsum(
            "eB,dB->ed", dout, rearrange(out_z, "b d l -> d (b l)")
        )
        dout_proj_bias = dout.sum(dim=(0, 1)) if not ctx.out_proj_bias_is_None else None
        dD = dD if D is not None else None
        dx_dbl = torch.empty_like(x_dbl)
        dB_proj_bias = None
        if ctx.is_variable_B:
            if not A.is_complex():
                dB = rearrange(dB, "b 1 dstate l -> (b l) dstate").contiguous()
            else:
                dB = rearrange(
                    dB, "b 1 dstate (l two) -> (b l) (dstate two)", two=2
                ).contiguous()
            dB_proj_bias = dB.sum(0) if not ctx.B_proj_bias_is_None else None
            dx_dbl[:, delta_rank : delta_rank + d_state] = dB  # (bl d)
            dB = None
        dC_proj_bias = None
        if ctx.is_variable_C:
            if not A.is_complex():
                dC = rearrange(dC, "b 1 dstate l -> (b l) dstate").contiguous()
            else:
                dC = rearrange(
                    dC, "b 1 dstate (l two) -> (b l) (dstate two)", two=2
                ).contiguous()
            dC_proj_bias = dC.sum(0) if not ctx.C_proj_bias_is_None else None
            dx_dbl[:, -d_state:] = dC  # (bl d)
            dC = None
        ddelta = rearrange(ddelta, "b d l -> d (b l)")
        ddelta_proj_weight = torch.einsum("dB,Br->dr", ddelta, x_dbl[:, :delta_rank])
        dx_dbl[:, :delta_rank] = torch.einsum("dB,dr->Br", ddelta, delta_proj_weight)
        dconv1d_out = rearrange(dconv1d_out, "b d l -> d (b l)")
        dx_proj_weight = torch.einsum(
            "Br,Bd->rd", dx_dbl, rearrange(conv1d_out, "b d l -> (b l) d")
        )
        dconv1d_out = torch.addmm(
            dconv1d_out, x_proj_weight.t(), dx_dbl.t(), out=dconv1d_out
        )
        dconv1d_out = rearrange(
            dconv1d_out, "d (b l) -> b d l", b=x.shape[0], l=x.shape[-1]
        )
        # The kernel supports passing in a pre-allocated dx (e.g., in case we want to fuse the
        # backward of conv1d with the backward of chunk).
        dx, dconv1d_weight, dconv1d_bias = causal_conv1d_cuda.causal_conv1d_bwd(
            x, conv1d_weight, conv1d_bias, dconv1d_out, None, dx, True
        )
        dconv1d_bias = dconv1d_bias if conv1d_bias is not None else None
        dconv1d_weight = rearrange(dconv1d_weight, "d w -> d 1 w")
        return (
            dxz,
            dconv1d_weight,
            dconv1d_bias,
            dx_proj_weight,
            ddelta_proj_weight,
            dout_proj_weight,
            dout_proj_bias,
            dA,
            dB,
            dC,
            dD,
            ddelta_bias if delta_bias is not None else None,
            dB_proj_bias,
            dC_proj_bias,
            None,
        )


class MambaInnerFnNoOutProj(torch.autograd.Function):
    @staticmethod
    @custom_fwd
    def forward(
        ctx,
        xz,
        conv1d_weight,
        conv1d_bias,
        x_proj_weight,
        delta_proj_weight,
        A,
        B=None,
        C=None,
        D=None,
        delta_bias=None,
        B_proj_bias=None,
        C_proj_bias=None,
        delta_softplus=True,
        checkpoint_lvl=1,
    ):
        """
        xz: (batch, dim, seqlen)
        """
        assert checkpoint_lvl in [0, 1]
        L = xz.shape[-1]
        delta_rank = delta_proj_weight.shape[1]
        d_state = A.shape[-1] * (1 if not A.is_complex() else 2)
        if torch.is_autocast_enabled():
            x_proj_weight = x_proj_weight.to(dtype=torch.get_autocast_gpu_dtype())
            delta_proj_weight = delta_proj_weight.to(
                dtype=torch.get_autocast_gpu_dtype()
            )
        if xz.stride(-1) != 1:
            xz = xz.contiguous()
        conv1d_weight = rearrange(conv1d_weight, "d 1 w -> d w")
        x, z = xz.chunk(2, dim=1)
        conv1d_bias = conv1d_bias.contiguous() if conv1d_bias is not None else None
        conv1d_out = causal_conv1d_cuda.causal_conv1d_fwd(
            x, conv1d_weight, conv1d_bias, None, True
        )
        # We're being very careful here about the layout, to avoid extra transposes.
        # We want delta to have d as the slowest moving dimension
        # and L as the fastest moving dimension, since those are what the ssm_scan kernel expects.
        x_dbl = F.linear(
            rearrange(conv1d_out, "b d l -> (b l) d"), x_proj_weight
        )  # (bl d)
        delta = rearrange(
            delta_proj_weight @ x_dbl[:, :delta_rank].t(), "d (b l) -> b d l", l=L
        )
        ctx.is_variable_B = B is None
        ctx.is_variable_C = C is None
        ctx.B_proj_bias_is_None = B_proj_bias is None
        ctx.C_proj_bias_is_None = C_proj_bias is None
        if B is None:  # variable B
            B = x_dbl[:, delta_rank : delta_rank + d_state]  # (bl dstate)
            if B_proj_bias is not None:
                B = B + B_proj_bias.to(dtype=B.dtype)
            if not A.is_complex():
                # B = rearrange(B, "(b l) dstate -> b dstate l", l=L).contiguous()
                B = rearrange(B, "(b l) dstate -> b 1 dstate l", l=L).contiguous()
            else:
                B = rearrange(
                    B, "(b l) (dstate two) -> b 1 dstate (l two)", l=L, two=2
                ).contiguous()
        else:
            if B.stride(-1) != 1:
                B = B.contiguous()
        if C is None:  # variable C
            C = x_dbl[:, -d_state:]  # (bl dstate)
            if C_proj_bias is not None:
                C = C + C_proj_bias.to(dtype=C.dtype)
            if not A.is_complex():
                # C = rearrange(C, "(b l) dstate -> b dstate l", l=L).contiguous()
                C = rearrange(C, "(b l) dstate -> b 1 dstate l", l=L).contiguous()
            else:
                C = rearrange(
                    C, "(b l) (dstate two) -> b 1 dstate (l two)", l=L, two=2
                ).contiguous()
        else:
            if C.stride(-1) != 1:
                C = C.contiguous()
        if D is not None:
            D = D.contiguous()
        out, scan_intermediates, out_z = selective_scan_cuda.fwd(
            conv1d_out, delta, A, B, C, D, z, delta_bias, delta_softplus
        )
        ctx.delta_softplus = delta_softplus
        ctx.checkpoint_lvl = checkpoint_lvl
        if (
            checkpoint_lvl >= 1
        ):  # Will recompute conv1d_out and delta in the backward pass
            conv1d_out, delta = None, None
        ctx.save_for_backward(
            xz,
            conv1d_weight,
            conv1d_bias,
            x_dbl,
            x_proj_weight,
            delta_proj_weight,
            conv1d_out,
            delta,
            A,
            B,
            C,
            D,
            delta_bias,
            scan_intermediates,
            out,
        )
        # return rearrange(out_z, "b d l -> b l d")
        return out_z

    @staticmethod
    @custom_bwd
    def backward(ctx, dout):
        # dout: (batch, seqlen, dim)
        (
            xz,
            conv1d_weight,
            conv1d_bias,
            x_dbl,
            x_proj_weight,
            delta_proj_weight,
            conv1d_out,
            delta,
            A,
            B,
            C,
            D,
            delta_bias,
            scan_intermediates,
            out,
        ) = ctx.saved_tensors
        L = xz.shape[-1]
        delta_rank = delta_proj_weight.shape[1]
        d_state = A.shape[-1] * (1 if not A.is_complex() else 2)
        x, z = xz.chunk(2, dim=1)
        if dout.stride(-1) != 1:
            dout = dout.contiguous()
        if ctx.checkpoint_lvl == 1:
            conv1d_out = causal_conv1d_cuda.causal_conv1d_fwd(
                x, conv1d_weight, conv1d_bias, None, True
            )
            delta = rearrange(
                delta_proj_weight @ x_dbl[:, :delta_rank].t(), "d (b l) -> b d l", l=L
            )
        # The kernel supports passing in a pre-allocated dz (e.g., in case we want to fuse the
        # backward of selective_scan_cuda with the backward of chunk).
        dxz = torch.empty_like(xz)  # (batch, dim, seqlen)
        dx, dz = dxz.chunk(2, dim=1)
        # dout_y = rearrange(dout, "b l d -> b d l") # because no arrange at end of forward, so dout shape is b d l
        dconv1d_out, ddelta, dA, dB, dC, dD, ddelta_bias, dz, out_z = (
            selective_scan_cuda.bwd(
                conv1d_out,
                delta,
                A,
                B,
                C,
                D,
                z,
                delta_bias,
                dout,
                scan_intermediates,
                out,
                dz,
                ctx.delta_softplus,
                True,  # option to recompute out_z
            )
        )
        dD = dD if D is not None else None
        dx_dbl = torch.empty_like(x_dbl)
        dB_proj_bias = None
        if ctx.is_variable_B:
            if not A.is_complex():
                dB = rearrange(dB, "b 1 dstate l -> (b l) dstate").contiguous()
            else:
                dB = rearrange(
                    dB, "b 1 dstate (l two) -> (b l) (dstate two)", two=2
                ).contiguous()
            dB_proj_bias = dB.sum(0) if not ctx.B_proj_bias_is_None else None
            dx_dbl[:, delta_rank : delta_rank + d_state] = dB  # (bl d)
            dB = None
        dC_proj_bias = None
        if ctx.is_variable_C:
            if not A.is_complex():
                dC = rearrange(dC, "b 1 dstate l -> (b l) dstate").contiguous()
            else:
                dC = rearrange(
                    dC, "b 1 dstate (l two) -> (b l) (dstate two)", two=2
                ).contiguous()
            dC_proj_bias = dC.sum(0) if not ctx.C_proj_bias_is_None else None
            dx_dbl[:, -d_state:] = dC  # (bl d)
            dC = None
        ddelta = rearrange(ddelta, "b d l -> d (b l)")
        ddelta_proj_weight = torch.einsum("dB,Br->dr", ddelta, x_dbl[:, :delta_rank])
        dx_dbl[:, :delta_rank] = torch.einsum("dB,dr->Br", ddelta, delta_proj_weight)
        dconv1d_out = rearrange(dconv1d_out, "b d l -> d (b l)")
        dx_proj_weight = torch.einsum(
            "Br,Bd->rd", dx_dbl, rearrange(conv1d_out, "b d l -> (b l) d")
        )
        dconv1d_out = torch.addmm(
            dconv1d_out, x_proj_weight.t(), dx_dbl.t(), out=dconv1d_out
        )
        dconv1d_out = rearrange(
            dconv1d_out, "d (b l) -> b d l", b=x.shape[0], l=x.shape[-1]
        )
        # The kernel supports passing in a pre-allocated dx (e.g., in case we want to fuse the
        # backward of conv1d with the backward of chunk).
        dx, dconv1d_weight, dconv1d_bias = causal_conv1d_cuda.causal_conv1d_bwd(
            x, conv1d_weight, conv1d_bias, dconv1d_out, None, dx, True
        )  # ?
        dconv1d_bias = dconv1d_bias if conv1d_bias is not None else None
        dconv1d_weight = rearrange(dconv1d_weight, "d w -> d 1 w")
        return (
            dxz,
            dconv1d_weight,
            dconv1d_bias,
            dx_proj_weight,
            ddelta_proj_weight,
            dA,
            dB,
            dC,
            dD,
            ddelta_bias if delta_bias is not None else None,
            dB_proj_bias,
            dC_proj_bias,
            None,
        )


def mamba_inner_fn(
    xz,
    conv1d_weight,
    conv1d_bias,
    x_proj_weight,
    delta_proj_weight,
    out_proj_weight,
    out_proj_bias,
    A,
    B=None,
    C=None,
    D=None,
    delta_bias=None,
    B_proj_bias=None,
    C_proj_bias=None,
    delta_softplus=True,
):
    return MambaInnerFn.apply(
        xz,
        conv1d_weight,
        conv1d_bias,
        x_proj_weight,
        delta_proj_weight,
        out_proj_weight,
        out_proj_bias,
        A,
        B,
        C,
        D,
        delta_bias,
        B_proj_bias,
        C_proj_bias,
        delta_softplus,
    )


def mamba_inner_fn_no_out_proj(
    xz,
    conv1d_weight,
    conv1d_bias,
    x_proj_weight,
    delta_proj_weight,
    A,
    B=None,
    C=None,
    D=None,
    delta_bias=None,
    B_proj_bias=None,
    C_proj_bias=None,
    delta_softplus=True,
):
    return MambaInnerFnNoOutProj.apply(
        xz,
        conv1d_weight,
        conv1d_bias,
        x_proj_weight,
        delta_proj_weight,
        A,
        B,
        C,
        D,
        delta_bias,
        B_proj_bias,
        C_proj_bias,
        delta_softplus,
    )


def mamba_inner_ref(
    xz,
    conv1d_weight,
    conv1d_bias,
    x_proj_weight,
    delta_proj_weight,
    out_proj_weight,
    out_proj_bias,
    A,
    B=None,
    C=None,
    D=None,
    delta_bias=None,
    B_proj_bias=None,
    C_proj_bias=None,
    delta_softplus=True,
):
    L = xz.shape[-1]
    delta_rank = delta_proj_weight.shape[1]
    d_state = A.shape[-1] * (1 if not A.is_complex() else 2)
    x, z = xz.chunk(2, dim=1)
    x = causal_conv1d_fn(
        x, rearrange(conv1d_weight, "d 1 w -> d w"), conv1d_bias, "silu"
    )
    # We're being very careful here about the layout, to avoid extra transposes.
    # We want delta to have d as the slowest moving dimension
    # and L as the fastest moving dimension, since those are what the ssm_scan kernel expects.
    x_dbl = F.linear(rearrange(x, "b d l -> (b l) d"), x_proj_weight)  # (bl d)
    delta = delta_proj_weight @ x_dbl[:, :delta_rank].t()
    delta = rearrange(delta, "d (b l) -> b d l", l=L)
    if B is None:  # variable B
        B = x_dbl[:, delta_rank : delta_rank + d_state]  # (bl d)
        if B_proj_bias is not None:
            B = B + B_proj_bias.to(dtype=B.dtype)
        if not A.is_complex():
            B = rearrange(B, "(b l) dstate -> b dstate l", l=L).contiguous()
        else:
            B = rearrange(
                B, "(b l) (dstate two) -> b dstate (l two)", l=L, two=2
            ).contiguous()
    if C is None:  # variable B
        C = x_dbl[:, -d_state:]  # (bl d)
        if C_proj_bias is not None:
            C = C + C_proj_bias.to(dtype=C.dtype)
        if not A.is_complex():
            C = rearrange(C, "(b l) dstate -> b dstate l", l=L).contiguous()
        else:
            C = rearrange(
                C, "(b l) (dstate two) -> b dstate (l two)", l=L, two=2
            ).contiguous()
    y = selective_scan_fn(
        x, delta, A, B, C, D, z=z, delta_bias=delta_bias, delta_softplus=True
    )
    return F.linear(rearrange(y, "b d l -> b l d"), out_proj_weight, out_proj_bias)



================================================
FILE: sauron/mil_models/mamba_ssm/ops/triton/__init__.py
================================================



================================================
FILE: sauron/mil_models/mamba_ssm/ops/triton/layernorm.py
================================================
# Copyright (c) 2023, Tri Dao.
# Implement residual + layer_norm / rms_norm.

# Based on the Triton LayerNorm tutorial: https://triton-lang.org/main/getting-started/tutorials/05-layer-norm.html
# For the backward pass, we keep weight_grad and bias_grad in registers and accumulate.
# This is faster for dimensions up to 8k, but after that it's much slower due to register spilling.
# The models we train have hidden dim up to 8k anyway (e.g. Llama 70B), so this is fine.

import math

import torch
import torch.nn.functional as F
import triton
import triton.language as tl
from torch.cuda.amp import custom_bwd, custom_fwd


def layer_norm_ref(
    x, weight, bias, residual=None, eps=1e-6, prenorm=False, upcast=False
):
    dtype = x.dtype
    if upcast:
        weight = weight.float()
        bias = bias.float() if bias is not None else None
    if upcast:
        x = x.float()
        residual = residual.float() if residual is not None else residual
    if residual is not None:
        x = (x + residual).to(x.dtype)
    out = F.layer_norm(
        x.to(weight.dtype), x.shape[-1:], weight=weight, bias=bias, eps=eps
    ).to(dtype)
    return out if not prenorm else (out, x)


def rms_norm_ref(x, weight, bias, residual=None, eps=1e-6, prenorm=False, upcast=False):
    dtype = x.dtype
    if upcast:
        weight = weight.float()
        bias = bias.float() if bias is not None else None
    if upcast:
        x = x.float()
        residual = residual.float() if residual is not None else residual
    if residual is not None:
        x = (x + residual).to(x.dtype)
    rstd = 1 / torch.sqrt((x.square()).mean(dim=-1, keepdim=True) + eps)
    out = (x * rstd * weight) + bias if bias is not None else (x * rstd * weight)
    out = out.to(dtype)
    return out if not prenorm else (out, x)


@triton.autotune(
    configs=[
        triton.Config({}, num_warps=1),
        triton.Config({}, num_warps=2),
        triton.Config({}, num_warps=4),
        triton.Config({}, num_warps=8),
        triton.Config({}, num_warps=16),
        triton.Config({}, num_warps=32),
    ],
    key=["N", "HAS_RESIDUAL", "STORE_RESIDUAL_OUT", "IS_RMS_NORM", "HAS_BIAS"],
)
# @triton.heuristics({"HAS_BIAS": lambda args: args["B"] is not None})
# @triton.heuristics({"HAS_RESIDUAL": lambda args: args["RESIDUAL"] is not None})
@triton.jit
def _layer_norm_fwd_1pass_kernel(
    X,  # pointer to the input
    Y,  # pointer to the output
    W,  # pointer to the weights
    B,  # pointer to the biases
    RESIDUAL,  # pointer to the residual
    RESIDUAL_OUT,  # pointer to the residual
    Mean,  # pointer to the mean
    Rstd,  # pointer to the 1/std
    stride_x_row,  # how much to increase the pointer when moving by 1 row
    stride_y_row,
    stride_res_row,
    stride_res_out_row,
    N,  # number of columns in X
    eps,  # epsilon to avoid division by zero
    IS_RMS_NORM: tl.constexpr,
    BLOCK_N: tl.constexpr,
    HAS_RESIDUAL: tl.constexpr,
    STORE_RESIDUAL_OUT: tl.constexpr,
    HAS_BIAS: tl.constexpr,
):
    # Map the program id to the row of X and Y it should compute.
    row = tl.program_id(0)
    X += row * stride_x_row
    Y += row * stride_y_row
    if HAS_RESIDUAL:
        RESIDUAL += row * stride_res_row
    if STORE_RESIDUAL_OUT:
        RESIDUAL_OUT += row * stride_res_out_row
    # Compute mean and variance
    cols = tl.arange(0, BLOCK_N)
    x = tl.load(X + cols, mask=cols < N, other=0.0).to(tl.float32)
    if HAS_RESIDUAL:
        residual = tl.load(RESIDUAL + cols, mask=cols < N, other=0.0).to(tl.float32)
        x += residual
    if STORE_RESIDUAL_OUT:
        tl.store(RESIDUAL_OUT + cols, x, mask=cols < N)
    if not IS_RMS_NORM:
        mean = tl.sum(x, axis=0) / N
        tl.store(Mean + row, mean)
        xbar = tl.where(cols < N, x - mean, 0.0)
        var = tl.sum(xbar * xbar, axis=0) / N
    else:
        xbar = tl.where(cols < N, x, 0.0)
        var = tl.sum(xbar * xbar, axis=0) / N
    rstd = 1 / tl.sqrt(var + eps)
    tl.store(Rstd + row, rstd)
    # Normalize and apply linear transformation
    mask = cols < N
    w = tl.load(W + cols, mask=mask).to(tl.float32)
    if HAS_BIAS:
        b = tl.load(B + cols, mask=mask).to(tl.float32)
    x_hat = (x - mean) * rstd if not IS_RMS_NORM else x * rstd
    y = x_hat * w + b if HAS_BIAS else x_hat * w
    # Write output
    tl.store(Y + cols, y, mask=mask)


def _layer_norm_fwd(
    x,
    weight,
    bias,
    eps,
    residual=None,
    out_dtype=None,
    residual_dtype=None,
    is_rms_norm=False,
):
    if residual is not None:
        residual_dtype = residual.dtype
    M, N = x.shape
    assert x.stride(-1) == 1
    if residual is not None:
        assert residual.stride(-1) == 1
        assert residual.shape == (M, N)
    assert weight.shape == (N,)
    assert weight.stride(-1) == 1
    if bias is not None:
        assert bias.stride(-1) == 1
        assert bias.shape == (N,)
    # allocate output
    y = torch.empty_like(x, dtype=x.dtype if out_dtype is None else out_dtype)
    assert y.stride(-1) == 1
    if residual is not None or (
        residual_dtype is not None and residual_dtype != x.dtype
    ):
        residual_out = torch.empty(M, N, device=x.device, dtype=residual_dtype)
        assert residual_out.stride(-1) == 1
    else:
        residual_out = None
    mean = (
        torch.empty((M,), dtype=torch.float32, device=x.device)
        if not is_rms_norm
        else None
    )
    rstd = torch.empty((M,), dtype=torch.float32, device=x.device)
    # Less than 64KB per feature: enqueue fused kernel
    MAX_FUSED_SIZE = 65536 // x.element_size()
    BLOCK_N = min(MAX_FUSED_SIZE, triton.next_power_of_2(N))
    if N > BLOCK_N:
        raise RuntimeError("This layer norm doesn't support feature dim >= 64KB.")
    # heuristics for number of warps
    with torch.cuda.device(x.device.index):
        _layer_norm_fwd_1pass_kernel[(M,)](
            x,
            y,
            weight,
            bias,
            residual,
            residual_out,
            mean,
            rstd,
            x.stride(0),
            y.stride(0),
            residual.stride(0) if residual is not None else 0,
            residual_out.stride(0) if residual_out is not None else 0,
            N,
            eps,
            is_rms_norm,
            BLOCK_N,
            residual is not None,
            residual_out is not None,
            bias is not None,
        )
    # residual_out is None if residual is None and residual_dtype == input_dtype
    return y, mean, rstd, residual_out if residual_out is not None else x


@triton.autotune(
    configs=[
        triton.Config({}, num_warps=1),
        triton.Config({}, num_warps=2),
        triton.Config({}, num_warps=4),
        triton.Config({}, num_warps=8),
        triton.Config({}, num_warps=16),
        triton.Config({}, num_warps=32),
    ],
    key=["N", "HAS_DRESIDUAL", "STORE_DRESIDUAL", "IS_RMS_NORM", "HAS_BIAS"],
)
# @triton.heuristics({"HAS_BIAS": lambda args: args["B"] is not None})
# @triton.heuristics({"HAS_DRESIDUAL": lambda args: args["DRESIDUAL"] is not None})
# @triton.heuristics({"STORE_DRESIDUAL": lambda args: args["DRESIDUAL_IN"] is not None})
@triton.heuristics({"RECOMPUTE_OUTPUT": lambda args: args["Y"] is not None})
@triton.jit
def _layer_norm_bwd_kernel(
    X,  # pointer to the input
    W,  # pointer to the weights
    B,  # pointer to the biases
    Y,  # pointer to the output to be recomputed
    DY,  # pointer to the output gradient
    DX,  # pointer to the input gradient
    DW,  # pointer to the partial sum of weights gradient
    DB,  # pointer to the partial sum of biases gradient
    DRESIDUAL,
    DRESIDUAL_IN,
    Mean,  # pointer to the mean
    Rstd,  # pointer to the 1/std
    stride_x_row,  # how much to increase the pointer when moving by 1 row
    stride_y_row,
    stride_dy_row,
    stride_dx_row,
    stride_dres_row,
    stride_dres_in_row,
    M,  # number of rows in X
    N,  # number of columns in X
    eps,  # epsilon to avoid division by zero
    rows_per_program,
    IS_RMS_NORM: tl.constexpr,
    BLOCK_N: tl.constexpr,
    HAS_DRESIDUAL: tl.constexpr,
    STORE_DRESIDUAL: tl.constexpr,
    HAS_BIAS: tl.constexpr,
    RECOMPUTE_OUTPUT: tl.constexpr,
):
    # Map the program id to the elements of X, DX, and DY it should compute.
    row_block_id = tl.program_id(0)
    row_start = row_block_id * rows_per_program
    cols = tl.arange(0, BLOCK_N)
    mask = cols < N
    X += row_start * stride_x_row
    if HAS_DRESIDUAL:
        DRESIDUAL += row_start * stride_dres_row
    if STORE_DRESIDUAL:
        DRESIDUAL_IN += row_start * stride_dres_in_row
    DY += row_start * stride_dy_row
    DX += row_start * stride_dx_row
    if RECOMPUTE_OUTPUT:
        Y += row_start * stride_y_row
    w = tl.load(W + cols, mask=mask).to(tl.float32)
    if RECOMPUTE_OUTPUT and HAS_BIAS:
        b = tl.load(B + cols, mask=mask, other=0.0).to(tl.float32)
    dw = tl.zeros((BLOCK_N,), dtype=tl.float32)
    if HAS_BIAS:
        db = tl.zeros((BLOCK_N,), dtype=tl.float32)
    row_end = min((row_block_id + 1) * rows_per_program, M)
    for row in range(row_start, row_end):
        # Load data to SRAM
        x = tl.load(X + cols, mask=mask, other=0).to(tl.float32)
        dy = tl.load(DY + cols, mask=mask, other=0).to(tl.float32)
        if not IS_RMS_NORM:
            mean = tl.load(Mean + row)
        rstd = tl.load(Rstd + row)
        # Compute dx
        xhat = (x - mean) * rstd if not IS_RMS_NORM else x * rstd
        xhat = tl.where(mask, xhat, 0.0)
        if RECOMPUTE_OUTPUT:
            y = xhat * w + b if HAS_BIAS else xhat * w
            tl.store(Y + cols, y, mask=mask)
        wdy = w * dy
        dw += dy * xhat
        if HAS_BIAS:
            db += dy
        if not IS_RMS_NORM:
            c1 = tl.sum(xhat * wdy, axis=0) / N
            c2 = tl.sum(wdy, axis=0) / N
            dx = (wdy - (xhat * c1 + c2)) * rstd
        else:
            c1 = tl.sum(xhat * wdy, axis=0) / N
            dx = (wdy - xhat * c1) * rstd
        if HAS_DRESIDUAL:
            dres = tl.load(DRESIDUAL + cols, mask=mask, other=0).to(tl.float32)
            dx += dres
        # Write dx
        if STORE_DRESIDUAL:
            tl.store(DRESIDUAL_IN + cols, dx, mask=mask)
        tl.store(DX + cols, dx, mask=mask)

        X += stride_x_row
        if HAS_DRESIDUAL:
            DRESIDUAL += stride_dres_row
        if STORE_DRESIDUAL:
            DRESIDUAL_IN += stride_dres_in_row
        if RECOMPUTE_OUTPUT:
            Y += stride_y_row
        DY += stride_dy_row
        DX += stride_dx_row
    tl.store(DW + row_block_id * N + cols, dw, mask=mask)
    if HAS_BIAS:
        tl.store(DB + row_block_id * N + cols, db, mask=mask)


def _layer_norm_bwd(
    dy,
    x,
    weight,
    bias,
    eps,
    mean,
    rstd,
    dresidual=None,
    has_residual=False,
    is_rms_norm=False,
    x_dtype=None,
    recompute_output=False,
):
    M, N = x.shape
    assert x.stride(-1) == 1
    assert dy.stride(-1) == 1
    assert dy.shape == (M, N)
    if dresidual is not None:
        assert dresidual.stride(-1) == 1
        assert dresidual.shape == (M, N)
    assert weight.shape == (N,)
    assert weight.stride(-1) == 1
    if bias is not None:
        assert bias.stride(-1) == 1
        assert bias.shape == (N,)
    # allocate output
    dx = (
        torch.empty_like(x)
        if x_dtype is None
        else torch.empty(M, N, dtype=x_dtype, device=x.device)
    )
    dresidual_in = torch.empty_like(x) if has_residual and dx.dtype != x.dtype else None
    y = (
        torch.empty(M, N, dtype=dy.dtype, device=dy.device)
        if recompute_output
        else None
    )

    # Less than 64KB per feature: enqueue fused kernel
    MAX_FUSED_SIZE = 65536 // x.element_size()
    BLOCK_N = min(MAX_FUSED_SIZE, triton.next_power_of_2(N))
    if N > BLOCK_N:
        raise RuntimeError("This layer norm doesn't support feature dim >= 64KB.")
    sm_count = torch.cuda.get_device_properties(x.device).multi_processor_count
    _dw = torch.empty((sm_count, N), dtype=torch.float32, device=weight.device)
    _db = (
        torch.empty((sm_count, N), dtype=torch.float32, device=bias.device)
        if bias is not None
        else None
    )
    rows_per_program = math.ceil(M / sm_count)
    grid = (sm_count,)
    with torch.cuda.device(x.device.index):
        _layer_norm_bwd_kernel[grid](
            x,
            weight,
            bias,
            y,
            dy,
            dx,
            _dw,
            _db,
            dresidual,
            dresidual_in,
            mean,
            rstd,
            x.stride(0),
            0 if not recompute_output else y.stride(0),
            dy.stride(0),
            dx.stride(0),
            dresidual.stride(0) if dresidual is not None else 0,
            dresidual_in.stride(0) if dresidual_in is not None else 0,
            M,
            N,
            eps,
            rows_per_program,
            is_rms_norm,
            BLOCK_N,
            dresidual is not None,
            dresidual_in is not None,
            bias is not None,
        )
    dw = _dw.sum(0).to(weight.dtype)
    db = _db.sum(0).to(bias.dtype) if bias is not None else None
    # Don't need to compute dresidual_in separately in this case
    if has_residual and dx.dtype == x.dtype:
        dresidual_in = dx
    return (
        (dx, dw, db, dresidual_in)
        if not recompute_output
        else (dx, dw, db, dresidual_in, y)
    )


class LayerNormFn(torch.autograd.Function):
    @staticmethod
    def forward(
        ctx,
        x,
        weight,
        bias,
        residual=None,
        eps=1e-6,
        prenorm=False,
        residual_in_fp32=False,
        is_rms_norm=False,
    ):
        x_shape_og = x.shape
        # reshape input data into 2D tensor
        x = x.reshape(-1, x.shape[-1])
        if x.stride(-1) != 1:
            x = x.contiguous()
        if residual is not None:
            assert residual.shape == x_shape_og
            residual = residual.reshape(-1, residual.shape[-1])
            if residual.stride(-1) != 1:
                residual = residual.contiguous()
        weight = weight.contiguous()
        if bias is not None:
            bias = bias.contiguous()
        residual_dtype = (
            residual.dtype
            if residual is not None
            else (torch.float32 if residual_in_fp32 else None)
        )
        y, mean, rstd, residual_out = _layer_norm_fwd(
            x,
            weight,
            bias,
            eps,
            residual,
            residual_dtype=residual_dtype,
            is_rms_norm=is_rms_norm,
        )
        ctx.save_for_backward(residual_out, weight, bias, mean, rstd)
        ctx.x_shape_og = x_shape_og
        ctx.eps = eps
        ctx.is_rms_norm = is_rms_norm
        ctx.has_residual = residual is not None
        ctx.prenorm = prenorm
        ctx.x_dtype = x.dtype
        y = y.reshape(x_shape_og)
        return y if not prenorm else (y, residual_out.reshape(x_shape_og))

    @staticmethod
    def backward(ctx, dy, *args):
        x, weight, bias, mean, rstd = ctx.saved_tensors
        dy = dy.reshape(-1, dy.shape[-1])
        if dy.stride(-1) != 1:
            dy = dy.contiguous()
        assert dy.shape == x.shape
        if ctx.prenorm:
            dresidual = args[0]
            dresidual = dresidual.reshape(-1, dresidual.shape[-1])
            if dresidual.stride(-1) != 1:
                dresidual = dresidual.contiguous()
            assert dresidual.shape == x.shape
        else:
            dresidual = None
        dx, dw, db, dresidual_in = _layer_norm_bwd(
            dy,
            x,
            weight,
            bias,
            ctx.eps,
            mean,
            rstd,
            dresidual,
            ctx.has_residual,
            ctx.is_rms_norm,
            x_dtype=ctx.x_dtype,
        )
        return (
            dx.reshape(ctx.x_shape_og),
            dw,
            db,
            dresidual_in.reshape(ctx.x_shape_og) if ctx.has_residual else None,
            None,
            None,
            None,
            None,
        )


def layer_norm_fn(
    x,
    weight,
    bias,
    residual=None,
    eps=1e-6,
    prenorm=False,
    residual_in_fp32=False,
    is_rms_norm=False,
):
    return LayerNormFn.apply(
        x, weight, bias, residual, eps, prenorm, residual_in_fp32, is_rms_norm
    )


def rms_norm_fn(
    x, weight, bias, residual=None, prenorm=False, residual_in_fp32=False, eps=1e-6
):
    return LayerNormFn.apply(
        x, weight, bias, residual, eps, prenorm, residual_in_fp32, True
    )


class RMSNorm(torch.nn.Module):
    def __init__(self, hidden_size, eps=1e-5, device=None, dtype=None):
        factory_kwargs = {"device": device, "dtype": dtype}
        super().__init__()
        self.eps = eps
        self.weight = torch.nn.Parameter(torch.empty(hidden_size, **factory_kwargs))
        self.register_parameter("bias", None)
        self.reset_parameters()

    def reset_parameters(self):
        torch.nn.init.ones_(self.weight)

    def forward(self, x, residual=None, prenorm=False, residual_in_fp32=False):
        return rms_norm_fn(
            x,
            self.weight,
            self.bias,
            residual=residual,
            eps=self.eps,
            prenorm=prenorm,
            residual_in_fp32=residual_in_fp32,
        )


class LayerNormLinearFn(torch.autograd.Function):
    @staticmethod
    @custom_fwd
    def forward(
        ctx,
        x,
        norm_weight,
        norm_bias,
        linear_weight,
        linear_bias,
        residual=None,
        eps=1e-6,
        prenorm=False,
        residual_in_fp32=False,
        is_rms_norm=False,
    ):
        x_shape_og = x.shape
        # reshape input data into 2D tensor
        x = x.reshape(-1, x.shape[-1])
        if x.stride(-1) != 1:
            x = x.contiguous()
        if residual is not None:
            assert residual.shape == x_shape_og
            residual = residual.reshape(-1, residual.shape[-1])
            if residual.stride(-1) != 1:
                residual = residual.contiguous()
        norm_weight = norm_weight.contiguous()
        if norm_bias is not None:
            norm_bias = norm_bias.contiguous()
        residual_dtype = (
            residual.dtype
            if residual is not None
            else (torch.float32 if residual_in_fp32 else None)
        )
        y, mean, rstd, residual_out = _layer_norm_fwd(
            x,
            norm_weight,
            norm_bias,
            eps,
            residual,
            out_dtype=(
                None
                if not torch.is_autocast_enabled()
                else torch.get_autocast_gpu_dtype()
            ),
            residual_dtype=residual_dtype,
            is_rms_norm=is_rms_norm,
        )
        y = y.reshape(x_shape_og)
        dtype = (
            torch.get_autocast_gpu_dtype() if torch.is_autocast_enabled() else y.dtype
        )
        linear_weight = linear_weight.to(dtype)
        linear_bias = linear_bias.to(dtype) if linear_bias is not None else None
        out = F.linear(y.to(linear_weight.dtype), linear_weight, linear_bias)
        # We don't store y, will be recomputed in the backward pass to save memory
        ctx.save_for_backward(
            residual_out, norm_weight, norm_bias, linear_weight, mean, rstd
        )
        ctx.x_shape_og = x_shape_og
        ctx.eps = eps
        ctx.is_rms_norm = is_rms_norm
        ctx.has_residual = residual is not None
        ctx.prenorm = prenorm
        ctx.x_dtype = x.dtype
        ctx.linear_bias_is_none = linear_bias is None
        return out if not prenorm else (out, residual_out.reshape(x_shape_og))

    @staticmethod
    @custom_bwd
    def backward(ctx, dout, *args):
        x, norm_weight, norm_bias, linear_weight, mean, rstd = ctx.saved_tensors
        dout = dout.reshape(-1, dout.shape[-1])
        dy = F.linear(dout, linear_weight.t())
        dlinear_bias = None if ctx.linear_bias_is_none else dout.sum(0)
        if dy.stride(-1) != 1:
            dy = dy.contiguous()
        assert dy.shape == x.shape
        if ctx.prenorm:
            dresidual = args[0]
            dresidual = dresidual.reshape(-1, dresidual.shape[-1])
            if dresidual.stride(-1) != 1:
                dresidual = dresidual.contiguous()
            assert dresidual.shape == x.shape
        else:
            dresidual = None
        dx, dnorm_weight, dnorm_bias, dresidual_in, y = _layer_norm_bwd(
            dy,
            x,
            norm_weight,
            norm_bias,
            ctx.eps,
            mean,
            rstd,
            dresidual,
            ctx.has_residual,
            ctx.is_rms_norm,
            x_dtype=ctx.x_dtype,
            recompute_output=True,
        )
        dlinear_weight = torch.einsum("bo,bi->oi", dout, y)
        return (
            dx.reshape(ctx.x_shape_og),
            dnorm_weight,
            dnorm_bias,
            dlinear_weight,
            dlinear_bias,
            dresidual_in.reshape(ctx.x_shape_og) if ctx.has_residual else None,
            None,
            None,
            None,
            None,
        )


def layer_norm_linear_fn(
    x,
    norm_weight,
    norm_bias,
    linear_weight,
    linear_bias,
    residual=None,
    eps=1e-6,
    prenorm=False,
    residual_in_fp32=False,
    is_rms_norm=False,
):
    return LayerNormLinearFn.apply(
        x,
        norm_weight,
        norm_bias,
        linear_weight,
        linear_bias,
        residual,
        eps,
        prenorm,
        residual_in_fp32,
        is_rms_norm,
    )



================================================
FILE: sauron/mil_models/mamba_ssm/ops/triton/selective_state_update.py
================================================
# Copyright (c) 2023, Tri Dao.

"""We want triton==2.1.0 for this"""

import torch
import torch.nn.functional as F
import triton
import triton.language as tl
from einops import rearrange


@triton.heuristics({"HAS_DT_BIAS": lambda args: args["dt_bias_ptr"] is not None})
@triton.heuristics({"HAS_D": lambda args: args["D_ptr"] is not None})
@triton.heuristics({"HAS_Z": lambda args: args["z_ptr"] is not None})
@triton.heuristics(
    {"BLOCK_SIZE_DSTATE": lambda args: triton.next_power_of_2(args["dstate"])}
)
@triton.jit
def _selective_scan_update_kernel(
    # Pointers to matrices
    state_ptr,
    x_ptr,
    dt_ptr,
    dt_bias_ptr,
    A_ptr,
    B_ptr,
    C_ptr,
    D_ptr,
    z_ptr,
    out_ptr,
    # Matrix dimensions
    batch,
    dim,
    dstate,
    # Strides
    stride_state_batch,
    stride_state_dim,
    stride_state_dstate,
    stride_x_batch,
    stride_x_dim,
    stride_dt_batch,
    stride_dt_dim,
    stride_dt_bias_dim,
    stride_A_dim,
    stride_A_dstate,
    stride_B_batch,
    stride_B_dstate,
    stride_C_batch,
    stride_C_dstate,
    stride_D_dim,
    stride_z_batch,
    stride_z_dim,
    stride_out_batch,
    stride_out_dim,
    # Meta-parameters
    DT_SOFTPLUS: tl.constexpr,
    BLOCK_SIZE_M: tl.constexpr,
    HAS_DT_BIAS: tl.constexpr,
    HAS_D: tl.constexpr,
    HAS_Z: tl.constexpr,
    BLOCK_SIZE_DSTATE: tl.constexpr,
):
    pid_m = tl.program_id(axis=0)
    pid_b = tl.program_id(axis=1)
    state_ptr += pid_b * stride_state_batch
    x_ptr += pid_b * stride_x_batch
    dt_ptr += pid_b * stride_dt_batch
    B_ptr += pid_b * stride_B_batch
    C_ptr += pid_b * stride_C_batch
    if HAS_Z:
        z_ptr += pid_b * stride_z_batch
    out_ptr += pid_b * stride_out_batch

    offs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)
    offs_n = tl.arange(0, BLOCK_SIZE_DSTATE)
    state_ptrs = state_ptr + (
        offs_m[:, None] * stride_state_dim + offs_n[None, :] * stride_state_dstate
    )
    x_ptrs = x_ptr + offs_m * stride_x_dim
    dt_ptrs = dt_ptr + offs_m * stride_dt_dim
    if HAS_DT_BIAS:
        dt_bias_ptrs = dt_bias_ptr + offs_m * stride_dt_bias_dim
    A_ptrs = A_ptr + (
        offs_m[:, None] * stride_A_dim + offs_n[None, :] * stride_A_dstate
    )
    B_ptrs = B_ptr + offs_n * stride_B_dstate
    C_ptrs = C_ptr + offs_n * stride_C_dstate
    if HAS_D:
        D_ptrs = D_ptr + offs_m * stride_D_dim
    if HAS_Z:
        z_ptrs = z_ptr + offs_m * stride_z_dim
    out_ptrs = out_ptr + offs_m * stride_out_dim

    state = tl.load(
        state_ptrs, mask=(offs_m[:, None] < dim) & (offs_n[None, :] < dstate), other=0.0
    )
    x = tl.load(x_ptrs, mask=offs_m < dim, other=0.0).to(tl.float32)
    dt = tl.load(dt_ptrs, mask=offs_m < dim, other=0.0).to(tl.float32)
    if HAS_DT_BIAS:
        dt += tl.load(dt_bias_ptrs, mask=offs_m < dim, other=0.0).to(tl.float32)
    if DT_SOFTPLUS:
        dt = tl.log(1.0 + tl.exp(dt))
    A = tl.load(
        A_ptrs, mask=(offs_m[:, None] < dim) & (offs_n[None, :] < dstate), other=0.0
    ).to(tl.float32)
    dA = tl.exp(A * dt[:, None])
    B = tl.load(B_ptrs, mask=offs_n < dstate, other=0.0).to(tl.float32)
    C = tl.load(C_ptrs, mask=offs_n < dstate, other=0.0).to(tl.float32)
    if HAS_D:
        D = tl.load(D_ptrs, mask=offs_m < dim, other=0.0).to(tl.float32)
    if HAS_Z:
        z = tl.load(z_ptrs, mask=offs_m < dim, other=0.0).to(tl.float32)

    dB = B[None, :] * dt[:, None]
    state = state * dA + dB * x[:, None]
    tl.store(
        state_ptrs, state, mask=(offs_m[:, None] < dim) & (offs_n[None, :] < dstate)
    )
    out = tl.sum(state * C[None, :], axis=1)
    if HAS_D:
        out += x * D
    if HAS_Z:
        out *= z * tl.sigmoid(z)
    tl.store(out_ptrs, out, mask=offs_m < dim)


def selective_state_update(
    state, x, dt, A, B, C, D=None, z=None, dt_bias=None, dt_softplus=False
):
    """
    Argument:
        state: (batch, dim, dstate)
        x: (batch, dim)
        dt: (batch, dim)
        A: (dim, dstate)
        B: (batch, dstate)
        C: (batch, dstate)
        D: (dim,)
        z: (batch, dim)
        dt_bias: (dim,)
    Return:
        out: (batch, dim)
    """
    batch, dim, dstate = state.shape
    assert x.shape == (batch, dim)
    assert dt.shape == x.shape
    assert A.shape == (dim, dstate)
    assert B.shape == (batch, dstate)
    assert C.shape == B.shape
    if D is not None:
        assert D.shape == (dim,)
    if z is not None:
        assert z.shape == x.shape
    if dt_bias is not None:
        assert dt_bias.shape == (dim,)
    out = torch.empty_like(x)

    def grid(META):
        return triton.cdiv(dim, META["BLOCK_SIZE_M"]), batch

    z_strides = (z.stride(0), z.stride(1)) if z is not None else (0, 0)
    # We don't want autotune since it will overwrite the state
    # We instead tune by hand.
    BLOCK_SIZE_M, num_warps = (
        (32, 4)
        if dstate <= 16
        else (
            (16, 4)
            if dstate <= 32
            else ((8, 4) if dstate <= 64 else ((4, 4) if dstate <= 128 else ((4, 8))))
        )
    )
    with torch.cuda.device(x.device.index):
        _selective_scan_update_kernel[grid](
            state,
            x,
            dt,
            dt_bias,
            A,
            B,
            C,
            D,
            z,
            out,
            batch,
            dim,
            dstate,
            state.stride(0),
            state.stride(1),
            state.stride(2),
            x.stride(0),
            x.stride(1),
            dt.stride(0),
            dt.stride(1),
            dt_bias.stride(0) if dt_bias is not None else 0,
            A.stride(0),
            A.stride(1),
            B.stride(0),
            B.stride(1),
            C.stride(0),
            C.stride(1),
            D.stride(0) if D is not None else 0,
            z_strides[0],
            z_strides[1],
            out.stride(0),
            out.stride(1),
            dt_softplus,
            BLOCK_SIZE_M,
            num_warps=num_warps,
        )
    return out


def selective_state_update_ref(
    state, x, dt, A, B, C, D=None, z=None, dt_bias=None, dt_softplus=False
):
    """
    Argument:
        state: (batch, dim, dstate)
        x: (batch, dim)
        dt: (batch, dim)
        A: (dim, dstate)
        B: (batch, dstate)
        C: (batch, dstate)
        D: (dim,)
        z: (batch, dim)
        dt_bias: (dim,)
    Return:
        out: (batch, dim)
    """
    batch, dim, dstate = state.shape
    assert x.shape == (batch, dim)
    assert dt.shape == x.shape
    assert A.shape == (dim, dstate)
    assert B.shape == (batch, dstate)
    assert C.shape == B.shape
    if D is not None:
        assert D.shape == (dim,)
    if z is not None:
        assert z.shape == x.shape
    if dt_bias is not None:
        assert dt_bias.shape == (dim,)
        dt = dt + dt_bias
    dt = F.softplus(dt) if dt_softplus else dt
    dA = torch.exp(rearrange(dt, "b d -> b d 1") * A)  # (batch, dim, dstate)
    dB = rearrange(dt, "b d -> b d 1") * rearrange(
        B, "b n -> b 1 n"
    )  # (batch, dim, dstate)
    state.copy_(state * dA + dB * rearrange(x, "b d -> b d 1"))  # (batch, dim, dstate
    out = torch.einsum("bdn,bn->bd", state.to(C.dtype), C)
    if D is not None:
        out += (x * D).to(out.dtype)
    return (out if z is None else out * F.silu(z)).to(x.dtype)



================================================
FILE: sauron/mil_models/mamba_ssm/utils/__init__.py
================================================



================================================
FILE: sauron/mil_models/mamba_ssm/utils/generation.py
================================================
# Copyright (c) 2023, Albert Gu, Tri Dao.
import gc
import time
from collections import namedtuple
from dataclasses import dataclass, field
from functools import partial
from typing import Callable, Optional, Sequence, Union

import torch
import torch.nn.functional as F
from einops import rearrange, repeat
from torch import Tensor
from torch.profiler import ProfilerActivity, profile, record_function
from transformers.generation import (
    GreedySearchDecoderOnlyOutput,
    SampleDecoderOnlyOutput,
    TextStreamer,
)


@dataclass
class InferenceParams:
    """Inference parameters that are passed to the main model in order
    to efficienly calculate and store the context during inference."""

    max_seqlen: int
    max_batch_size: int
    seqlen_offset: int = 0
    batch_size_offset: int = 0
    key_value_memory_dict: dict = field(default_factory=dict)
    lengths_per_sample: Optional[Tensor] = None

    def reset(self, max_seqlen, max_batch_size):
        self.max_seqlen = max_seqlen
        self.max_batch_size = max_batch_size
        self.seqlen_offset = 0
        if self.lengths_per_sample is not None:
            self.lengths_per_sample.zero_()


# https://github.com/NVIDIA/Megatron-LM/blob/0bb597b42c53355a567aba2a1357cc34b9d99ddd/megatron/text_generation/sampling.py
# https://github.com/huggingface/transformers/blob/a44985b41cfa2de48a5e1de7f1f93b7483da25d1/src/transformers/generation/logits_process.py#L231
def modify_logits_for_top_k_filtering(logits, top_k):
    """Set the logits for none top-k values to -inf. Done in-place."""
    indices_to_remove = logits < torch.topk(logits, top_k)[0][..., -1, None]
    logits.masked_fill_(indices_to_remove, float("-Inf"))


# https://github.com/NVIDIA/Megatron-LM/blob/0bb597b42c53355a567aba2a1357cc34b9d99ddd/megatron/text_generation/sampling.py
# https://github.com/huggingface/transformers/blob/a44985b41cfa2de48a5e1de7f1f93b7483da25d1/src/transformers/generation/logits_process.py#L170
def modify_logits_for_top_p_filtering(logits, top_p):
    """Set the logits for none top-p values to -inf. Done in-place."""
    if top_p <= 0.0 or top_p >= 1.0:
        return
    # First sort and calculate cumulative sum of probabilities.
    sorted_logits, sorted_indices = torch.sort(logits, descending=False)
    cumulative_probs = sorted_logits.softmax(dim=-1).cumsum(dim=-1)
    # Remove tokens with cumulative top_p above the threshold (token with 0 are kept)
    sorted_indices_to_remove = cumulative_probs <= (1 - top_p)
    # scatter sorted tensors to original indexing
    indices_to_remove = sorted_indices_to_remove.scatter(
        1, sorted_indices, sorted_indices_to_remove
    )
    logits.masked_fill_(indices_to_remove, float("-inf"))


def modify_logit_for_repetition_penalty(
    logits, prev_output_tokens, repetition_penalty=1.0
):
    """Apply repetition penalty. See https://arxiv.org/abs/1909.05858
    logits: (batch_size, vocab_size)
    prev_output_tokens: (batch_size, seq_len)
    """
    if repetition_penalty == 1.0:
        return logits
    score = torch.gather(logits, 1, prev_output_tokens)
    # if score < 0 then repetition penalty has to be multiplied to reduce the previous token probability
    score = torch.where(
        score < 0, score * repetition_penalty, score / repetition_penalty
    )
    logits.scatter_(1, prev_output_tokens, score)
    return logits


def sample(logits, top_k=1, top_p=0.0, temperature=1.0):
    """Sample from top-k logits.
    Arguments:
        logits: Tensor of shape (batch_size, vocab_size)
    """
    if top_k == 1:  # Short-circuit for greedy decoding
        return logits.argmax(dim=-1)
    else:
        if top_p > 0.0:
            assert top_p <= 1.0, "top-p should be in (0, 1]."
        if top_k > 0:
            top_k = min(top_k, logits.size(-1))  # Safety check
            logits_top, indices = torch.topk(logits, top_k, dim=-1)
            if temperature != 1.0:
                logits_top /= temperature
            modify_logits_for_top_p_filtering(logits_top, top_p)
            return indices[
                torch.arange(indices.shape[0], device=indices.device),
                torch.multinomial(
                    torch.softmax(logits_top, dim=-1), num_samples=1
                ).squeeze(dim=-1),
            ]
        else:
            # Clone so that when we modify for top_p we don't change the original logits
            logits_top = logits / temperature if temperature != 1.0 else logits.clone()
            modify_logits_for_top_p_filtering(logits_top, top_p)
            return torch.multinomial(
                torch.softmax(logits_top, dim=-1), num_samples=1
            ).squeeze(dim=-1)


@torch.inference_mode()
def decode(
    input_ids,
    model,
    max_length,
    top_k=1,
    top_p=0.0,
    temperature=1.0,
    repetition_penalty=1.0,
    eos_token_id=None,
    teacher_outputs=None,
    vocab_size=None,
    cg=False,
    enable_timing=False,
    streamer: Optional[TextStreamer] = None,
):
    """Decoding, either greedy or with top-k or top-p sampling.
    If top-k = 0, don't limit the number of candidates (pure sampling).
    Top-k and top-p can be used together. If top_k > 0 and top_p > 0, then top-k is applied first,
    then top-p.
    We assume that all sequences in the same batch have the same length.

    Arguments:
        input_ids: (batch, seq_len)
        max_length: int
        teacher_outputs (optional): (batch, seq_len). If provided, instead of sampling from the
            logits, the next token is taken from the teacher_outputs. Useful for testing.
    Returns: GreedySearchDecoderOnlyOutput or SampleDecoderOnlyOutput, with the following fields:
        sequences: (batch, max_length)
        scores: tuples of (batch, vocab_size)
    """
    if streamer is not None:
        streamer.put(input_ids.cpu())

    batch_size, seqlen_og = input_ids.shape
    teacher_output_len = teacher_outputs.shape[1] if teacher_outputs is not None else 0
    if cg:
        if not hasattr(model, "_decoding_cache"):
            model._decoding_cache = None
        model._decoding_cache = update_graph_cache(
            model,
            model._decoding_cache,
            batch_size,
            seqlen_og,
            max_length,
        )
        inference_params = model._decoding_cache.inference_params
        inference_params.reset(max_length, batch_size)
    else:
        inference_params = InferenceParams(
            max_seqlen=max_length, max_batch_size=batch_size
        )

    def get_logits(input_ids, inference_params):
        decoding = inference_params.seqlen_offset > 0
        if decoding:
            position_ids = torch.full(
                (batch_size, 1),
                inference_params.seqlen_offset,
                dtype=torch.long,
                device=input_ids.device,
            )
        else:
            position_ids = None
        if not cg or not decoding:
            logits = model(
                input_ids,
                position_ids=position_ids,
                inference_params=inference_params,
                num_last_tokens=1,
            ).logits.squeeze(dim=1)
        else:
            logits = model._decoding_cache.run(
                input_ids, position_ids, inference_params.seqlen_offset
            ).squeeze(dim=1)
        return logits[..., :vocab_size] if vocab_size is not None else logits

    def sample_tokens(logits, inference_params):
        if (
            teacher_outputs is None
            or teacher_output_len <= inference_params.seqlen_offset
        ):
            token = sample(logits, top_k=top_k, top_p=top_p, temperature=temperature)
        else:
            token = teacher_outputs[:, inference_params.seqlen_offset]
        # return rearrange(token, "b -> b 1")
        return token.unsqueeze(1)

    def should_stop(current_token, inference_params):
        if inference_params.seqlen_offset == 0:
            return False
        if eos_token_id is not None and (current_token == eos_token_id).all():
            return True
        if inference_params.seqlen_offset >= max_length - 1:
            return True
        return False

    start = torch.cuda.Event(enable_timing=enable_timing)
    end = torch.cuda.Event(enable_timing=enable_timing)

    if enable_timing:
        start.record()
    scores, sequences = [], [input_ids]
    sequences_cat = input_ids
    while not should_stop(sequences[-1], inference_params):
        scores.append(get_logits(sequences[-1], inference_params))
        inference_params.seqlen_offset += sequences[-1].shape[1]
        if repetition_penalty == 1.0:
            sampled_tokens = sample_tokens(scores[-1], inference_params)
        else:
            logits = modify_logit_for_repetition_penalty(
                scores[-1].clone(), sequences_cat, repetition_penalty
            )
            sampled_tokens = sample_tokens(logits, inference_params)
            sequences_cat = torch.cat([sequences_cat, sampled_tokens], dim=1)
        sequences.append(sampled_tokens)
        if streamer is not None:
            streamer.put(sampled_tokens.cpu())
    if streamer is not None:
        streamer.end()
    if enable_timing:
        end.record()
        torch.cuda.synchronize()
        print(f"Prompt processing + decoding time: {(start.elapsed_time(end)):.0f}ms")
    output_cls = (
        GreedySearchDecoderOnlyOutput if top_k == 1 else SampleDecoderOnlyOutput
    )
    return output_cls(sequences=torch.cat(sequences, dim=1), scores=tuple(scores))


class GenerationMixin:
    def allocate_inference_cache(self, batch_size, max_seqlen, dtype=None, **kwargs):
        raise NotImplementedError

    def generate(
        self,
        input_ids,
        max_length,
        top_k=1,
        top_p=0.0,
        temperature=1.0,
        return_dict_in_generate=False,
        output_scores=False,
        **kwargs,
    ):
        output = decode(
            input_ids,
            self,
            max_length,
            top_k=top_k,
            top_p=top_p,
            temperature=temperature,
            **kwargs,
        )
        if not output_scores:
            output.scores = None
        return output if return_dict_in_generate else output.sequences


@dataclass
class DecodingCGCache:
    max_batch_size: int = 0
    max_seqlen: int = 0
    device = None
    dtype = None
    callables: dict = field(default_factory=dict)
    mempool = None
    inference_params: Optional[InferenceParams] = None
    run: Optional[Callable] = None


@torch.inference_mode()
def update_graph_cache(
    model,
    cache,
    batch_size,
    seqlen_og,
    max_seqlen,
    decoding_seqlens=(1,),
    dtype=None,
    n_warmups=2,
):
    if cache is None:
        cache = DecodingCGCache()
    param_example = next(iter(model.parameters()))
    device = param_example.device
    if dtype is None:
        dtype = param_example.dtype
    if (
        (device, dtype) != (cache.device, cache.dtype)
        or batch_size > cache.max_batch_size
        or max_seqlen > cache.max_seqlen
    ):  # Invalidate the cache
        cache.callables = {}
        cache.mempool = None
        cache.inference_params = None
        gc.collect()
        cache.device, cache.dtype = device, dtype
        cache.max_batch_size, cache.max_seqlen = batch_size, max_seqlen
        assert hasattr(
            model, "allocate_inference_cache"
        ), "CUDA graph decoding requires that the model has a method allocate_inference_cache"
        inf_cache = model.allocate_inference_cache(batch_size, max_seqlen, dtype)
        lengths_per_sample = torch.full(
            (batch_size,), seqlen_og, dtype=torch.int32, device=device
        )
        cache.inference_params = InferenceParams(
            max_seqlen=max_seqlen,
            max_batch_size=batch_size,
            seqlen_offset=seqlen_og,
            key_value_memory_dict=inf_cache,
            lengths_per_sample=lengths_per_sample,
        )
        cache.mempool = torch.cuda.graphs.graph_pool_handle()
    for decoding_seqlen in decoding_seqlens:
        if (batch_size, decoding_seqlen) not in cache.callables:
            cache.callables[batch_size, decoding_seqlen] = capture_graph(
                model,
                cache.inference_params,
                batch_size,
                max_seqlen,
                decoding_seqlen=decoding_seqlen,
                mempool=cache.mempool,
                n_warmups=n_warmups,
            )

    def dispatch(input_ids, position_ids, seqlen):
        batch_size, decoding_seqlen = input_ids.shape[:2]
        return cache.callables[batch_size, decoding_seqlen](
            input_ids, position_ids, seqlen
        )

    cache.run = dispatch
    cache.inference_params.seqlen_offset = 0  # Reset so it's not confusing
    return cache


def capture_graph(
    model,
    inference_params,
    batch_size,
    max_seqlen,
    decoding_seqlen=1,
    mempool=None,
    n_warmups=2,
):
    device = next(iter(model.parameters())).device
    input_ids = torch.full(
        (batch_size, decoding_seqlen), 0, dtype=torch.long, device=device
    )
    position_ids = torch.full(
        (batch_size, decoding_seqlen), 0, dtype=torch.long, device=device
    )
    seqlen_offset_og = inference_params.seqlen_offset
    inference_params.seqlen_offset = max_seqlen - decoding_seqlen
    inference_params.lengths_per_sample[:] = inference_params.seqlen_offset

    # Warmup before capture
    s = torch.cuda.Stream()
    s.wait_stream(torch.cuda.current_stream())
    with torch.cuda.stream(s):
        for _ in range(n_warmups):
            logits = model(
                input_ids,
                position_ids=position_ids,
                inference_params=inference_params,
                num_last_tokens=decoding_seqlen,
            ).logits
        s.synchronize()
        # This might be needed for correctness if we run with NCCL_GRAPH_MIXING_SUPPORT=0,
        # which requires that graph launch and non-captured launch to not overlap (I think,
        # that's how I interpret the documentation). I'm not sure if this is required.
        if torch.distributed.is_initialized():
            torch.distributed.barrier()
    torch.cuda.current_stream().wait_stream(s)
    # Captures the graph
    # To allow capture, automatically sets a side stream as the current stream in the context
    graph = torch.cuda.CUDAGraph()
    with torch.cuda.graph(graph, pool=mempool):
        logits = model(
            input_ids,
            position_ids=position_ids,
            inference_params=inference_params,
            num_last_tokens=decoding_seqlen,
        ).logits

    def run(new_input_ids, new_position_ids, seqlen):
        inference_params.lengths_per_sample[:] = seqlen
        input_ids.copy_(new_input_ids)
        position_ids.copy_(new_position_ids)
        graph.replay()
        return logits.clone()

    inference_params.seqlen_offset = seqlen_offset_og
    return run



================================================
FILE: sauron/parse/cli_parsers.py
================================================
import argparse


def build_feature_extraction_parser():
    """
    Parse command-line arguments for the Sauron feature extraction script.
    """
    parser = argparse.ArgumentParser(
        description="Run Sauron Whole Slide Image Processing"
    )

    # Generic arguments
    parser.add_argument(
        "--gpu", type=int, default=0, help="GPU index to use for processing tasks."
    )
    parser.add_argument(
        "--task",
        type=str,
        default="seg",
        choices=["seg", "coords", "feat", "all", "cache"],
        help="Task to run: seg (segmentation), coords (save tissue coordinates), feat (extract features), all (run all steps), cache (populate WSI cache only).",
    )
    parser.add_argument(
        "--job_dir", type=str, required=True, help="Directory to store outputs."
    )
    parser.add_argument(
        "--skip_errors",
        action="store_true",
        default=False,
        help="Skip errored slides and continue processing.",
    )
    parser.add_argument(
        "--max_workers",
        type=int,
        default=None,
        help="Maximum number of workers for data loading (e.g., in DataLoader). If None, inferred based on CPU cores.",
    )
    parser.add_argument(
        "--batch_size",
        type=int,
        default=64,
        help="Batch size used for segmentation and feature extraction. Will be overridden by "
        "`seg_batch_size` and `feat_batch_size` if specified. Defaults to 64.",
    )

    # Caching argument for fast WSI processing
    parser.add_argument(
        "--wsi_cache",
        type=str,
        default=None,
        help="Path to a local cache (e.g., SSD) used to speed up access to WSIs stored on slower drives (e.g., HDD). "
        "If provided, WSIs are copied here before processing.",
    )
    parser.add_argument(
        "--cache_batch_size",
        type=int,
        default=32,
        help="Maximum number of slides to cache locally at once when using --wsi_cache. Helps control disk usage.",
    )
    parser.add_argument(
        "--clear_cache",
        action="store_true",
        default=False,
        help="If using --wsi_cache, delete cached WSIs after processing each batch.",
    )

    # Slide-related arguments
    parser.add_argument(
        "--wsi_dir",
        type=str,
        required=True,
        help="Directory containing WSI files (can be nested if --search_nested is used).",
    )
    parser.add_argument(
        "--wsi_ext",
        type=str,
        nargs="+",
        default=None,
        help="List of allowed file extensions for WSI files (e.g., .svs .tif). If None, common extensions are used.",
    )
    parser.add_argument(
        "--custom_mpp_keys",
        type=str,
        nargs="+",
        default=None,
        help="Custom keys used to store the resolution as MPP (micron per pixel) in WSI metadata.",
    )
    parser.add_argument(
        "--custom_list_of_wsis",
        type=str,
        default=None,
        help='Path to a CSV file specifying a custom list of WSIs to process. Must contain a "wsi" column and optionally an "mpp" column.',
    )
    parser.add_argument(
        "--reader_type",
        type=str,
        choices=["openslide", "image", "cucim"],
        default=None,
        help='Force the use of a specific WSI image reader. Options are ["openslide", "image", "cucim"]. Defaults to None (auto-determine which reader to use).',
    )
    parser.add_argument(
        "--search_nested",
        action="store_true",
        help=(
            "If set, recursively search for whole-slide images (WSIs) within all subdirectories of "
            "`wsi_dir`. Uses `os.walk` to include slides from nested folders. "
            "Defaults to False (only top-level slides are included)."
        ),
    )

    # Segmentation arguments
    parser.add_argument(
        "--segmenter",
        type=str,
        default="hest",
        choices=["hest", "grandqc"],
        help="Type of tissue vs background segmenter model to use. Options are HEST or GrandQC.",
    )
    parser.add_argument(
        "--seg_conf_thresh",
        type=float,
        default=0.5,
        help="Confidence threshold to apply to binarize segmentation predictions. Lower this threshold to retain more tissue. Defaults to 0.5. Try 0.4 as 2nd option.",
    )
    parser.add_argument(
        "--remove_holes",
        action="store_true",
        default=False,
        help="If set, removes holes detected within tissue regions from the segmentation mask.",
    )
    parser.add_argument(
        "--remove_artifacts",
        action="store_true",
        default=False,
        help="If set, runs an additional GrandQC-based model to remove artifacts (including penmarks, blurs, stains, etc.) from the tissue segmentation.",
    )
    parser.add_argument(
        "--remove_penmarks",
        action="store_true",
        default=False,
        help="If set (and --remove_artifacts is not set), runs a specialized GrandQC-based model to remove only penmarks from the tissue segmentation.",
    )
    parser.add_argument(
        "--seg_batch_size",
        type=int,
        default=None,
        help="Batch size for segmentation. Defaults to None (use `batch_size` argument instead).",
    )

    # Patching arguments
    parser.add_argument(
        "--mag",
        type=int,
        choices=[5, 10, 20, 40, 80],
        default=20,
        help="Magnification level (e.g., 20 for 20x) at which to extract patches and features.",
    )
    parser.add_argument(
        "--patch_size",
        type=int,
        default=512,
        help="Side length of square patches in pixels at the specified magnification.",
    )
    parser.add_argument(
        "--overlap",
        type=int,
        default=0,
        help="Absolute overlap between adjacent patches in pixels (at the specified magnification). Defaults to 0.",
    )
    parser.add_argument(
        "--min_tissue_proportion",
        type=float,
        default=0.0,
        help="Minimum proportion of the patch area that must contain tissue to be kept. Between 0. and 1.0. Defaults to 0. (any tissue).",
    )
    parser.add_argument(
        "--coords_dir_name",
        type=str,
        default=None,  # Changed from coords_dir
        help="Name of the directory to save/restore tissue coordinates (relative to job_dir). If None, auto-generated.",
    )

    # Feature extraction arguments
    parser.add_argument(
        "--patch_encoder",
        type=str,
        default="conch_v15",
        choices=[  # List all supported patch encoders from sauron.feature_extraction.models.patch_encoders.factory
            "conch_v1",
            "uni_v1",
            "uni_v2",
            "ctranspath",
            "phikon",
            "resnet50",
            "gigapath",
            "virchow",
            "virchow2",
            "hoptimus0",
            "hoptimus1",
            "phikon_v2",
            "conch_v15",
            "musk",
            "hibou_l",
            "kaiko-vits8",
            "kaiko-vits16",
            "kaiko-vitb8",
            "kaiko-vitb16",
            "kaiko-vitl14",
            "lunit-vits8",
            "midnight12k",
        ],
        help="Patch encoder model to use for feature extraction.",
    )
    parser.add_argument(
        "--patch_encoder_ckpt_path",
        type=str,
        default=None,
        help=(
            "Optional local path to a patch encoder checkpoint (.pt, .pth, .bin, or .safetensors). "
            "This overrides the default download mechanism and model registry. "
            "Useful for offline environments or custom checkpoints."
        ),
    )
    parser.add_argument(
        "--slide_encoder",
        type=str,
        default=None,
        choices=[  # List all supported slide encoders from sauron.feature_extraction.models.slide_encoders.factory
            "threads",
            "titan",
            "prism",
            "gigapath",
            "chief",
            "madeleine",
            # Mean-pooling variants (derived from patch encoders)
            "mean-virchow",
            "mean-virchow2",
            "mean-conch_v1",
            "mean-conch_v15",
            "mean-ctranspath",
            "mean-gigapath",
            "mean-resnet50",
            "mean-hoptimus0",
            "mean-phikon",
            "mean-phikon_v2",
            "mean-musk",
            "mean-uni_v1",
            "mean-uni_v2",
            "mean-hibou_l",
            "mean-lunit-vits8",
            "mean-midnight12k",
            "mean-kaiko-vits8",
            "mean-kaiko-vits16",
            "mean-kaiko-vitb8",
            "kaiko-vitb16",
            "kaiko-vitl14",
        ],
        help="Slide encoder model to use for feature extraction. If specified, will automatically extract required patch features.",
    )
    parser.add_argument(
        "--feat_batch_size",
        type=int,
        default=None,
        help="Batch size for feature extraction. Defaults to None (use `batch_size` argument instead).",
    )

    return parser


def parse_feature_extraction_arguments():
    return build_feature_extraction_parser().parse_args()


def get_mil_args():
    parser = argparse.ArgumentParser(
        description="Configurations for Whole Slide Image (WSI) Training",
        formatter_class=argparse.ArgumentDefaultsHelpFormatter,
    )

    # Data & I/O Configuration
    parser.add_argument(
        "--data_root_dir",
        type=str,
        default=None,
        help="Specify the root directory where the dataset is located. This is essential for loading the data correctly.",
    )
    parser.add_argument(
        "--results_dir",
        default="./results",
        help="Path to the directory where training results and model checkpoints will be saved. Default is './results'.",
    )
    parser.add_argument(
        "--split_dir",
        type=str,
        default=None,
        help="Path to the directory containing custom data splits. If not provided, splits will be generated based on the task and label fraction.",
    )
    parser.add_argument(
        "--patch_size",
        type=str,
        default="",
        help="Define the size of image patches in the format [height]x[width]. This is important for processing images.",
    )
    parser.add_argument(
        "--resolution",
        type=str,
        default="20x",
        help="Set the magnification level for processing images. Examples include '10x' or '10x_40x' for combined levels.",
    )
    parser.add_argument(
        "--early_fusion",
        action="store_true",  # Use action='store_true' for boolean flags with default False
        help="Enable or disable early fusion for models that utilize multiple magnification levels. This can enhance model performance.",
    )
    parser.add_argument(
        "--preloading",
        choices=["yes", "no"],
        default="no",
        help="Specify whether to preload data into memory for faster access during training. Options are 'yes' or 'no'.",
    )

    # Training Hyperparameters
    parser.add_argument(
        "--max_epochs",
        type=int,
        default=200,
        help="Set the maximum number of epochs for training the model. Default is 200.",
    )
    parser.add_argument(
        "--lr",
        type=float,
        default=1e-4,
        help="Initial learning rate for the optimizer. Adjust this for better convergence.",
    )
    parser.add_argument(
        "--reg",
        type=float,
        default=1e-5,
        help="Weight decay factor for L2 regularization. Helps prevent overfitting.",
    )
    parser.add_argument(
        "--opt",
        choices=["adam", "sgd", "adamw"],
        default="adam",
        help="Choose the optimizer to use for training. Options include 'adam', 'sgd', or 'adamw'.",
    )
    parser.add_argument(
        "--drop_out",
        type=float,
        default=0.25,
        help="Set the dropout probability to prevent overfitting during training.",
    )
    parser.add_argument(
        "--early_stopping",
        action="store_true",
        help="Enable early stopping to halt training when validation performance stops improving.",
    )
    parser.add_argument(
        "--weighted_sample",
        action="store_true",
        help="Enable weighted sampling to address class imbalance in the training dataset.",
    )
    parser.add_argument(
        "--batch_size",
        type=int,
        default=1,
        help="Set the batch size for training.",
    )

    # Model Configuration
    parser.add_argument(
        "--model_type",
        type=str,
        default="att_mil",
        help="Specify the type of model architecture to use for training. Default is 'att_mil'.",
    )
    parser.add_argument(
        "--backbone",
        type=str,
        default="resnet50",
        help="Select the backbone network for feature extraction. Default is 'resnet50'.",
    )
    parser.add_argument(
        "--in_dim",
        type=int,
        default=1024,
        help="Set the input dimension for the model. This should match the output of the backbone network.",
    )

    # MambaMIL Specific Configuration
    parser.add_argument(
        "--mambamil_rate",
        type=int,
        default=10,
        help="Rate parameter for MambaMIL, influencing the model's behavior.",
    )
    parser.add_argument(
        "--mambamil_layer",
        type=int,
        default=2,
        help="Number of layers in the MambaMIL architecture.",
    )
    parser.add_argument(
        "--mambamil_type",
        choices=["Mamba", "BiMamba", "SRMamba"],
        default="SRMamba",
        help="Select the type of Mamba architecture to use. Options include 'Mamba', 'BiMamba', or 'SRMamba'.",
    )

    # Experiment & Reproducibility
    parser.add_argument(
        "--task",
        type=str,
        required=True,
        help="Specify the task name or identifier for the experiment.",
    )
    parser.add_argument(
        "--task_type",
        type=str,
        required=True,
        help="Specify the task type ('classification' or 'survival').",
    )
    parser.add_argument(
        "--exp_code",
        type=str,
        required=True,
        help="Provide a unique experiment code for tracking purposes.",
    )
    parser.add_argument(
        "--seed",
        type=int,
        default=1,
        help="Set the random seed for reproducibility of results. Default is 1.",
    )
    parser.add_argument(
        "--label_frac",
        type=float,
        default=1.0,
        help="Specify the fraction of training labels to use. Default is 1.0 (use all labels).",
    )
    parser.add_argument(
        "--log_data",
        action="store_true",
        help="Enable logging of training data using TensorBoard for visualization and analysis.",
    )
    parser.add_argument(
        "--testing",
        action="store_true",
        help="Enable testing/debugging mode for the experiment.",
    )

    # Cross-Validation Configuration
    parser.add_argument(
        "--k",
        type=int,
        default=10,
        help="Specify the total number of folds for cross-validation. Default is 10.",
    )
    parser.add_argument(
        "--k_start",
        type=int,
        default=-1,
        help="Set the starting fold for cross-validation. Use -1 for the last fold.",
    )
    parser.add_argument(
        "--k_end",
        type=int,
        default=-1,
        help="Set the ending fold for cross-validation. Use -1 for the first fold.",
    )

    # Survival Configuration
    parser.add_argument(
        "--bag_loss",
        type=str,
        choices=["svm", "ce", "ce_surv", "nll_surv", "cox_surv"],
        default="nll_surv",
        help="Slide-level classification loss function (default: nll_surv).",
    )
    parser.add_argument(
        "--alpha_surv",
        type=float,
        default=0.0,
        help="How much to weigh uncensored patients.",
    )
    parser.add_argument(
        "--lambda_reg",
        type=float,
        default=1e-4,
        help="L1-Regularization Strength (Default 1e-4).",
    )
    parser.add_argument(
        "--inst_loss",
        type=str,
        choices=["svm", "ce", None],
        default=None,
        help="Instance-level clustering loss function (default: None).",
    )
    parser.add_argument(
        "--subtyping",
        action="store_true",  # Use action='store_true' for boolean flags with default False
        help="Enable subtyping problem.",
    )
    parser.add_argument(
        "--bag_weight",
        type=float,
        default=0.7,
        help="Weight coefficient for bag-level loss (default: 0.7).",
    )
    parser.add_argument(
        "--B",
        type=int,
        default=8,
        help="Number of positive/negative patches to sample for clam.",
    )
    parser.add_argument(
        "--gc",
        type=int,
        default=32,
        help="Gradient Accumulation Step.",
    )

    args = parser.parse_args()
    return args



================================================
FILE: sauron/training/__init__.py
================================================



================================================
FILE: sauron/training/cli_runner.py
================================================
# sauron/training/cli_runner.py (This replaces your root train_mil.py script)
import argparse
import json
import os

import pandas as pd
from torch.utils.tensorboard import SummaryWriter

# Relative imports within the sauron package
from sauron.data.dataset_factory import determine_split_directory, get_data_manager

# IMPORTANT: Ensure sauron/parse/argparse.py is renamed to sauron/parse/cli_parsers.py
from sauron.parse.cli_parsers import get_mil_args
from sauron.training.pipeline import train_fold
from sauron.utils.environment_setup import (
    create_results_directory,
    log_experiment_details,
    seed_everything,
    setup_device,
)
from sauron.utils.generic_utils import (  # Assuming these are general utils
    log_results,
    save_pkl,
)


def run_experiment_folds(
    data_manager,  # Now takes a DataManager instance
    args: argparse.Namespace,
    experiment_main_results_dir: str,
) -> pd.DataFrame:
    """
    Manages the k-fold cross-validation loop.
    """
    if args.task_type.lower() == "classification":
        metric_keys = [
            "test_auc",
            "val_auc",
            "test_acc",
            "val_acc",
        ]  # Expected from train_fold
    elif args.task_type.lower() == "survival":
        metric_keys = [
            "test_c_index",
            "val_c_index",
        ]  # Example, ensure train_fold returns these
    else:
        raise ValueError(f"Unknown task_type: {args.task_type} for defining metrics.")

    all_fold_metrics = {key: [] for key in metric_keys}

    # DataManager creates and manages its own splits internally
    # For ClassificationDataManager:
    if args.task_type.lower() == "classification":
        data_manager.create_k_fold_splits(
            num_folds=args.k, test_set_size=getattr(args, "test_frac", 0.1)
        )
        num_actual_folds = data_manager.get_number_of_folds()
        if num_actual_folds == 0 and args.k > 0:  # No k-folds, but test set might exist
            print(
                "No K-folds generated (num_folds=0 in DataManager), running as single train/test split if test_frac > 0."
            )
            # In this case, loop runs once if test_set_size > 0, setting fold_idx=0
            if args.k <= 1:  # Consider 0 or 1 fold as a single run (train/val/test)
                loop_range = range(1)  # Run once for the main split
                print(f"Running a single train/val/test split (args.k={args.k}).")
            else:  # k > 1 but num_actual_folds is 0 - this is an issue
                raise ValueError(
                    f"args.k={args.k} but DataManager created 0 folds. Check data or split logic."
                )
        else:
            loop_range = range(args.k_start, min(args.k_end, num_actual_folds))

    # For SurvivalDataManager:
    elif args.task_type.lower() == "survival":
        # Create splits using the survival-specific function
        data_manager.create_splits_from_generating_function(
            k=args.k,
            val_num=getattr(args, "val_num_survival", (0.15, 0.15)),
            test_num=getattr(args, "test_num_survival", (0.15, 0.15)),
            label_frac=getattr(args, "label_frac", 1.0),
            custom_test_ids=getattr(args, "custom_test_ids", None),
        )
        loop_range = range(args.k_start, args.k_end)
    else:
        raise ValueError(f"Task type {args.task_type} split creation not defined.")

    for i in loop_range:
        print(f"\n{'=' * 10} Processing Fold: {i} {'=' * 10}")

        # Set the current fold in the DataManager
        if args.task_type.lower() == "classification":
            data_manager.set_current_fold(fold_index=i)
        elif args.task_type.lower() == "survival":
            if not data_manager.set_next_fold_from_generator(
                start_from_fold=i if i == args.k_start else None
            ):
                print(
                    f"SurvivalDataManager's split generator exhausted before reaching fold {i}."
                )
                break

        # Get MILDataset instances for the current fold
        # Common MILDataset parameters from args
        mil_dataset_params = {
            "backbone": args.backbone,
            "patch_size": args.patch_size,
            "use_hdf5": getattr(args, "use_hdf5", False),
            "cache_enabled": getattr(args, "preloading", "no").lower() == "yes",
        }

        # Add survival-specific parameters if needed
        if args.task_type.lower() == "survival":
            mil_dataset_params["mode"] = getattr(args, "survival_mode", "pathomic")

        train_dataset, val_dataset, test_dataset = data_manager.get_mil_datasets(
            **mil_dataset_params
        )

        # Preload data if requested
        if getattr(args, "preloading", "no").lower() == "yes":
            print(f"Preloading data for fold {i}...")
            if train_dataset:
                train_dataset.preload_data()
            if val_dataset:
                val_dataset.preload_data()
            if test_dataset:
                test_dataset.preload_data()

        # Save current split patient IDs if requested
        if getattr(args, "save_splits", False):
            split_file = os.path.join(
                args.split_dir_determined, f"fold_{i}_patient_ids.csv"
            )
            data_manager.save_current_split_patient_ids(split_file)

        # Get fold-specific results directory
        fold_results_dir = os.path.join(experiment_main_results_dir, f"fold_{i}")
        os.makedirs(fold_results_dir, exist_ok=True)

        # Train the model for this fold
        fold_results_tuple = train_fold(
            train_dataset=train_dataset,
            val_dataset=val_dataset,  # Add val_dataset if train_fold expects it
            test_dataset=test_dataset,
            cur_fold_num=i,
            args=args,
            experiment_base_results_dir=fold_results_dir,
        )

        patient_level_results_dict, *fold_metrics_values = fold_results_tuple

        if len(fold_metrics_values) != len(metric_keys):
            raise ValueError(
                f"Mismatch in metrics from train_fold ({len(fold_metrics_values)}) vs expected ({len(metric_keys)}). "
                f"Task: {args.task_type}. Returned: {fold_metrics_values}"
            )

        for key_idx, metric_value in enumerate(fold_metrics_values):
            metric_name = metric_keys[key_idx]
            all_fold_metrics[metric_name].append(metric_value)

        if patient_level_results_dict:
            fold_results_pkl_path = os.path.join(
                fold_results_dir, "patient_level_results.pkl"
            )
            save_pkl(fold_results_pkl_path, patient_level_results_dict)
            print(
                f"Saved patient-level results for fold {i} to {fold_results_pkl_path}"
            )

    # Check if any folds were actually run
    num_folds_run = (
        len(all_fold_metrics[metric_keys[0]])
        if metric_keys and all_fold_metrics[metric_keys[0]]
        else 0
    )
    if num_folds_run == 0:
        print("No folds were successfully processed. Returning empty DataFrame.")
        return pd.DataFrame()

    fold_num_series = list(range(args.k_start, args.k_start + num_folds_run))
    return pd.DataFrame({"fold_num": fold_num_series, **all_fold_metrics})


def run_mil_training_job(args: argparse.Namespace):  # Renamed `main_experiment_runner`
    """
    Main function to run the entire MIL training experiment.
    """
    _ = setup_device()
    seed_everything(args.seed)

    experiment_main_results_dir = create_results_directory(
        args.results_dir, args.exp_code, args.seed
    )
    args.results_dir = experiment_main_results_dir  # Update args for train_fold
    log_experiment_details(args, experiment_main_results_dir)

    # Determine split directory (where DataManager might save split definitions if asked)
    # This is more for organizing outputs than for DataManager to read from, unless you implement that.
    args.split_dir_determined = determine_split_directory(  # Store in a new arg field
        getattr(
            args, "split_dir_base", None
        ),  # Base for auto-generated split dir names
        args.task_name,
        getattr(args, "label_frac", 1.0),
        args.k > 1,  # k_fold is true if num_folds > 1
    )
    os.makedirs(args.split_dir_determined, exist_ok=True)

    with SummaryWriter(
        log_dir=os.path.join(experiment_main_results_dir, "summary_all_folds")
    ) as summary_writer:
        args.k_start = max(0, args.k_start)
        # args.k_end should be such that loop runs for args.k folds, or up to actual num_folds
        # If args.k is the total number of folds intended:
        args.k_end = args.k if args.k_end == -1 or args.k_end > args.k else args.k_end

        if args.k_start >= args.k_end and args.k > 0:  # k_end is exclusive
            print(
                f"Warning: k_start ({args.k_start}) is >= k_end ({args.k_end}). No folds will be run."
            )
            return

        manager_params = {
            "task_name": args.task_name,
            "task_type": args.task_type,
            "csv_path": getattr(
                args, "dataset_csv", None
            ),  # Added check for dataset_csv
            "data_directory": args.data_root_dir,
            "seed": args.seed,
            "verbose": getattr(args, "verbose_data", True),
            # Classification specific from args
            "label_column": getattr(args, "label_col", "label"),
            "patient_id_col_name": getattr(args, "patient_id_col", "case_id"),
            "slide_id_col_name": getattr(args, "slide_id_col", "slide_id"),
            "filter_criteria": json.loads(args.filter_criteria)
            if hasattr(args, "filter_criteria") and args.filter_criteria
            else None,
            "ignore_labels": args.ignore_labels.split(",")
            if hasattr(args, "ignore_labels") and args.ignore_labels
            else None,
            "patient_label_aggregation": getattr(
                args, "patient_label_aggregation", "max"
            ),
            "shuffle": getattr(
                args, "shuffle_data", False
            ),  # For ClassificationDataManager initial load
            # Survival specific from args
            "time_column": getattr(args, "time_col", None),
            "event_column": getattr(args, "event_col", None),
            "n_bins": getattr(args, "n_bins_survival", 4),
            "filter_dict": json.loads(getattr(args, "filter_dict_survival", "null"))
            if hasattr(args, "filter_dict_survival")
            and getattr(args, "filter_dict_survival", "null") != "null"
            else None,
            "omic_csv_path": getattr(args, "omic_csv", None),
            "omic_patient_id_col": getattr(args, "omic_patient_id_col", "case_id"),
            "apply_sig": getattr(args, "apply_sig_survival", False),
            "signatures_csv_path": getattr(args, "signatures_csv", None),
            "shuffle_slide_data": getattr(
                args, "shuffle_slide_data_survival", False
            ),  # For SurvivalDataManager
        }

        # Parse label_mapping if provided as a JSON string
        label_mapping_str = getattr(args, "label_mapping", None)
        if label_mapping_str:
            try:
                manager_params["label_mapping"] = json.loads(label_mapping_str)
            except json.JSONDecodeError:
                raise ValueError(
                    f"Invalid JSON string for label_mapping: {label_mapping_str}"
                )
        else:
            manager_params["label_mapping"] = None

        data_manager_instance = get_data_manager(**manager_params)

        # Get n_classes from DataManager and set it in args for train_fold
        args.n_classes = data_manager_instance.num_classes
        print(f"DataManager initialized. Number of classes: {args.n_classes}")
        if args.n_classes == 0 and args.task_type.lower() == "classification":
            print(
                "Warning: Number of classes is 0 for classification task. Check data and label mapping."
            )

        print(
            f"Starting experiment: {args.exp_code} with up to {args.k} folds (from {args.k_start} to {args.k_end - 1})"
        )
        overall_results_df = run_experiment_folds(
            data_manager_instance, args, experiment_main_results_dir
        )

        if not overall_results_df.empty:
            log_results(overall_results_df, args, summary_writer)
            print("\nAggregated Results over Folds:")
            print(overall_results_df)
        else:
            print("No results to log as no folds were processed.")


if __name__ == "__main__":
    # This block is for direct execution during development/testing outside of package
    # For package usage, `sauron.cli:train_mil_main` will be called.
    args = get_mil_args()
    # Potentially add more argument validation or default setting here if needed
    if not hasattr(args, "task_name"):
        args.task_name = args.task
    if not hasattr(args, "k_fold"):
        args.k_fold = args.k
    run_mil_training_job(args)
    print("Experiment Finished!")



================================================
FILE: sauron/training/lightning_module.py
================================================
import torch
import torch.nn as nn
import pytorch_lightning as pl
from torch.optim.lr_scheduler import CosineAnnealingLR

from sauron.mil_models.models_factory import mil_model_factory
from sauron.losses.surv_loss import NLLSurvLoss, CoxPHSurvLoss
from sauron.utils.metrics import concordance_index, accuracy_cox

class Sauron(pl.LightningModule):
    def __init__(self, args):
        super().__init__()
        self.args = args
        self.model = mil_model_factory(args)
        if self.args.task == 'classification':
            self.loss = nn.CrossEntropyLoss()
        elif self.args.task == 'survival':
            if self.args.loss == 'nll':
                self.loss = NLLSurvLoss()
            elif self.args.loss == 'cox':
                self.loss = CoxPHSurvLoss()
            else:
                raise ValueError(f"Unknown survival loss: {self.args.loss}")
        else:
            raise ValueError(f"Unknown task: {self.args.task}")

    def forward(self, x):
        return self.model(x)

    def training_step(self, batch, batch_idx):
        data, label, event = batch
        output = self(data)
        loss = self.loss(output, label, event)
        self.log('train_loss', loss, on_step=True, on_epoch=True, prog_bar=True, logger=True)
        return loss

    def validation_step(self, batch, batch_idx):
        data, label, event = batch
        output = self(data)
        loss = self.loss(output, label, event)
        self.log('val_loss', loss, on_step=True, on_epoch=True, prog_bar=True, logger=True)
        if self.args.task == 'survival':
            c_index = concordance_index(output, label, event)
            self.log('c_index', c_index, on_step=True, on_epoch=True, prog_bar=True, logger=True)

    def configure_optimizers(self):
        optimizer = torch.optim.Adam(self.parameters(), lr=self.args.lr)
        scheduler = CosineAnnealingLR(optimizer, T_max=self.args.epochs)
        return [optimizer], [scheduler]


================================================
FILE: sauron/training/pipeline.py
================================================
import torch
from torch.utils.data import DataLoader
import pytorch_lightning as pl
from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping

from sauron.data.dataset_factory import dataset_factory
from sauron.training.lightning_module import Sauron

def run_pipeline(args):
    train_dataset = dataset_factory(args, 'train')
    val_dataset = dataset_factory(args, 'val')

    train_loader = DataLoader(train_dataset, batch_size=args.batch_size, shuffle=True, num_workers=args.num_workers)
    val_loader = DataLoader(val_dataset, batch_size=args.batch_size, shuffle=False, num_workers=args.num_workers)

    model = Sauron(args)

    checkpoint_callback = ModelCheckpoint(
        dirpath=f'checkpoints/{args.experiment_name}',
        filename='{epoch}-{val_loss:.2f}-{c_index:.2f}',
        save_top_k=1,
        verbose=True,
        monitor='val_loss',
        mode='min'
    )

    early_stop_callback = EarlyStopping(
        monitor='val_loss',
        min_delta=0.00,
        patience=10,
        verbose=False,
        mode='min'
    )

    trainer = pl.Trainer(
        gpus=1,
        max_epochs=args.epochs,
        callbacks=[checkpoint_callback, early_stop_callback]
    )

    trainer.fit(model, train_loader, val_loader)


================================================
FILE: sauron/utils/callbacks.py
================================================
import os
from typing import Tuple

import numpy as np
import torch
import torch.nn as nn


class AccuracyLogger:
    def __init__(self, n_classes: int):
        self.n_classes = n_classes
        self.data = [{"count": 0, "correct": 0} for _ in range(self.n_classes)]

    def log(self, y_hat: int, y: int):
        self.data[y]["count"] += 1
        self.data[y]["correct"] += int(y_hat == y)

    def log_batch(self, y_hat: np.ndarray, y: np.ndarray):
        y_hat = y_hat.astype(int)
        y = y.astype(int)
        for label_class in range(self.n_classes):
            cls_mask = y == label_class
            self.data[label_class]["count"] += cls_mask.sum()
            self.data[label_class]["correct"] += (y_hat[cls_mask] == y[cls_mask]).sum()

    def get_summary(self, c: int) -> Tuple[float, int, int]:
        count = self.data[c]["count"]
        correct = self.data[c]["correct"]
        return (float(correct) / count if count else 0.0, correct, count)


class EarlyStopping:
    """Early stops the training if validation loss doesn't improve after a given patience."""

    def __init__(self, warmup=5, patience=15, stop_epoch=20, verbose=False):
        """
        Args:
            patience (int): How long to wait after last time validation loss improved.
                            Default: 20
            stop_epoch (int): Earliest epoch possible for stopping
            verbose (bool): If True, prints a message for each validation loss improvement.
                            Default: False
        """
        self.warmup = warmup
        self.patience = patience
        self.stop_epoch = stop_epoch
        self.verbose = verbose
        self.counter = 0
        self.best_score = float("inf")
        self.early_stop = False
        self.val_loss_min = float("inf")

    def __call__(self, epoch, val_loss, model, ckpt_name="checkpoint.pt"):
        score = val_loss

        if epoch < self.warmup:
            pass
        elif score < self.best_score:
            self.best_score = score
            self.save_checkpoint(val_loss, model, ckpt_name)
            self.counter = 0
        else:
            self.counter += 1
            print(f"EarlyStopping counter: {self.counter} out of {self.patience}")
            if self.counter >= self.patience or epoch > self.stop_epoch:
                self.early_stop = True

    def save_checkpoint(self, val_loss, model, ckpt_name):
        """Saves model when validation loss decrease."""
        if self.verbose:
            print(
                f"Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}). Saving model ..."
            )

        # Ensure the directory exists
        os.makedirs(os.path.dirname(ckpt_name), exist_ok=True)

        torch.save(model.state_dict(), ckpt_name)
        self.val_loss_min = val_loss


class EarlyStopping_cindex:
    """Early stops the training if validation loss doesn't improve after a given patience."""

    def __init__(self, warmup=5, patience=15, stop_epoch=20, verbose=False):
        """
        Args:
            patience (int): How long to wait after last time validation loss improved.
                            Default: 20
            stop_epoch (int): Earliest epoch possible for stopping
            verbose (bool): If True, prints a message for each validation loss improvement.
                            Default: False
        """
        self.warmup = warmup
        self.patience = patience
        self.stop_epoch = stop_epoch
        self.verbose = verbose
        self.counter = 0
        self.best_score = None
        self.early_stop = False
        self.val_loss_min = np.Inf

    def __call__(self, epoch, val_loss, model, ckpt_name="checkpoint.pt"):
        score = val_loss
        # score = -val_loss

        if epoch < self.warmup:
            pass
        elif self.best_score is None:
            self.best_score = score
            self.save_checkpoint(val_loss, model, ckpt_name)
        elif score <= self.best_score:
            self.counter += 1
            print(f"EarlyStopping counter: {self.counter} out of {self.patience}")
            if self.counter >= self.patience and epoch > self.stop_epoch:
                self.early_stop = True
        else:
            self.best_score = score
            self.save_checkpoint(val_loss, model, ckpt_name)
            self.counter = 0

    def save_checkpoint(self, val_loss, model, ckpt_name):
        """Saves model when validation loss decrease."""
        if self.verbose:
            print(
                f"Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ..."
            )
        torch.save(model.state_dict(), ckpt_name)
        self.val_loss_min = val_loss


class Monitor_CIndex:
    """Early stops the training if validation loss doesn't improve after a given patience."""

    def __init__(self):
        """
        Args:
            patience (int): How long to wait after last time validation loss improved.
                            Default: 20
            stop_epoch (int): Earliest epoch possible for stopping
            verbose (bool): If True, prints a message for each validation loss improvement.
                            Default: False
        """
        self.best_score = None

    def __call__(self, val_cindex, model, ckpt_name: str = "checkpoint.pt"):
        score = val_cindex

        if self.best_score is None:
            self.best_score = score
            self.save_checkpoint(model, ckpt_name)
        elif score > self.best_score:
            self.best_score = score
            self.save_checkpoint(model, ckpt_name)
        else:
            pass

    def save_checkpoint(self, model, ckpt_name):
        """Saves model when validation loss decrease."""
        torch.save(model.state_dict(), ckpt_name)



================================================
FILE: sauron/utils/drawing_utils.py
================================================
from typing import Union

import cv2
import geopandas as gpd
import numpy as np
import openslide
from PIL import Image

from sauron.utils.WSIObjects import WholeSlideImage, wsi_factory


def draw_contours(
    image: np.ndarray,
    contours: gpd.GeoDataFrame,
    draw_outline: bool = False,
    line_thickness: int = 1,
    scale_factor: float = 1.0,
    contour_color: tuple = (0, 255, 0),
) -> np.ndarray:
    """
    Draw contours on an image.

    Args:
        image (np.ndarray): Image on which to draw.
        contours (gpd.GeoDataFrame): Contours to draw.
        draw_outline (bool): Whether to draw the outline of contours.
        line_thickness (int): Thickness of the contour lines.
        scale_factor (float): Scaling factor for the contours.
        contour_color (tuple): Color of the contours.

    Returns:
        np.ndarray: Image with contours drawn.
    """
    for _, group in contours.groupby("tissue_id"):
        for _, row in group.iterrows():
            exterior = np.array(
                [
                    [
                        int(round(x * scale_factor)),
                        int(round(y * scale_factor)),
                    ]
                    for x, y in row.geometry.exterior.coords
                ]
            )
            interiors = [
                np.array(
                    [
                        [
                            int(round(x * scale_factor)),
                            int(round(y * scale_factor)),
                        ]
                        for x, y in interior.coords
                    ]
                )
                for interior in row.geometry.interiors
            ]

            cv2.drawContours(
                image,
                [exterior],
                contourIdx=-1,
                color=contour_color,
                thickness=cv2.FILLED,
            )
            for hole in interiors:
                cv2.drawContours(
                    image,
                    [hole],
                    contourIdx=-1,
                    color=(0, 0, 0),
                    thickness=cv2.FILLED,
                )
            if draw_outline:
                cv2.drawContours(
                    image,
                    [exterior],
                    contourIdx=-1,
                    color=contour_color,
                    thickness=line_thickness,
                )
    return image


def visualize_tissue(
    wsi: Union[np.ndarray, openslide.OpenSlide, WholeSlideImage],
    tissue_contours: gpd.GeoDataFrame,
    contour_color: tuple = (0, 255, 0),
    line_thickness: int = 5,
    target_width: int = 1000,
) -> Image.Image:
    """
    Visualize tissue contours on a whole slide image.

    Args:
        wsi (Union[np.ndarray, openslide.OpenSlide, WSI]): The whole slide image.
        tissue_contours (gpd.GeoDataFrame): Contours of the tissue regions.
        contour_color (tuple): Color of the contour lines.
        line_thickness (int): Thickness of the contour lines.
        target_width (int): Target width for the visualization image.

    Returns:
        Image.Image: The visualization image.
    """
    wsi = wsi_factory(wsi)
    width, height = wsi.get_dimensions()
    scale_factor = target_width / width

    thumbnail = wsi.get_thumbnail(
        width=int(width * scale_factor), height=int(height * scale_factor)
    )

    if tissue_contours.empty:
        return Image.fromarray(thumbnail)

    overlay = np.zeros_like(thumbnail, dtype=np.uint8)
    overlay = draw_contours(
        overlay,
        tissue_contours,
        draw_outline=True,
        line_thickness=line_thickness,
        scale_factor=scale_factor,
        contour_color=contour_color,
    )

    alpha = 0.4
    blended = cv2.addWeighted(thumbnail, 1 - alpha, overlay, alpha, 0)
    return Image.fromarray(blended)



================================================
FILE: sauron/utils/environment_setup.py
================================================
# sauron/utils/environment_setup.py
import argparse
import os
import random
from typing import Optional

import numpy as np
import torch


def setup_device() -> torch.device:
    """Sets up and returns the device (CUDA or CPU)."""
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"Current device is set to: {device}")
    return device


def seed_everything(seed: int):
    """Seeds random number generators for reproducibility."""
    random.seed(seed)
    os.environ["PYTHONHASHSEED"] = str(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed(seed)
        torch.cuda.manual_seed_all(seed)  # if you are using multi-GPU.
        # The following two lines are often recommended for reproducibility with CuDNN
        # However, they can impact performance. Use with caution.
        # torch.backends.cudnn.deterministic = True
        # torch.backends.cudnn.benchmark = False
    print(f"Seeded everything with seed: {seed}")


def create_results_directory(
    base_results_dir: str, exp_code: str, seed: int, fold_num: Optional[int] = None
) -> str:
    """
    Creates the results directory for the experiment.
    If fold_num is provided, creates a subdirectory for that fold.
    """
    experiment_path = os.path.join(base_results_dir, f"{exp_code}_s{seed}")
    if fold_num is not None:
        results_path = os.path.join(experiment_path, f"fold_{fold_num}")
    else:
        results_path = experiment_path  # Main experiment directory

    os.makedirs(results_path, exist_ok=True)
    return results_path


def log_experiment_details(args: argparse.Namespace, results_dir: str):
    """Logs experiment arguments to a file."""
    # Ensure results_dir here is the main experiment directory, not a fold-specific one
    # if log_experiment_details is called once per experiment.
    # If called per fold, then it's fine if results_dir is fold-specific.
    # For now, assuming it's called once for the main experiment.

    # If results_dir might be a fold-specific path, get the parent experiment path
    # e.g., experiment_base_path = os.path.dirname(results_dir) if "fold_" in os.path.basename(results_dir) else results_dir
    experiment_base_path = (
        results_dir  # Assuming results_dir passed is already the main exp dir
    )

    filepath = os.path.join(experiment_base_path, "experiment_args.txt")
    with open(filepath, "w") as f:
        for key, val in sorted(vars(args).items()):  # Sort for consistent order
            f.write(f"{key}: {val}\n")
    print(f"Experiment arguments logged to: {filepath}")



================================================
FILE: sauron/utils/filehandler.py
================================================
import os
from typing import Any, Dict, List, Optional

import pandas as pd

# File extensions for slide images
EXTENSIONS: List[str] = [".svs", ".mrxs", ".tiff", ".tif", ".TIFF", ".ndpi"]


class PatientFolder:
    def __init__(
        self, patients_dir: str, labels: Optional[pd.DataFrame] = None
    ) -> None:
        self.patients_dir: str = patients_dir
        self.labels: Optional[pd.DataFrame] = labels
        self.data: pd.DataFrame = pd.DataFrame(columns=["pid", "slide_id", "label"])

    def analyze(self) -> None:
        try:
            new_data: List[Dict[str, Any]] = []
            for folder_name in os.listdir(self.patients_dir):
                folder_path: str = os.path.join(self.patients_dir, folder_name)
                if os.path.isdir(folder_path):
                    label = self._get_label(folder_name)
                    self._process_folder(folder_name, folder_path, label, new_data)
            if new_data:
                self.data = pd.concat(
                    [self.data, pd.DataFrame(new_data)], ignore_index=True
                )
        except FileNotFoundError as e:
            raise RuntimeError(
                f"Error analyzing patient folders: Directory '{self.patients_dir}' not found. {e}"
            )
        except PermissionError as e:
            raise RuntimeError(
                f"Error analyzing patient folders: Permission denied when accessing '{self.patients_dir}'. {e}"
            )
        except Exception as e:
            raise RuntimeError(f"Unexpected error analyzing patient folders: {e}")

    def _get_label(self, folder_name: str) -> Any:
        if self.labels is not None and folder_name in self.labels.index:
            return self.labels.loc[folder_name]
        elif folder_name.isdigit():
            return int(folder_name)
        else:
            raise ValueError(
                f"Label for folder '{folder_name}' not found. Ensure that the folder name is either a digit or is present in the provided labels."
            )

    def _process_folder(
        self,
        folder_name: str,
        folder_path: str,
        label: Any,
        new_data: List[Dict[str, Any]],
    ) -> None:
        for file_name in os.listdir(folder_path):
            if any(file_name.endswith(ext) for ext in EXTENSIONS):
                slide_id: str = file_name
                new_data.append(
                    {
                        "pid": folder_name,
                        "slide_id": slide_id,
                        "label": label,
                    }
                )

    def __len__(self) -> int:
        return len(self.data)

    def get_data(self) -> pd.DataFrame:
        return self.data



================================================
FILE: sauron/utils/generic_utils.py
================================================
import argparse
import os
import pickle
from typing import List

import numpy as np
import pandas as pd
import torch
import torch.nn as nn
from torch.utils.data import Dataset
from torch.utils.tensorboard import SummaryWriter


def save_pkl(filename: str, save_object: object) -> None:
    with open(filename, "wb") as f:
        pickle.dump(save_object, f)


def load_pkl(filename: str) -> object:
    with open(filename, "rb") as file:
        return pickle.load(file)


def seed_everything(seed: int = 42) -> None:
    import random

    random.seed(seed)
    os.environ["PYTHONHASHSEED"] = str(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed_all(seed)
    torch.backends.cudnn.benchmark = False
    torch.backends.cudnn.deterministic = True


def calculate_error(Y_hat: torch.Tensor, Y: torch.Tensor) -> float:
    return 1.0 - Y_hat.float().eq(Y.float()).float().mean().item()


def save_splits(
    split_datasets: List[Dataset],
    column_keys: List[str],
    filename: str,
    boolean_style: bool = False,
) -> None:
    splits = [
        split_datasets[i].slide_data["slide_id"] for i in range(len(split_datasets))
    ]
    if not boolean_style:
        df = pd.concat(splits, ignore_index=True, axis=1)
        df.columns = column_keys
    else:
        df = pd.concat(splits, ignore_index=True, axis=0)
        index = df.values.tolist()
        one_hot = np.eye(len(split_datasets)).astype(bool)
        bool_array = np.repeat(one_hot, [len(dset) for dset in split_datasets], axis=0)
        df = pd.DataFrame(bool_array, index=index, columns=["train", "val", "test"])

    df.to_csv(filename)
    print(f"Splits saved to {filename}")


def initialize_weights(module):
    for m in module.modules():
        if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):
            nn.init.xavier_normal_(m.weight)
            if m.bias is not None:
                m.bias.data.zero_()
        elif isinstance(m, nn.LayerNorm):
            nn.init.constant_(m.bias, 0)
            nn.init.constant_(m.weight, 1.0)


def log_results(
    df: pd.DataFrame, args: argparse.Namespace, writer: SummaryWriter
) -> None:
    mean_metrics = df.mean()
    std_metrics = df.std()

    for metric in ["test_auc", "val_auc", "test_acc", "val_acc"]:
        writer.add_scalar(f"mean_{metric}", mean_metrics[metric], args.k_end)
        writer.add_scalar(f"std_{metric}", std_metrics[metric], args.k_end)

    df_append = pd.DataFrame(
        {
            "folds": ["mean", "std"],
            **{
                metric: [mean_metrics[metric], std_metrics[metric]]
                for metric in ["test_auc", "val_auc", "test_acc", "val_acc"]
            },
        }
    )

    final_df = pd.concat([df, df_append])
    save_name = (
        f"summary_partial_{args.k_start}_{args.k_end}.csv"
        if args.k_end - args.k_start != args.k
        else "summary.csv"
    )
    final_df.to_csv(os.path.join(args.results_dir, save_name), index=False)



================================================
FILE: sauron/utils/metrics.py
================================================
from typing import Tuple

import numpy as np
from sklearn.metrics import (
    roc_auc_score,
)
from sklearn.preprocessing import label_binarize
from sksurv.metrics import concordance_index_censored


# --- Metric Calculation Helpers ---
def _calculate_classification_auc(
    all_labels_np: np.ndarray, all_probs_np: np.ndarray, n_classes: int
) -> float:
    """Calculates AUC for classification."""
    if n_classes == 2:
        # Ensure there are at least two classes in labels for AUC calculation
        if len(np.unique(all_labels_np)) < 2:
            print(
                f"Warning: Only one class present in labels for binary AUC calculation. AUC set to 0.0."
            )
            return 0.0
        try:
            return roc_auc_score(all_labels_np, all_probs_np[:, 1])
        except ValueError as e:
            print(
                f"Warning: Could not calculate AUC (binary): {e}. Check if all_probs_np has two columns."
            )
            return 0.0
    else:  # Multi-class
        try:
            # Binarize labels against all potential classes (0 to n_classes-1)
            # This ensures consistent shape for roc_auc_score's `average` parameter.
            all_labels_bin = label_binarize(all_labels_np, classes=range(n_classes))

            # Check if any class has no instances after binarization.
            # Some classes might not be present in this specific batch/dataset split.
            # roc_auc_score (ovr) can handle cases where some classes specified in `labels`
            # are not present, as long as y_true and y_score shapes match on the number of classes.
            # However, if all_labels_bin has fewer columns than n_classes (e.g. max label is 1 for n_classes=3)
            # then roc_auc_score needs y_score to match that shape or careful slicing.
            # Assuming all_probs_np always has n_classes columns.
            if all_labels_bin.shape[1] < n_classes:
                # This can happen if the max label in all_labels_np is less than n_classes-1.
                # Pad all_labels_bin with zero columns for missing classes up to n_classes.
                padding = np.zeros(
                    (all_labels_bin.shape[0], n_classes - all_labels_bin.shape[1])
                )
                all_labels_bin = np.hstack((all_labels_bin, padding))

            # Ensure at least two classes are present in the actual data for meaningful OvR AUC
            if len(np.unique(all_labels_np)) < 2:
                print(
                    f"Warning: Less than 2 unique classes present in labels for multi-class AUC. AUC set to 0.0."
                )
                return 0.0

            return roc_auc_score(
                all_labels_bin, all_probs_np, multi_class="ovr", average="weighted"
            )
        except ValueError as e:
            # This can happen if all_probs_np doesn't have n_classes columns, or other inconsistencies.
            print(f"Warning: Could not calculate AUC (multi-class, weighted OvR): {e}")
            return 0.0


def _calculate_survival_c_index(
    all_event_times_np: np.ndarray,
    all_censorships_np: np.ndarray,
    all_risks_np: np.ndarray,
) -> float:
    """Calculates C-Index for survival."""
    if (
        len(all_event_times_np) == 0
        or len(all_censorships_np) == 0
        or len(all_risks_np) == 0
    ):
        print("Warning: Empty arrays provided for C-index calculation. Returning 0.0.")
        return 0.0
    event_observed = (1 - all_censorships_np).astype(bool)
    try:
        # Check for trivial cases that concordance_index_censored might not handle well
        if (
            len(np.unique(event_observed)) == 1 and not event_observed[0]
        ):  # All censored
            print(
                "Warning: All samples are censored. C-index is undefined, returning 0.0."
            )
            return 0.0
        if len(np.unique(all_event_times_np[event_observed])) < 1 and np.any(
            event_observed
        ):  # All observed events at the same time
            pass  # This case is handled by sksurv

        c_index, _, _, _, _ = concordance_index_censored(
            event_observed, all_event_times_np, all_risks_np
        )
        return c_index
    except Exception as e:  # Catch more general exceptions from sksurv if any
        print(f"Warning: Could not calculate C-index: {e}")
        return 0.0



================================================
FILE: sauron/utils/optimizers.py
================================================
from typing import Any

import torch
import torch.nn as nn
import torch.optim as optim


def get_optim(model: nn.Module, args: Any) -> optim.Optimizer:
    if args.opt == "adam":
        return optim.Adam(
            filter(lambda p: p.requires_grad, model.parameters()),
            lr=args.lr,
            weight_decay=args.reg,
        )
    elif args.opt == "adamw":
        return optim.AdamW(
            filter(lambda p: p.requires_grad, model.parameters()),
            lr=args.lr,
            weight_decay=args.reg,
        )
    elif args.opt == "sgd":
        return optim.SGD(
            filter(lambda p: p.requires_grad, model.parameters()),
            lr=args.lr,
            momentum=0.9,
            weight_decay=args.reg,
        )
    raise NotImplementedError(f"Optimizer {args.opt} not implemented")



================================================
FILE: sauron/utils/process_args.py
================================================
import argparse


def main(args):
    """Main function for training the model."""
    # Placeholder for actual training logic
    print(f"Starting training with the following configurations:\n{args}")


def get_args():
    """
    Parse command line arguments for Whole Slide Image (WSI) Training configurations.

    Returns:
        argparse.Namespace: Parsed command line arguments.
    """
    parser = argparse.ArgumentParser(
        description="Configurations for Whole Slide Image (WSI) Training",
        formatter_class=argparse.ArgumentDefaultsHelpFormatter,
    )

    # Dataset and paths
    parser.add_argument(
        "--data_root_dir",
        type=str,
        default=None,
        help="Root directory containing the dataset",
    )
    parser.add_argument(
        "--results_dir",
        type=str,
        default="./results",
        help="Directory to save training results and model checkpoints",
    )
    parser.add_argument(
        "--split_dir",
        type=str,
        default=None,
        help=(
            "Directory containing custom data splits. If not specified, splits will be "
            "inferred from the task and label_frac arguments"
        ),
    )
    parser.add_argument(
        "--dataset",
        type=str,
        default=None,
        help="Name of the dataset to use",
    )
    parser.add_argument(
        "--csv_fpath",
        type=str,
        default=None,
        help="Path to CSV file containing labels",
    )
    parser.add_argument(
        "--cohort",
        type=str,
        default=None,
        help="Cohort or disease model identifier",
    )

    # Training hyperparameters
    parser.add_argument(
        "--max_epochs",
        type=int,
        default=100,
        help="Maximum number of epochs to train",
    )
    parser.add_argument(
        "--lr",
        type=float,
        default=1e-4,
        help="Initial learning rate for the optimizer",
    )
    parser.add_argument(
        "--weight_decay",
        type=float,
        default=1e-5,
        help="Weight decay (L2 regularization) factor",
    )
    parser.add_argument(
        "--opt",
        type=str,
        choices=["adam", "adamW", "sgd"],
        default="adamW",
        help="Optimizer to use for training",
    )
    parser.add_argument(
        "--drop_out",
        type=float,
        default=0.25,
        help="Dropout probability for regularization",
    )
    parser.add_argument(
        "--early_stopping",
        action="store_true",
        help="Enable early stopping to prevent overfitting",
    )
    parser.add_argument(
        "--weighted_sample",
        action="store_true",
        help="Enable weighted sampling to handle class imbalance",
    )
    parser.add_argument(
        "--batch_size",
        type=int,
        default=32,
        help="Batch size for training",
    )
    parser.add_argument(
        "--n_subsamples",
        type=int,
        default=-1,
        help="Number of patches to sample during training",
    )
    parser.add_argument(
        "--scheduler",
        type=str,
        default=None,
        help="Scheduler used for training",
    )
    parser.add_argument(
        "--num_workers",
        type=int,
        default=1,
        help="Number of CPU workers for data loading",
    )
    parser.add_argument(
        "--temperature",
        type=float,
        default=0.001,
        help="Temperature parameter for training",
    )
    parser.add_argument(
        "--warmup",
        action="store_true",
        default=False,
        help="Enable warmup",
    )
    parser.add_argument(
        "--warmup_epochs",
        type=int,
        default=5,
        help="Number of epochs for warmup",
    )
    parser.add_argument(
        "--end_learning_rate",
        type=float,
        default=1e-8,
        help="End learning rate for scheduler",
    )
    parser.add_argument(
        "--num_gpus",
        type=int,
        default=1,
        help="Number of GPUs to use",
    )
    parser.add_argument(
        "--precision",
        type=str,
        default="float32",
        help="Precision for training (e.g., float32, float64)",
    )

    # Model configuration
    parser.add_argument(
        "--model_type",
        type=str,
        default="att_mil",
        choices=[
            "att_mil",
            "trans_mil",
            "diff_att_mil",
            "mean_mil",
            "max_mil",
            "Mamba",
            "BiMamba",
            "SRMamba",
        ],
        help="Type of model architecture (e.g., 'att_mil')",
    )
    parser.add_argument(
        "--wsi_encoder",
        type=str,
        default="abmil",
        help="WSI encoder to use",
    )
    parser.add_argument(
        "--backbone",
        type=str,
        default="resnet50",
        help="Backbone network for feature extraction",
    )
    parser.add_argument(
        "--activation",
        type=str,
        default="softmax",
        help="Activation function to use",
    )
    parser.add_argument(
        "--in_dim",
        type=int,
        default=1024,
        help="Input dimension for the model",
    )
    parser.add_argument(
        "--wsi_encoder_hidden_dim",
        type=int,
        default=512,
        help="Hidden dimension for WSI encoder",
    )
    parser.add_argument(
        "--n_heads",
        type=int,
        default=4,
        help="Number of heads in attention mechanism",
    )
    parser.add_argument(
        "--add_stain_encoding",
        action="store_true",
        default=False,
        help="Include stain encodings in the model",
    )
    parser.add_argument(
        "--mambamil_rate",
        type=int,
        default=10,
        help="Rate parameter for MambaMIL",
    )
    parser.add_argument(
        "--mambamil_layer",
        type=int,
        default=2,
        help="Number of layers in MambaMIL",
    )
    # Experiment settings
    parser.add_argument(
        "--task",
        type=str,
        required=True,
        help="Task name or identifier for the current experiment",
    )
    parser.add_argument(
        "--seed",
        type=int,
        default=1,
        help="Random seed for reproducibility",
    )
    parser.add_argument(
        "--label_frac",
        type=float,
        default=1.0,
        help="Fraction of training labels to use",
    )
    parser.add_argument(
        "--k",
        type=int,
        default=10,
        help="Total number of folds for cross-validation",
    )
    parser.add_argument(
        "--k_start",
        type=int,
        default=-1,
        help="Starting fold for cross-validation (-1 for last fold)",
    )
    parser.add_argument(
        "--k_end",
        type=int,
        default=-1,
        help="Ending fold for cross-validation (-1 for first fold)",
    )

    # Data processing
    parser.add_argument(
        "--patch_size",
        type=str,
        default="",
        help="Size of image patches (format: [height]x[width])",
    )
    parser.add_argument(
        "--resolution",
        type=str,
        default="20x",
        help=(
            "Magnification level to work with, e.g., '10x' or '10x_40x' "
            "(for multiple levels)"
        ),
    )
    parser.add_argument(
        "--early_fusion",
        action="store_true",
        default=False,
        help=(
            "Create an early fusion instead of a late fusion of models with "
            "multiple magnification levels"
        ),
    )
    parser.add_argument(
        "--preloading",
        type=str,
        default="no",
        choices=["yes", "no"],
        help="Whether to preload data into memory",
    )
    parser.add_argument(
        "--num_workers",
        type=int,
        default=4,
        help="Number of worker threads for data loading",
    )

    # Logging and debugging
    parser.add_argument(
        "--log_data",
        action="store_true",
        help="Enable logging of training data using TensorBoard",
    )
    parser.add_argument(
        "--testing",
        action="store_true",
        help="Enable testing/debugging mode",
    )
    parser.add_argument(
        "--log_ml",
        action="store_true",
        help="Enable logging of results in MLflow and TensorBoard",
    )
    parser.add_argument(
        "--wandb_project_name",
        type=str,
        default="WSI_Project",
        help="Project name to use for logging to WANDB",
    )
    parser.add_argument(
        "--wandb_entity",
        type=str,
        default="wsi_entity",
        help="Entity to use for logging to WANDB",
    )

    # Loss functions
    parser.add_argument(
        "--symmetric_cl",
        action="store_true",
        default=False,
        help="Use symmetric contrastive loss",
    )
    parser.add_argument(
        "--global_loss",
        type=str,
        default="-1",
        help="Loss used for global alignment of different WSI",
    )
    parser.add_argument(
        "--local_loss",
        type=str,
        default="-1",
        help="Loss used for local alignment of different WSI",
    )
    parser.add_argument(
        "--intra_modality_loss",
        type=str,
        default="-1",
        help="Info-NCE loss for comparing different views of the same WSI",
    )
    parser.add_argument(
        "--local_loss_weight",
        type=float,
        default=1.0,
        help="Weight for local loss",
    )

    # Pretrained model
    parser.add_argument(
        "--pretrained",
        type=str,
        default=None,
        help="Path to directory with checkpoint",
    )

    args = parser.parse_args()
    return args


